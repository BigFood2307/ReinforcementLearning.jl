<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Policies · ReinforcementLearning.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>ReinforcementLearning.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Introduction</a></li><li><a class="toctext" href="../usage/">Usage</a></li><li><span class="toctext">Reference</span><ul><li><a class="toctext" href="../comparison/">Comparison</a></li><li><a class="toctext" href="../learning/">Learning</a></li><li><a class="toctext" href="../learners/">Learners</a></li><li><a class="toctext" href="../environments/">Environments</a></li><li><a class="toctext" href="../stop/">Stopping Criteria</a></li><li><a class="toctext" href="../preprocessors/">Preprocessors</a></li><li class="current"><a class="toctext" href>Policies</a><ul class="internal"><li><a class="toctext" href="#Epsilon-Greedy-Policies-1">Epsilon Greedy Policies</a></li><li><a class="toctext" href="#Softmax-Policies-1">Softmax Policies</a></li><li><a class="toctext" href="#Forced-Policy-and-Episode-1">Forced Policy and Episode</a></li></ul></li><li><a class="toctext" href="../callbacks/">Callbacks</a></li><li><a class="toctext" href="../metrics/">Evaluation Metrics</a></li></ul></li><li><a class="toctext" href="../api/">API</a></li></ul></nav><article id="docs"><header><nav><ul><li>Reference</li><li><a href>Policies</a></li></ul><a class="edit-page" href="https://github.com/jbrea/ReinforcementLearning.jl/blob/master/docs/src/policies.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Policies</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="policies-1" href="#policies-1">Policies</a></h1><h2><a class="nav-anchor" id="Epsilon-Greedy-Policies-1" href="#Epsilon-Greedy-Policies-1">Epsilon Greedy Policies</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.EpsilonGreedyPolicy" href="#ReinforcementLearning.EpsilonGreedyPolicy"><code>ReinforcementLearning.EpsilonGreedyPolicy</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct EpsilonGreedyPolicy &lt;: AbstractEpsilonGreedyPolicy
    ϵ::Float64</code></pre><p>Chooses the action with the highest value with probability 1 - ϵ and selects an action uniformly random with probability ϵ. For states with actions that where never performed before, the behavior of the <a href="#ReinforcementLearning.VeryOptimisticEpsilonGreedyPolicy"><code>VeryOptimisticEpsilonGreedyPolicy</code></a> is followed.</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/ReinforcementLearning.jl/blob/1642a513ec62dd837b1404f66044c5b3d3cd9778/src/epsilongreedypolicies.jl#L11-L19">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.OptimisticEpsilonGreedyPolicy" href="#ReinforcementLearning.OptimisticEpsilonGreedyPolicy"><code>ReinforcementLearning.OptimisticEpsilonGreedyPolicy</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct OptimisticEpsilonGreedyPolicy &lt;: AbstractEpsilonGreedyPolicy
    ϵ::Float64</code></pre><p><a href="#ReinforcementLearning.EpsilonGreedyPolicy"><code>EpsilonGreedyPolicy</code></a> that samples uniformly from the actions with the highest Q-value and novel actions in each state where actions are available that where never chosen before. </p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/ReinforcementLearning.jl/blob/1642a513ec62dd837b1404f66044c5b3d3cd9778/src/epsilongreedypolicies.jl#L30-L37">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.PesimisticEpsilonGreedyPolicy" href="#ReinforcementLearning.PesimisticEpsilonGreedyPolicy"><code>ReinforcementLearning.PesimisticEpsilonGreedyPolicy</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct PesimisticEpsilonGreedyPolicy &lt;: AbstractEpsilonGreedyPolicy
    ϵ::Float64</code></pre><p><a href="#ReinforcementLearning.EpsilonGreedyPolicy"><code>EpsilonGreedyPolicy</code></a> that does not handle novel actions differently.</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/ReinforcementLearning.jl/blob/1642a513ec62dd837b1404f66044c5b3d3cd9778/src/epsilongreedypolicies.jl#L39-L44">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.VeryOptimisticEpsilonGreedyPolicy" href="#ReinforcementLearning.VeryOptimisticEpsilonGreedyPolicy"><code>ReinforcementLearning.VeryOptimisticEpsilonGreedyPolicy</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct VeryOptimisticEpsilonGreedyPolicy &lt;: AbstractEpsilonGreedyPolicy
    ϵ::Float64</code></pre><p><a href="#ReinforcementLearning.EpsilonGreedyPolicy"><code>EpsilonGreedyPolicy</code></a> that samples uniformly from novel actions in each state where actions are available that where never chosen before. See also  <a href="../learners/#initunseen-1">Initial values, novel actions and unseen values</a>.</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/ReinforcementLearning.jl/blob/1642a513ec62dd837b1404f66044c5b3d3cd9778/src/epsilongreedypolicies.jl#L21-L28">source</a></section><h2><a class="nav-anchor" id="Softmax-Policies-1" href="#Softmax-Policies-1">Softmax Policies</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.AbstractSoftmaxPolicy" href="#ReinforcementLearning.AbstractSoftmaxPolicy"><code>ReinforcementLearning.AbstractSoftmaxPolicy</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct SoftmaxPolicy &lt;: AbstractSoftmaxPolicy
    β::Float64</code></pre><p>Choose action <span>$a$</span> with probability</p><div>\[\frac{e^{\beta x_a}}{\sum_{a&#39;} e^{\beta x_{a&#39;}}}\]</div><p>where <span>$x$</span> is a vector of values for each action. In states with actions that were never chosen before, a uniform random novel action is returned.</p><pre><code class="language-none">SoftmaxPolicy(; β = 1.)</code></pre><p>Returns a SoftmaxPolicy with default β = 1.</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/ReinforcementLearning.jl/blob/1642a513ec62dd837b1404f66044c5b3d3cd9778/src/softmaxpolicy.jl#L1-L17">source</a></section><h2><a class="nav-anchor" id="Forced-Policy-and-Episode-1" href="#Forced-Policy-and-Episode-1">Forced Policy and Episode</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.ForcedEpisode" href="#ReinforcementLearning.ForcedEpisode"><code>ReinforcementLearning.ForcedEpisode</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct ForcedEpisode{Ts}
    t::Int64
    states::Ts
    dones::Array{Bool, 1}
    rewards::Array{Float64, 1}</code></pre></div><a class="source-link" target="_blank" href="https://github.com/jbrea/ReinforcementLearning.jl/blob/1642a513ec62dd837b1404f66044c5b3d3cd9778/src/forced.jl#L23-L29">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.ForcedPolicy" href="#ReinforcementLearning.ForcedPolicy"><code>ReinforcementLearning.ForcedPolicy</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct ForcedPolicy 
    t::Int64
    actions::Array{Int64, 1}</code></pre></div><a class="source-link" target="_blank" href="https://github.com/jbrea/ReinforcementLearning.jl/blob/1642a513ec62dd837b1404f66044c5b3d3cd9778/src/forced.jl#L1-L5">source</a></section><footer><hr/><a class="previous" href="../preprocessors/"><span class="direction">Previous</span><span class="title">Preprocessors</span></a><a class="next" href="../callbacks/"><span class="direction">Next</span><span class="title">Callbacks</span></a></footer></article></body></html>
