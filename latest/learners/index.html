<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Learners · ReinforcementLearning.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>ReinforcementLearning.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Introduction</a></li><li><a class="toctext" href="../usage/">Usage</a></li><li><a class="toctext" href="../tutorial/">Tutorial</a></li><li><span class="toctext">Reference</span><ul><li><a class="toctext" href="../comparison/">Comparison</a></li><li><a class="toctext" href="../learning/">Learning</a></li><li class="current"><a class="toctext" href>Learners</a><ul class="internal"><li><a class="toctext" href="#TD-Learner-1">TD Learner</a></li><li><a class="toctext" href="#Policy-Gradient-Learner-1">Policy Gradient Learner</a></li><li><a class="toctext" href="#N-step-Learner-1">N-step Learner</a></li><li><a class="toctext" href="#Model-Based-Learner-1">Model Based Learner</a></li><li><a class="toctext" href="#Deep-Reinforcement-Learning-1">Deep Reinforcement Learning</a></li></ul></li><li><a class="toctext" href="../buffers/">Buffers</a></li><li><a class="toctext" href="../environments/">Environments</a></li><li><a class="toctext" href="../stop/">Stopping Criteria</a></li><li><a class="toctext" href="../preprocessors/">Preprocessors</a></li><li><a class="toctext" href="../policies/">Policies</a></li><li><a class="toctext" href="../callbacks/">Callbacks</a></li><li><a class="toctext" href="../metrics/">Evaluation Metrics</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Reference</li><li><a href>Learners</a></li></ul><a class="edit-page" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/master/docs/src/learners.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Learners</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="learners-1" href="#learners-1">Learners</a></h1><h2><a class="nav-anchor" id="TD-Learner-1" href="#TD-Learner-1">TD Learner</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.ExpectedSarsa-Tuple{}" href="#ReinforcementLearning.ExpectedSarsa-Tuple{}"><code>ReinforcementLearning.ExpectedSarsa</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">ExpectedSarsa(; kargs...) = TDLearner(; endvaluepolicy = ExpectedSarsaEndPolicy(VeryOptimisticEpsilonGreedyPolicy(.1)), kargs...)</code></pre></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/b33948caf525675efc4d1e8fbb11a56f0681b8e0/src/learner/tdlearning.jl#L45-L47">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.QLearning-Tuple{}" href="#ReinforcementLearning.QLearning-Tuple{}"><code>ReinforcementLearning.QLearning</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">QLearning(; kargs...) = TDLearner(; endvaluepolicy = QLearningEndPolicy(), kargs...)</code></pre></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/b33948caf525675efc4d1e8fbb11a56f0681b8e0/src/learner/tdlearning.jl#L39-L41">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.Sarsa-Tuple{}" href="#ReinforcementLearning.Sarsa-Tuple{}"><code>ReinforcementLearning.Sarsa</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">Sarsa(; kargs...) = TDLearner(; kargs...)</code></pre></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/b33948caf525675efc4d1e8fbb11a56f0681b8e0/src/learner/tdlearning.jl#L35-L37">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.AccumulatingTraces" href="#ReinforcementLearning.AccumulatingTraces"><code>ReinforcementLearning.AccumulatingTraces</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">struct AccumulatingTraces &lt;: AbstractTraces
    λ::Float64
    γλ::Float64
    trace::Array{Float64, 2}
    minimaltracevalue::Float64</code></pre><p>Decaying traces with factor γλ. </p><p>Traces are updated according to <span>$e(a, s) ←  1 + e(a, s)$</span> for the current action-state pair and <span>$e(a, s) ←  γλ e(a, s)$</span> for all other pairs unless <span>$e(a, s) &lt;$</span> <code>minimaltracevalue</code> where the trace is set to 0  (for computational efficiency).</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/b33948caf525675efc4d1e8fbb11a56f0681b8e0/src/traces.jl#L42-L55">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.AccumulatingTraces-Tuple{}" href="#ReinforcementLearning.AccumulatingTraces-Tuple{}"><code>ReinforcementLearning.AccumulatingTraces</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">AccumulatingTraces(ns, na, λ::Float64, γ::Float64; minimaltracevalue = 1e-12)</code></pre></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/b33948caf525675efc4d1e8fbb11a56f0681b8e0/src/traces.jl#L56-L58">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.NoTraces" href="#ReinforcementLearning.NoTraces"><code>ReinforcementLearning.NoTraces</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">struct NoTraces &lt;: AbstractTraces</code></pre><p>No eligibility traces, i.e. <span>$e(a, s) = 1$</span> for current action <span>$a$</span> and state <span>$s$</span> and zero otherwise.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/b33948caf525675efc4d1e8fbb11a56f0681b8e0/src/traces.jl#L1-L6">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.ReplacingTraces" href="#ReinforcementLearning.ReplacingTraces"><code>ReinforcementLearning.ReplacingTraces</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">struct ReplacingTraces &lt;: AbstractTraces
    λ::Float64
    γλ::Float64
    trace::Array{Float64, 2}
    minimaltracevalue::Float64</code></pre><p>Decaying traces with factor γλ. </p><p>Traces are updated according to <span>$e(a, s) ←  1$</span> for the current action-state pair and <span>$e(a, s) ←  γλ e(a, s)$</span> for all other pairs unless <span>$e(a, s) &lt;$</span> <code>minimaltracevalue</code> where the trace is set to 0  (for computational efficiency).</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/b33948caf525675efc4d1e8fbb11a56f0681b8e0/src/traces.jl#L25-L38">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.ReplacingTraces-Tuple{}" href="#ReinforcementLearning.ReplacingTraces-Tuple{}"><code>ReinforcementLearning.ReplacingTraces</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">ReplacingTraces(ns, na, λ::Float64, γ::Float64; minimaltracevalue = 1e-12)</code></pre></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/b33948caf525675efc4d1e8fbb11a56f0681b8e0/src/traces.jl#L39-L41">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.TDLearner" href="#ReinforcementLearning.TDLearner"><code>ReinforcementLearning.TDLearner</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct TDLearner{T,Tp}
    ns::Int64 = 10
    na::Int64 = 4
    γ::Float64 = .9
    λ::Float64 = .8
    α::Float64 = .1
    nsteps::Int64 = 1
    initvalue::Float64 = 0.
    unseenvalue::Float64 = initvalue == Inf64 ? 0. : initvalue
    params::Array{Float64, 2} = zeros(na, ns) .+ initvalue
    tracekind = DataType = λ == 0 ? NoTraces : ReplacingTraces
    traces::T = tracekind == NoTraces ? NoTraces() : tracekind(ns, na, λ, γ)
    endvaluepolicy::Tp = SarsaEndPolicy()</code></pre></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/b33948caf525675efc4d1e8fbb11a56f0681b8e0/src/learner/tdlearning.jl#L1-L15">source</a></section><h3><a class="nav-anchor" id="initunseen-1" href="#initunseen-1">Initial values, novel actions and unseen values</a></h3><p>For td-error dependent methods, the exploration-exploitation trade-off depends on the <code>initvalue</code> and the <code>unseenvalue</code>.  To distinguish actions that were never choosen before, i.e. novel actions, the default initial Q-value (field <code>param</code>) is <code>initvalue = Inf64</code>. In a state with novel actions, the <a href="../policies/#policies-1">policy</a> determines how to deal with novel actions. To compute the td-error the <code>unseenvalue</code> is used for states with novel actions.  One way to achieve agressively exploratory behavior is to assure that <code>unseenvalue</code> (or <code>initvalue</code>) is larger than the largest possible Q-value.</p><h2><a class="nav-anchor" id="Policy-Gradient-Learner-1" href="#Policy-Gradient-Learner-1">Policy Gradient Learner</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.Critic" href="#ReinforcementLearning.Critic"><code>ReinforcementLearning.Critic</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct Critic &lt;: AbstractBiasCorrector
    α::Float64
    V::Array{Float64, 1}</code></pre></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/b33948caf525675efc4d1e8fbb11a56f0681b8e0/src/learner/policygradientlearning.jl#L107-L111">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.Critic-Tuple{}" href="#ReinforcementLearning.Critic-Tuple{}"><code>ReinforcementLearning.Critic</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">Critic(; γ = .9, α = .1, ns = 10, initvalue = 0.)</code></pre></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/b33948caf525675efc4d1e8fbb11a56f0681b8e0/src/learner/policygradientlearning.jl#L118-L120">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.NoBiasCorrector" href="#ReinforcementLearning.NoBiasCorrector"><code>ReinforcementLearning.NoBiasCorrector</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">struct NoBiasCorrector &lt;: AbstractBiasCorrector</code></pre></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/b33948caf525675efc4d1e8fbb11a56f0681b8e0/src/learner/policygradientlearning.jl#L78-L80">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.RewardLowpassFilterBiasCorrector" href="#ReinforcementLearning.RewardLowpassFilterBiasCorrector"><code>ReinforcementLearning.RewardLowpassFilterBiasCorrector</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct RewardLowpassFilterBiasCorrector &lt;: AbstractBiasCorrector
λ::Float64
rmean::Float64</code></pre><p>Filters the reward with factor λ and uses effective reward (r - rmean) to update the parameters.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/b33948caf525675efc4d1e8fbb11a56f0681b8e0/src/learner/policygradientlearning.jl#L85-L92">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.ActorCriticPolicyGradient-Tuple{}" href="#ReinforcementLearning.ActorCriticPolicyGradient-Tuple{}"><code>ReinforcementLearning.ActorCriticPolicyGradient</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">ActorCriticPolicyGradient(; nsteps = 1, γ = .9, ns = 10, na = 4, 
                            α = .1, αcritic = .1, initvalue = Inf64)</code></pre></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/b33948caf525675efc4d1e8fbb11a56f0681b8e0/src/learner/policygradientlearning.jl#L66-L69">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.EpisodicReinforce-Tuple{}" href="#ReinforcementLearning.EpisodicReinforce-Tuple{}"><code>ReinforcementLearning.EpisodicReinforce</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">EpisodicReinforce(; kwargs...) = PolicyGradientForward(; kwargs...)</code></pre></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/b33948caf525675efc4d1e8fbb11a56f0681b8e0/src/learner/policygradientlearning.jl#L61-L63">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.AbstractPolicyGradient" href="#ReinforcementLearning.AbstractPolicyGradient"><code>ReinforcementLearning.AbstractPolicyGradient</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct PolicyGradientBackward &lt;: AbstractPolicyGradient
    ns::Int64 = 10
    na::Int64 = 4
    γ::Float64 = .9
    α::Float64 = .1
    initvalue::Float64 = 0.
    params::Array{Float64, 2} = zeros(na, ns) + initvalue
    traces::AccumulatingTraces = AccumulatingTraces(ns, na, 1., γ, 
                                                    trace = zeros(na, ns))
    biascorrector::T = NoBiasCorrector()</code></pre><p>Policy gradient learning in the backward view.</p><p>The parameters are updated according to <span>$params[a, s] += α * r_{eff} * e[a, s]$</span> where <span>$r_{eff} =  r$</span> for <a href="#ReinforcementLearning.NoBiasCorrector"><code>NoBiasCorrector</code></a>, <span>$r_{eff} =  r - rmean$</span> for <a href="#ReinforcementLearning.RewardLowpassFilterBiasCorrector"><code>RewardLowpassFilterBiasCorrector</code></a> and e[a, s] is the eligibility trace.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/b33948caf525675efc4d1e8fbb11a56f0681b8e0/src/learner/policygradientlearning.jl#L2-L23">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.PolicyGradientForward" href="#ReinforcementLearning.PolicyGradientForward"><code>ReinforcementLearning.PolicyGradientForward</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct PolicyGradientForward &lt;: AbstractPolicyGradient
    ns::Int64 = 10
    na::Int64 = 4
    γ::Float64 = .9
    α::Float64 = .1
    initvalue::Float64 = 0.
    params::Array{Float64, 2} = zeros(na, ns) + initvalue
    biascorrector::Tb = NoBiasCorrector()
    nsteps::Int64 = typemax(Int64)</code></pre></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/b33948caf525675efc4d1e8fbb11a56f0681b8e0/src/learner/policygradientlearning.jl#L37-L47">source</a></section><h2><a class="nav-anchor" id="N-step-Learner-1" href="#N-step-Learner-1">N-step Learner</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.MonteCarlo" href="#ReinforcementLearning.MonteCarlo"><code>ReinforcementLearning.MonteCarlo</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct MonteCarlo &lt;: AbstractReinforcementLearner
    ns::Int64 = 10
    na::Int64 = 4
    γ::Float64 = .9
    initvalue = 0.
    Nsa::Array{Int64, 2} = zeros(Int64, na, ns)
    Q::Array{Float64, 2} = zeros(na, ns) + initvalue</code></pre><p>Estimate Q values by averaging over returns.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/b33948caf525675efc4d1e8fbb11a56f0681b8e0/src/learner/montecarlo.jl#L1-L12">source</a></section><h2><a class="nav-anchor" id="Model-Based-Learner-1" href="#Model-Based-Learner-1">Model Based Learner</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.MDPLearner" href="#ReinforcementLearning.MDPLearner"><code>ReinforcementLearning.MDPLearner</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">@with_kw struct MDPLearner
    mdp::MDP = MDP()
    γ::Float64 = .9
    policy::Array{Int64, 1} = ones(Int64, mdp.ns)
    values::Array{Float64, 1} = zeros(mdp.ns)</code></pre><p>Used to solve <code>mdp</code> with discount factor <code>γ</code>.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/b33948caf525675efc4d1e8fbb11a56f0681b8e0/src/learner/mdplearner.jl#L1-L9">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.policy_iteration!-Tuple{ReinforcementLearning.MDPLearner}" href="#ReinforcementLearning.policy_iteration!-Tuple{ReinforcementLearning.MDPLearner}"><code>ReinforcementLearning.policy_iteration!</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">policy_iteration!(mdplearner::MDPLearner)</code></pre><p>Solve MDP with policy iteration using <a href="#ReinforcementLearning.MDPLearner"><code>MDPLearner</code></a>.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/b33948caf525675efc4d1e8fbb11a56f0681b8e0/src/learner/mdplearner.jl#L68-L72">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.SmallBackups" href="#ReinforcementLearning.SmallBackups"><code>ReinforcementLearning.SmallBackups</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct SmallBackups &lt;: AbstractReinforcementLearner
    ns::Int64 = 10
    na::Int64 = 4
    γ::Float64 = .9
    initvalue::Float64 = Inf64
    maxcount::UInt64 = 3
    minpriority::Float64 = 1e-8
    M::Int64 = 1
    counter::Int64 = 0
    Q::Array{Float64, 2} = zeros(na, ns) .+ initvalue
    V::Array{Float64, 1} = zeros(ns) .+ (initvalue == Inf64 ? 0. : initvalue)
    U::Array{Float64, 1} = zeros(ns) .+ (initvalue == Inf64 ? 0. : initvalue)
    Nsa::Array{Int64, 2} = zeros(Int64, na, ns)
    Ns1a0s0::Array{Dict{Tuple{Int64, Int64}, Int64}, 1} = [Dict{Tuple{Int64, Int64}, Int64}() for _ in 1:ns]
    queue::PriorityQueue = PriorityQueue(Base.Order.Reverse, zip(Int64[], Float64[]))</code></pre><p>See <a href="http://proceedings.mlr.press/v28/vanseijen13.html">Harm Van Seijen, Rich Sutton ; Proceedings of the 30th International Conference on Machine Learning, PMLR 28(3):361-369, 2013.</a></p><p><code>maxcount</code> defines the maximal number of backups per action, <code>minpriority</code> is the smallest priority still added to the queue.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/b33948caf525675efc4d1e8fbb11a56f0681b8e0/src/learner/prioritizedsweeping.jl#L1-L22">source</a></section><h2><a class="nav-anchor" id="Deep-Reinforcement-Learning-1" href="#Deep-Reinforcement-Learning-1">Deep Reinforcement Learning</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.DQN" href="#ReinforcementLearning.DQN"><code>ReinforcementLearning.DQN</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct DQN{Tnet,TnetT,ToptT,Topt}
    γ::Float64 = .99
    na::Int64
    net::TnetT
    targetnet::Tnet = Flux.mapleaves(Flux.Tracker.data, deepcopy(net))
    policynet::Tnet = Flux.mapleaves(Flux.Tracker.data, net)
    updatetargetevery::Int64 = 500
    t::Int64 = 0
    updateevery::Int64 = 1
    opttype::ToptT = Flux.ADAM
    opt::Topt = opttype(Flux.params(net))
    startlearningat::Int64 = 10^3
    minibatchsize::Int64 = 32
    doubledqn::Bool = true
    nmarkov::Int64 = 1
    nsteps::Int64 = 1
    replaysize::Int64 = 10^4
    loss::Function = Flux.mse</code></pre></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/b33948caf525675efc4d1e8fbb11a56f0681b8e0/src/learner/dqn.jl#L1-L20">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.DeepActorCritic" href="#ReinforcementLearning.DeepActorCritic"><code>ReinforcementLearning.DeepActorCritic</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct DeepActorCritic{Tnet, Tpl, Tplm, Tvl, ToptT, Topt}
    nh::Int64 = 4
    na::Int64 = 2
    γ::Float64 = .9
    nsteps::Int64 = 5
    net::Tnet
    policylayer::Tpl = Linear(nh, na)
    policynet::Tplm = Flux.Chain(Flux.mapleaves(Flux.Tracker.data, net),
                             Flux.mapleaves(Flux.Tracker.data, policylayer))
    valuelayer::Tvl = Linear(nh, 1)
    params::Array{Any, 1} = vcat(map(Flux.params, [net, policylayer, valuelayer])...)
    t::Int64 = 0
    updateevery::Int64 = 1
    opttype::ToptT = Flux.ADAM
    opt::Topt = opttype(params)
    αcritic::Float64 = .1
    nmarkov::Int64 = 1</code></pre></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/b33948caf525675efc4d1e8fbb11a56f0681b8e0/src/learner/deepactorcritic.jl#L1-L19">source</a></section><footer><hr/><a class="previous" href="../learning/"><span class="direction">Previous</span><span class="title">Learning</span></a><a class="next" href="../buffers/"><span class="direction">Next</span><span class="title">Buffers</span></a></footer></article></body></html>
