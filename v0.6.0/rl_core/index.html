<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>RLCore · ReinforcementLearning.jl</title><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-149861753-1', 'auto');
ga('send', 'pageview', {'page': location.pathname + location.search + location.hash});
</script><link rel="canonical" href="https://juliareinforcementlearning.github.io/ReinforcementLearning.jl/latest/rl_core/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../assets/custom.css" rel="stylesheet" type="text/css"/><link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" rel="stylesheet" type="text/css"/></head><body><div id="top" class="navbar-wrapper">
<nav class="navbar navbar-expand-lg  navbar-dark fixed-top" style="background-color: #1fd1f9; background-image: linear-gradient(315deg, #1fd1f9 0%, #b621fe 74%); " id="mainNav">
  <div class="container-md">
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarTogglerDemo01" aria-controls="navbarTogglerDemo01" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>
  <div class="collapse navbar-collapse" id="navbarTogglerDemo01">
    <span class="navbar-brand">
        <a class="navbar-brand" href="/">
          <!-- <img src="/assets/site/logo.svg" width="30" height="30" alt="logo" loading="lazy"> -->
          JuliaReinforcementLearning
        </a>
    </span>

    <ul class="navbar-nav ml-auto">
        <li class="nav-item">
        <a class="nav-link" href="/get_started/">Get Started</a>
        </li>
        <li class="nav-item">
        <a class="nav-link" href="/guide/">Guide</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/contribute/">Contribute</a>
        </li>
        <li class="nav-item">
        <a class="nav-link" href="/blog/">Blog</a>
        </li>
        <li class="nav-item">
        <a class="nav-link" href="https://JuliaReinforcementLearning.github.io/ReinforcementLearning.jl/latest/">Doc</a>
        </li>
        <li class="nav-item">
        <a class="nav-link" href="https://github.com/JuliaReinforcementLearning">Github</a>
        </li>
    </ul>
  </div>
</nav>
</div>
<div class="documenter-wrapper" id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="ReinforcementLearning.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">ReinforcementLearning.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../rl_base/">RLBase</a></li><li class="is-active"><a class="tocitem" href>RLCore</a></li><li><a class="tocitem" href="../rl_envs/">RLEnvs</a></li><li><a class="tocitem" href="../rl_zoo/">RLZoo</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Manual</a></li><li class="is-active"><a href>RLCore</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>RLCore</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/master/docs/src/rl_core.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="ReinforcementLearningCore.jl"><a class="docs-heading-anchor" href="#ReinforcementLearningCore.jl">ReinforcementLearningCore.jl</a><a id="ReinforcementLearningCore.jl-1"></a><a class="docs-heading-anchor-permalink" href="#ReinforcementLearningCore.jl" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.RLCore" href="#ReinforcementLearningCore.RLCore"><code>ReinforcementLearningCore.RLCore</code></a> — <span class="docstring-category">Module</span></header><section><div><p><a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningCore.jl">ReinforcementLearningCore.jl</a> (<strong>RLCore</strong>) provides some standard and reusable components defined by <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningBase.jl"><strong>RLBase</strong></a>, hoping that they are useful for people to implement and experiment with different kinds of algorithms.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.RTSA" href="#ReinforcementLearningCore.RTSA"><code>ReinforcementLearningCore.RTSA</code></a> — <span class="docstring-category">Constant</span></header><section><div><p>An alias of <code>(:reward, :terminal, :state, :action)</code></p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.SARTSA" href="#ReinforcementLearningCore.SARTSA"><code>ReinforcementLearningCore.SARTSA</code></a> — <span class="docstring-category">Constant</span></header><section><div><p>An alias of <code>(:state, :action, :reward, :terminal, :next_state, :next_action)</code></p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.AbstractAgent" href="#ReinforcementLearningCore.AbstractAgent"><code>ReinforcementLearningCore.AbstractAgent</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">(agent::AbstractAgent)(env) = agent(PRE_ACT_STAGE, env) -&gt; action
(agent::AbstractAgent)(stage::AbstractStage, env)</code></pre><p>Similar to <a href="../rl_base/#ReinforcementLearningBase.AbstractPolicy"><code>AbstractPolicy</code></a>, an agent is also a functional object which takes in an observation and returns an action. The main difference is that, we divide an experiment into the following stages:</p><ul><li><code>PRE_EXPERIMENT_STAGE</code></li><li><code>PRE_EPISODE_STAGE</code></li><li><code>PRE_ACT_STAGE</code></li><li><code>POST_ACT_STAGE</code></li><li><code>POST_EPISODE_STAGE</code></li><li><code>POST_EXPERIMENT_STAGE</code></li></ul><p>In each stage, different types of agents may have different behaviors, like updating experience buffer, environment model or policy.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.AbstractApproximator" href="#ReinforcementLearningCore.AbstractApproximator"><code>ReinforcementLearningCore.AbstractApproximator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">(app::AbstractApproximator)(env)</code></pre><p>An approximator is a functional object for value estimation. It serves as a black box to provides an abstraction over different  kinds of approximate methods (for example DNN provided by Flux or Knet).</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.AbstractExplorer" href="#ReinforcementLearningCore.AbstractExplorer"><code>ReinforcementLearningCore.AbstractExplorer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">(p::AbstractExplorer)(x)
(p::AbstractExplorer)(x, mask)</code></pre><p>Define how to select an action based on action values.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.AbstractHook" href="#ReinforcementLearningCore.AbstractHook"><code>ReinforcementLearningCore.AbstractHook</code></a> — <span class="docstring-category">Type</span></header><section><div><p>A hook is called at different stage duiring a <a href="../rl_base/#Base.run-Tuple{Any,AbstractEnv}"><code>run</code></a> to allow users to inject customized runtime logic. By default, a <code>AbstractHook</code> will do nothing. One can override the behavior by implementing the following methods:</p><ul><li><code>(hook::YourHook)(::PreActStage, agent, env, action)</code>, note that there&#39;s an extra argument of <code>action</code>.</li><li><code>(hook::YourHook)(::PostActStage, agent, env)</code></li><li><code>(hook::YourHook)(::PreEpisodeStage, agent, env)</code></li><li><code>(hook::YourHook)(::PostEpisodeStage, agent, env)</code></li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.AbstractLearner" href="#ReinforcementLearningCore.AbstractLearner"><code>ReinforcementLearningCore.AbstractLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">(learner::AbstractLearner)(env)</code></pre><p>A learner is usually used to estimate state values, state-action values or distributional values based on experiences.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.AbstractStage" href="#ReinforcementLearningCore.AbstractStage"><code>ReinforcementLearningCore.AbstractStage</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">                  +-----------------------------------------------------------+                      
                  |Episode                                                    |                      
                  |                                                           |</code></pre><p>PRE<em>EXPERIMENT</em>STAGE  |            PRE<em>ACT</em>STAGE    POST<em>ACT</em>STAGE                | POST<em>EXPERIMENT</em>STAGE          |            |                  |                |                       |          |                     v            |        +––-+   v   +–––-+    v   +––-+             |          v                     ––––––––––-&gt;+ env +–––&gt;+ agent +–––-&gt;+ env +–-&gt; ... –––-&gt;......                                    |  ^     +––-+       +–––-+ action +––-+          ^  |                                             |  |                                                     |  |                                             |  +–PRE<em>EPISODE</em>STAGE            POST<em>EPISODE</em>STAGE––+  |                                             |                                                           |                                             |                                                           |                                             +–––––––––––––––––––––––––––––-+     </p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.AbstractTrajectory" href="#ReinforcementLearningCore.AbstractTrajectory"><code>ReinforcementLearningCore.AbstractTrajectory</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">AbstractTrajectory{names,types} &lt;: AbstractArray{NamedTuple{names,types},1}</code></pre><p>A trajectory is used to record some useful information during the interactions between agents and environments.</p><p><strong>Parameters</strong></p><ul><li><code>names</code>::<code>NTuple{Symbol}</code>, indicate what fields to be recorded.</li><li><code>types</code>::<code>Tuple{DataType...}</code>, the datatypes of <code>names</code>.</li></ul><p>The length of <code>names</code> and <code>types</code> must match.</p><p>Required Methods:</p><ul><li><a href="#ReinforcementLearningCore.get_trace-Tuple{AbstractTrajectory,Vararg{Symbol,N} where N}"><code>get_trace</code></a></li><li><code>Base.push!(t::AbstractTrajectory, kv::Pair{Symbol})</code></li><li><code>Base.pop!(t::AbstractTrajectory, s::Symbol)</code></li></ul><p>Optional Methods:</p><ul><li><code>Base.length</code></li><li><code>Base.size</code></li><li><code>Base.lastindex</code></li><li><code>Base.isempty</code></li><li><code>Base.empty!</code></li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.ActorCritic" href="#ReinforcementLearningCore.ActorCritic"><code>ReinforcementLearningCore.ActorCritic</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ActorCritic(;actor, critic, optimizer=ADAM())</code></pre><p>The <code>actor</code> part must return logits (<em>Do not use softmax in the last layer!</em>), and the <code>critic</code> part must return a state value.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L4">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.Agent" href="#ReinforcementLearningCore.Agent"><code>ReinforcementLearningCore.Agent</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Agent(;kwargs...)</code></pre><p>One of the most commonly used <a href="#ReinforcementLearningCore.AbstractAgent"><code>AbstractAgent</code></a>.</p><p>Generally speaking, it does nothing but update the trajectory and policy appropriately in different stages.</p><p><strong>Keywords &amp; Fields</strong></p><ul><li><code>policy</code>::<a href="../rl_base/#ReinforcementLearningBase.AbstractPolicy"><code>AbstractPolicy</code></a>: the policy to use</li><li><code>trajectory</code>::<a href="#ReinforcementLearningCore.AbstractTrajectory"><code>AbstractTrajectory</code></a>: used to store transitions between an agent and an environment</li><li><code>role=RLBase.DEFAULT_PLAYER</code>: used to distinguish different agents</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.BatchExplorer" href="#ReinforcementLearningCore.BatchExplorer"><code>ReinforcementLearningCore.BatchExplorer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">BatchExplorer(explorer::AbstractExplorer)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.BatchExplorer-Tuple{AbstractArray{T,2} where T}" href="#ReinforcementLearningCore.BatchExplorer-Tuple{AbstractArray{T,2} where T}"><code>ReinforcementLearningCore.BatchExplorer</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">(x::BatchExplorer)(values::AbstractMatrix)</code></pre><p>Apply inner explorer to each column of <code>values</code>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.BatchStepsPerEpisode-Tuple{Int64}" href="#ReinforcementLearningCore.BatchStepsPerEpisode-Tuple{Int64}"><code>ReinforcementLearningCore.BatchStepsPerEpisode</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">BatchStepsPerEpisode(batch_size::Int; tag = &quot;TRAINING&quot;)</code></pre><p>Similar to <a href="#ReinforcementLearningCore.StepsPerEpisode"><code>StepsPerEpisode</code></a>, but only work for <a href="../rl_base/#ReinforcementLearningBase.MultiThreadEnv"><code>MultiThreadEnv</code></a></p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.CircularArrayBuffer" href="#ReinforcementLearningCore.CircularArrayBuffer"><code>ReinforcementLearningCore.CircularArrayBuffer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">CircularArrayBuffer{T}(d::Integer...) -&gt; CircularArrayBuffer{T, N}</code></pre><p><code>CircularArrayBuffer</code> uses a <code>N</code>-dimention <code>Array</code> of size <code>d</code> to serve as a buffer for <code>N-1</code>-dimention <code>Array</code>s with the same size.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; b = CircularArrayBuffer{Float64}(2, 2, 3)
2×2×0 CircularArrayBuffer{Float64,3}

julia&gt; capacity(b)
3

julia&gt; length(b)
0

julia&gt; push!(b, [1. 1.; 2. 2.])
2×2×1 CircularArrayBuffer{Float64,3}:
[:, :, 1] =
 1.0  1.0
 2.0  2.0

julia&gt; b
2×2×1 CircularArrayBuffer{Float64,3}:
[:, :, 1] =
 1.0  1.0
 2.0  2.0

julia&gt; length(b)
4

julia&gt; nframes(cb::CircularArrayBuffer) = cb.length
nframes (generic function with 1 method)

julia&gt; nframes(b)
1

julia&gt; ones(2,2)
2×2 Array{Float64,2}:
 1.0  1.0
 1.0  1.0

julia&gt; 3 .* ones(2,2)
2×2 Array{Float64,2}:
 3.0  3.0
 3.0  3.0

julia&gt; 3 * ones(2,2)
2×2 Array{Float64,2}:
 3.0  3.0
 3.0  3.0

julia&gt; b = CircularArrayBuffer{Float64}(2, 2, 3)
2×2×0 CircularArrayBuffer{Float64,3}

julia&gt; capacity(b)
3

julia&gt; nframes(b)
0

julia&gt; push!(b, 1 .* ones(2,2))
2×2×1 CircularArrayBuffer{Float64,3}:
[:, :, 1] =
 1.0  1.0
 1.0  1.0

julia&gt; b
2×2×1 CircularArrayBuffer{Float64,3}:
[:, :, 1] =
 1.0  1.0
 1.0  1.0

julia&gt; nframes(b)
1

julia&gt; for i in 2:4
           push!(b, i .* ones(2,2))
       end

julia&gt; b
2×2×3 CircularArrayBuffer{Float64,3}:
[:, :, 1] =
 2.0  2.0
 2.0  2.0

[:, :, 2] =
 3.0  3.0
 3.0  3.0

[:, :, 3] =
 4.0  4.0
 4.0  4.0

julia&gt; isfull(b)
true

julia&gt; nframes(b)
3

julia&gt; size(b)
(2, 2, 3)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.CircularCompactPSARTSATrajectory-Tuple{}" href="#ReinforcementLearningCore.CircularCompactPSARTSATrajectory-Tuple{}"><code>ReinforcementLearningCore.CircularCompactPSARTSATrajectory</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">CircularCompactPSARTSATrajectory(;kwargs)</code></pre><p>Similar to <a href="#ReinforcementLearningCore.CircularCompactSARTSATrajectory-Tuple{}"><code>CircularCompactSARTSATrajectory</code></a>, except that another trace named <code>priority</code> is added.</p><p><strong>Key word arguments</strong></p><ul><li><code>capacity::Int</code>, the maximum length of each trace.</li><li><code>state_type = Int</code></li><li><code>state_size = ()</code></li><li><code>action_type = Int</code></li><li><code>action_size = ()</code></li><li><code>reward_type = Float32</code></li><li><code>reward_size = ()</code></li><li><code>terminal_type = Bool</code></li><li><code>terminal_size = ()</code></li><li><code>priority_type = Float32</code></li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.CircularCompactSARTSATrajectory-Tuple{}" href="#ReinforcementLearningCore.CircularCompactSARTSATrajectory-Tuple{}"><code>ReinforcementLearningCore.CircularCompactSARTSATrajectory</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">CircularCompactSARTSATrajectory(;kwargs...)</code></pre><p>Similar to <a href="#ReinforcementLearningCore.VectorialCompactSARTSATrajectory-Tuple{}"><code>VectorialCompactSARTSATrajectory</code></a>, instead of using <code>Vector</code>s as containers, <a href="#ReinforcementLearningCore.CircularArrayBuffer"><code>CircularArrayBuffer</code></a>s are used here.</p><p><strong>Key word arguments</strong></p><ul><li><code>capacity</code>::Int, the maximum length of each trace.</li><li><code>state_type</code> = Int</li><li><code>state_size</code> = ()</li><li><code>action_type</code> = Int</li><li><code>action_size</code> = ()</li><li><code>reward_type</code> = Float32</li><li><code>reward_size</code> = ()</li><li><code>terminal_type</code> = Bool</li><li><code>terminal_size</code> = ()</li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.CircularTrajectory-Tuple{}" href="#ReinforcementLearningCore.CircularTrajectory-Tuple{}"><code>ReinforcementLearningCore.CircularTrajectory</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">CircularTrajectory(; capacity, trace_name=eltype=&gt;size...)</code></pre><p>Similar to <a href="#ReinforcementLearningCore.VectorialTrajectory-Tuple{}"><code>VectorialTrajectory</code></a>, but we use the <a href="#ReinforcementLearningCore.CircularArrayBuffer"><code>CircularArrayBuffer</code></a> to store the traces. The <code>capacity</code> here is used to specify the maximum length of the trajectory.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl">julia&gt; t = CircularTrajectory(capacity=10, state=Float64=&gt;(3,3), reward=Int=&gt;tuple())
0-element Trajectory{(:state, :reward),Tuple{Float64,Int64},NamedTuple{(:state, :reward),Tuple{CircularArrayBuffer{Float64,3},CircularArrayBuffer{Int64,1}}}}

julia&gt; push!(t,state=rand(3,3), reward=1)

julia&gt; push!(t,state=rand(3,3), reward=2)

julia&gt; get_trace(t, :reward)
2-element CircularArrayBuffer{Int64,1}:
 1
 2

julia&gt; get_trace(t, :state)
3×3×2 CircularArrayBuffer{Float64,3}:
[:, :, 1] =
 0.699906  0.382396   0.927411
 0.269807  0.0581324  0.239609
 0.222304  0.514408   0.318905

[:, :, 2] =
 0.956228  0.992505  0.109743
 0.763497  0.381387  0.540566
 0.223081  0.834308  0.634759

julia&gt; pop!(t)

julia&gt; get_trace(t, :state)
3×3×1 CircularArrayBuffer{Float64,3}:
[:, :, 1] =
 0.699906  0.382396   0.927411
 0.269807  0.0581324  0.239609
 0.222304  0.514408   0.318905
</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.ComposedHook" href="#ReinforcementLearningCore.ComposedHook"><code>ReinforcementLearningCore.ComposedHook</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ComposedHook(hooks::AbstractHook...)</code></pre><p>Compose different hooks into a single hook.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.ComposedStopCondition" href="#ReinforcementLearningCore.ComposedStopCondition"><code>ReinforcementLearningCore.ComposedStopCondition</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ComposedStopCondition(stop_conditions; reducer = any)</code></pre><p>The result of <code>stop_conditions</code> is reduced by <code>reducer</code>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.CumulativeReward" href="#ReinforcementLearningCore.CumulativeReward"><code>ReinforcementLearningCore.CumulativeReward</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">CumulativeReward(rewards::Vector{Float64} = [0.0])</code></pre><p>Store cumulative rewards since the beginning to the field of <code>rewards</code>.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>If the environment is a <a href="@ref"><code>RewardOverriddenEnv</code></a>, then the original reward is recorded instead.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L7">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.DoEveryNEpisode" href="#ReinforcementLearningCore.DoEveryNEpisode"><code>ReinforcementLearningCore.DoEveryNEpisode</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">DoEveryNEpisode(f; n=1, t=0)</code></pre><p>Execute <code>f(agent, env)</code> every <code>n</code> episode. <code>t</code> is a counter of steps.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.DoEveryNStep" href="#ReinforcementLearningCore.DoEveryNStep"><code>ReinforcementLearningCore.DoEveryNStep</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">DoEveryNStep(f; n=1, t=0)</code></pre><p>Execute <code>f(agent, env)</code> every <code>n</code> step. <code>t</code> is a counter of steps.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.DynaAgent" href="#ReinforcementLearningCore.DynaAgent"><code>ReinforcementLearningCore.DynaAgent</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">DynaAgent(;kwargs...)</code></pre><p><code>DynaAgent</code> is first introduced in: <em>Sutton, Richard S. &quot;Dyna, an integrated architecture for learning, planning, and reacting.&quot; ACM Sigart Bulletin 2.4 (1991): 160-163.</em></p><p><strong>Keywords &amp; Fields</strong></p><ul><li><code>policy</code>::<a href="../rl_base/#ReinforcementLearningBase.AbstractPolicy"><code>AbstractPolicy</code></a>: the policy to use</li><li><code>model</code>::<a href="../rl_base/#ReinforcementLearningBase.AbstractEnvironmentModel"><code>AbstractEnvironmentModel</code></a>: describe the environment to interact with</li><li><code>trajectory</code>::<a href="#ReinforcementLearningCore.AbstractTrajectory"><code>AbstractTrajectory</code></a>: used to store transitions between agent and environment</li><li><code>role=:DEFAULT</code>: used to distinguish different agents</li><li><code>plan_step::Int=10</code>: the count of planning steps</li></ul><p>The main difference between <a href="#ReinforcementLearningCore.DynaAgent"><code>DynaAgent</code></a> and <a href="#ReinforcementLearningCore.Agent"><code>Agent</code></a> is that an environment model is involved. It is best described in the book: <em>Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</em></p><p><img src="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/raw/master/docs/src/assets/img/RL_book_fig_8_1.png" alt/> <img src="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/raw/master/docs/src/assets/img/RL_book_fig_8_2.png" alt/></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L17">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.EmptyHook" href="#ReinforcementLearningCore.EmptyHook"><code>ReinforcementLearningCore.EmptyHook</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Do nothing</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.EpisodicCompactSARTSATrajectory" href="#ReinforcementLearningCore.EpisodicCompactSARTSATrajectory"><code>ReinforcementLearningCore.EpisodicCompactSARTSATrajectory</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">EpisodicCompactSARTSATrajectory(; state_type = Int, action_type = Int, reward_type = Float32, terminal_type = Bool)</code></pre><p>Exactly the same with <a href="#ReinforcementLearningCore.VectorialCompactSARTSATrajectory-Tuple{}"><code>VectorialCompactSARTSATrajectory</code></a>. It only exists for multiple dispatch purpose.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>The <code>EpisodicCompactSARTSATrajectory</code> will not be automatically emptified when reaching the end of an episode.</p></div></div></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.EpsilonGreedyExplorer" href="#ReinforcementLearningCore.EpsilonGreedyExplorer"><code>ReinforcementLearningCore.EpsilonGreedyExplorer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">EpsilonGreedyExplorer{T}(;kwargs...)
EpsilonGreedyExplorer(ϵ) -&gt; EpsilonGreedyExplorer{:linear}(; ϵ_stable = ϵ)</code></pre><blockquote><p>Epsilon-greedy strategy: The best lever is selected for a proportion <code>1 - epsilon</code> of the trials, and a lever is selected at random (with uniform probability) for a proportion epsilon . <a href="https://en.wikipedia.org/wiki/Multi-armed_bandit">Multi-armed_bandit</a></p></blockquote><p>Two kinds of epsilon-decreasing strategy are implmented here (<code>linear</code> and <code>exp</code>).</p><blockquote><p>Epsilon-decreasing strategy: Similar to the epsilon-greedy strategy, except that the value of epsilon decreases as the experiment progresses, resulting in highly explorative behaviour at the start and highly exploitative behaviour at the finish.  - <a href="https://en.wikipedia.org/wiki/Multi-armed_bandit">Multi-armed_bandit</a></p></blockquote><p><strong>Keywords</strong></p><ul><li><code>T::Symbol</code>: defines how to calculate the epsilon in the warmup steps. Supported values are <code>linear</code> and <code>exp</code>.</li><li><code>step::Int = 1</code>: record the current step.</li><li><code>ϵ_init::Float64 = 1.0</code>: initial epsilon.</li><li><code>warmup_steps::Int=0</code>: the number of steps to use <code>ϵ_init</code>.</li><li><code>decay_steps::Int=0</code>: the number of steps for epsilon to decay from <code>ϵ_init</code> to <code>ϵ_stable</code>.</li><li><code>ϵ_stable::Float64</code>: the epsilon after <code>warmup_steps + decay_steps</code>.</li><li><code>is_break_tie=false</code>: randomly select an action of the same maximum values if set to <code>true</code>.</li><li><code>rng=Random.GLOBAL_RNG</code>: set the internal RNG.</li><li><code>is_training=true</code>, in training mode, <code>step</code> will not be updated. And the <code>ϵ</code> will be set to 0.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia">s = EpsilonGreedyExplorer{:linear}(ϵ_init=0.9, ϵ_stable=0.1, warmup_steps=100, decay_steps=100)
plot([RL.get_ϵ(s, i) for i in 1:500], label=&quot;linear epsilon&quot;)</code></pre><p><img src="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/raw/master/docs/src/assets/img/linear_epsilon_greedy_selector.png" alt/></p><pre><code class="language-julia">s = EpsilonGreedyExplorer{:exp}(ϵ_init=0.9, ϵ_stable=0.1, warmup_steps=100, decay_steps=100)
plot([RL.get_ϵ(s, i) for i in 1:500], label=&quot;exp epsilon&quot;)</code></pre><p><img src="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/raw/master/docs/src/assets/img/exp_epsilon_greedy_selector.png" alt/></p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.EpsilonGreedyExplorer-Tuple{Any}" href="#ReinforcementLearningCore.EpsilonGreedyExplorer-Tuple{Any}"><code>ReinforcementLearningCore.EpsilonGreedyExplorer</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">(s::EpsilonGreedyExplorer)(values; step) where T</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>If multiple values with the same maximum value are found. Then a random one will be returned!</p><p><code>NaN</code> will be filtered unless all the values are <code>NaN</code>. In that case, a random one will be returned.</p></div></div></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.NeuralNetworkApproximator" href="#ReinforcementLearningCore.NeuralNetworkApproximator"><code>ReinforcementLearningCore.NeuralNetworkApproximator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">NeuralNetworkApproximator(;kwargs)</code></pre><p>Use a DNN model for value estimation.</p><p><strong>Keyword arguments</strong></p><ul><li><code>model</code>, a Flux based DNN model.</li><li><code>optimizer=nothing</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L9">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.OffPolicy" href="#ReinforcementLearningCore.OffPolicy"><code>ReinforcementLearningCore.OffPolicy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">OffPolicy(π_target::P, π_behavior::B) -&gt; OffPolicy{P,B}</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L2">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.QBasedPolicy" href="#ReinforcementLearningCore.QBasedPolicy"><code>ReinforcementLearningCore.QBasedPolicy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">QBasedPolicy(;learner::Q, explorer::S)</code></pre><p>Use a Q-<code>learner</code> to generate estimations of action values. Then an <code>explorer</code> is applied on the estimations to select an action.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.ResizeImage" href="#ReinforcementLearningCore.ResizeImage"><code>ReinforcementLearningCore.ResizeImage</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ResizeImage(img::Array{T, N})
ResizeImage(dims::Int...) -&gt; ResizeImage(Float32, dims...)
ResizeImage(T::Type{&lt;:Number}, dims::Int...)</code></pre><p>Using BSpline method to resize the <code>state</code> field of an observation to size of <code>img</code> (or <code>dims</code>).</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.RewardsPerEpisode" href="#ReinforcementLearningCore.RewardsPerEpisode"><code>ReinforcementLearningCore.RewardsPerEpisode</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">RewardsPerEpisode(; rewards = Vector{Vector{Float64}}())</code></pre><p>Store each reward of each step in every episode in the field of <code>rewards</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L4">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.StackFrames" href="#ReinforcementLearningCore.StackFrames"><code>ReinforcementLearningCore.StackFrames</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">StackFrames(::Type{T}=Float32, d::Int...)</code></pre><p>Use a pre-initialized <a href="#ReinforcementLearningCore.CircularArrayBuffer"><code>CircularArrayBuffer</code></a> to store the latest several states specified by <code>d</code>. Before processing any observation, the buffer is filled with <code>zero{T}</code>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.StepsPerEpisode" href="#ReinforcementLearningCore.StepsPerEpisode"><code>ReinforcementLearningCore.StepsPerEpisode</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">StepsPerEpisode(; steps = Int[], count = 0)</code></pre><p>Store steps of each episode in the field of <code>steps</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L4">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.StopAfterEpisode" href="#ReinforcementLearningCore.StopAfterEpisode"><code>ReinforcementLearningCore.StopAfterEpisode</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">StopAfterEpisode(episode; cur = 0, is_show_progress = true)</code></pre><p>Return <code>true</code> after being called <code>episode</code>. If <code>is_show_progress</code> is <code>true</code>, the <code>ProgressMeter</code> will be used to show progress.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.StopAfterStep" href="#ReinforcementLearningCore.StopAfterStep"><code>ReinforcementLearningCore.StopAfterStep</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">StopAfterStep(step; cur = 1, is_show_progress = true)</code></pre><p>Return <code>true</code> after being called <code>step</code> times.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.StopWhenDone" href="#ReinforcementLearningCore.StopWhenDone"><code>ReinforcementLearningCore.StopWhenDone</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">StopWhenDone()</code></pre><p>Return <code>true</code> if the environment is terminated.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.SumTree" href="#ReinforcementLearningCore.SumTree"><code>ReinforcementLearningCore.SumTree</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">SumTree(capacity::Int)</code></pre><p>Efficiently sample and update weights. For more detals, see the post at <a href="https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/">here</a>. Here we use a vector to represent the binary tree. Suppose we will have <code>capacity</code> leaves at most. Every time we <code>push!</code> new node into the tree, only the recent <code>capacity</code> node and their sum will be updated! [––––––Parent nodes––––––][––––leaves––––] [size: 2^ceil(Int, log2(capacity))-1 ][     size: capacity   ]</p><p><strong>Example</strong></p><pre><code class="language-julia">julia&gt; t = SumTree(8)
0-element SumTree
julia&gt; for i in 1:16
       push!(t, i)
       end
julia&gt; t
8-element SumTree:
  9.0
 10.0
 11.0
 12.0
 13.0
 14.0
 15.0
 16.0
julia&gt; sample(t)
(2, 10.0)
julia&gt; sample(t)
(1, 9.0)
julia&gt; inds, ps = sample(t,100000)
([8, 4, 8, 1, 5, 2, 2, 7, 6, 6  …  1, 1, 7, 1, 6, 1, 5, 7, 2, 7], [16.0, 12.0, 16.0, 9.0, 13.0, 10.0, 10.0, 15.0, 14.0, 14.0  …  9.0, 9.0, 15.0, 9.0, 14.0, 9.0, 13.0, 15.0, 10.0, 15.0])
julia&gt; countmap(inds)
Dict{Int64,Int64} with 8 entries:
  7 =&gt; 14991
  4 =&gt; 12019
  2 =&gt; 10003
  3 =&gt; 11027
  5 =&gt; 12971
  8 =&gt; 16052
  6 =&gt; 13952
  1 =&gt; 8985
julia&gt; countmap(ps)
Dict{Float64,Int64} with 8 entries:
  9.0  =&gt; 8985
  13.0 =&gt; 12971
  10.0 =&gt; 10003
  14.0 =&gt; 13952
  16.0 =&gt; 16052
  11.0 =&gt; 11027
  15.0 =&gt; 14991
  12.0 =&gt; 12019</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.TabularApproximator" href="#ReinforcementLearningCore.TabularApproximator"><code>ReinforcementLearningCore.TabularApproximator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">TabularApproximator(table&lt;:AbstractArray)</code></pre><p>For <code>table</code> of 1-d, it will serve as a state value approximator. For <code>table</code> of 2-d, it will serve as a state-action value approximator.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>For <code>table</code> of 2-d, the first dimension is action and the second dimension is state.</p></div></div></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.TabularApproximator-Tuple{}" href="#ReinforcementLearningCore.TabularApproximator-Tuple{}"><code>ReinforcementLearningCore.TabularApproximator</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">TabularApproximator(; n_state, n_action = nothing, init = 0.0)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.TimePerStep" href="#ReinforcementLearningCore.TimePerStep"><code>ReinforcementLearningCore.TimePerStep</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">TimePerStep(;max_steps=100)
TimePerStep(times::CircularArrayBuffer{Float64}, t::UInt64)</code></pre><p>Store time cost of the latest <code>max_steps</code> in the <code>times</code> field.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.TotalBatchRewardPerEpisode-Tuple{Int64}" href="#ReinforcementLearningCore.TotalBatchRewardPerEpisode-Tuple{Int64}"><code>ReinforcementLearningCore.TotalBatchRewardPerEpisode</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">TotalBatchRewardPerEpisode(batch_size::Int)</code></pre><p>Similar to <a href="#ReinforcementLearningCore.TotalRewardPerEpisode"><code>TotalRewardPerEpisode</code></a>, but will record total rewards per episode in <a href="../rl_base/#ReinforcementLearningBase.MultiThreadEnv"><code>MultiThreadEnv</code></a>.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>If the environment is a <a href="@ref"><code>RewardOverriddenEnv</code></a>, then the original reward is recorded.</p></div></div></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.TotalRewardPerEpisode" href="#ReinforcementLearningCore.TotalRewardPerEpisode"><code>ReinforcementLearningCore.TotalRewardPerEpisode</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">TotalRewardPerEpisode(; rewards = Float64[], reward = 0.0)</code></pre><p>Store the total rewards of each episode in the field of <code>rewards</code>.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>If the environment is a <a href="@ref"><code>RewardOverriddenenv</code></a>, then the original reward is recorded.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L7">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.Trajectory" href="#ReinforcementLearningCore.Trajectory"><code>ReinforcementLearningCore.Trajectory</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Trajectory{names,types,Tbs}(trajectories::Tbs)</code></pre><p>A container of different <code>trajectories</code>. Usually you won&#39;t use it directly.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.UCBExplorer-Tuple{AbstractArray}" href="#ReinforcementLearningCore.UCBExplorer-Tuple{AbstractArray}"><code>ReinforcementLearningCore.UCBExplorer</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">(ucb::UCBExplorer)(values::AbstractArray)</code></pre><p>Unlike <a href="#ReinforcementLearningCore.EpsilonGreedyExplorer"><code>EpsilonGreedyExplorer</code></a>, uncertaintyies are considered in UCB.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>If multiple values with the same maximum value are found. Then a random one will be returned!</p></div></div><div>\[A_t = \underset{a}{\arg \max} \left[ Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \right]\]</div><p>See more details at Section (2.7) on Page 35 of the book <em>Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</em></p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.UCBExplorer-Tuple{Any}" href="#ReinforcementLearningCore.UCBExplorer-Tuple{Any}"><code>ReinforcementLearningCore.UCBExplorer</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">UCBExplorer(na; c=2.0, ϵ=1e-10, step=1, seed=nothing)</code></pre><p><strong>Arguments</strong></p><ul><li><code>na</code> is the number of actions used to create a internal counter.</li><li><code>t</code> is used to store current time step.</li><li><code>c</code> is used to control the degree of exploration.</li><li><code>seed</code>, set the seed of inner RNG.</li><li><code>is_training=true</code>, in training mode, time step and counter will not be updated.</li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.VBasedPolicy" href="#ReinforcementLearningCore.VBasedPolicy"><code>ReinforcementLearningCore.VBasedPolicy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">VBasedPolicy(;learner, mapping, explorer=GreedyExplorer())</code></pre><p><strong>Key words &amp; Fields</strong></p><ul><li><code>learner</code>::<a href="#ReinforcementLearningCore.AbstractLearner"><code>AbstractLearner</code></a>, learn how to estimate state values.</li><li><code>mapping</code>, a customized function <code>(env, learner) -&gt; action_values</code></li><li><code>explorer</code>::<a href="#ReinforcementLearningCore.AbstractExplorer"><code>AbstractExplorer</code></a>, decide which action to take based on action values.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L8">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.VectorialCompactSARTSATrajectory-Tuple{}" href="#ReinforcementLearningCore.VectorialCompactSARTSATrajectory-Tuple{}"><code>ReinforcementLearningCore.VectorialCompactSARTSATrajectory</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">VectorialCompactSARTSATrajectory(; state_type = Int, action_type = Int, reward_type = Float32, terminal_type = Bool)</code></pre><p>This function creates a <a href="#ReinforcementLearningCore.VectorialTrajectory-Tuple{}"><code>VectorialTrajectory</code></a> of <a href="#ReinforcementLearningCore.RTSA"><code>RTSA</code></a> fields. Here the <strong>Compact</strong> in the function name means that, <code>state</code> and <code>next_state</code>, <code>action</code> and <code>next_action</code> reuse a same vector underlying.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl">julia&gt; t = VectorialCompactSARTSATrajectory()
0-element Trajectory{(:state, :action, :reward, :terminal, :next_state, :next_action),Tuple{Int64,Int64,Float32,Bool,Int64,Int64},NamedTuple{(:reward, :terminal, :state, :action),Tuple{Array{Float32,1},Array{Bool,1},Array{Int64,1},Array{Int64,1}}}}

julia&gt; push!(t, state=0, action=0)

julia&gt; push!(t, reward=0.f0, terminal=false, state=1, action=1)

julia&gt; t
1-element Trajectory{(:state, :action, :reward, :terminal, :next_state, :next_action),Tuple{Int64,Int64,Float32,Bool,Int64,Int64},NamedTuple{(:reward, :terminal, :state, :action),Tuple{Array{Float32,1},Array{Bool,1},Array{Int64,1},Array{Int64,1}}}}:
 (state = 0, action = 0, reward = 0.0, terminal = 0, next_state = 1, next_action = 1)

julia&gt; push!(t, reward=1.f0, terminal=true, state=2, action=2)

julia&gt; t
2-element Trajectory{(:state, :action, :reward, :terminal, :next_state, :next_action),Tuple{Int64,Int64,Float32,Bool,Int64,Int64},NamedTuple{(:reward, :terminal, :state, :action),Tuple{Array{Float32,1},Array{Bool,1},Array{Int64,1},Array{Int64,1}}}}:
 (state = 0, action = 0, reward = 0.0, terminal = 0, next_state = 1, next_action = 1)
 (state = 1, action = 1, reward = 1.0, terminal = 1, next_state = 2, next_action = 2)

julia&gt; get_trace(t, :state, :action)
(state = [0, 1], action = [0, 1])

julia&gt; get_trace(t, :next_state, :next_action)
(next_state = [1, 2], next_action = [1, 2])

julia&gt; pop!(t)
1-element Trajectory{(:state, :action, :reward, :terminal, :next_state, :next_action),Tuple{Int64,Int64,Float32,Bool,Int64,Int64},NamedTuple{(:reward, :terminal, :state, :action),Tuple{Array{Float32,1},Array{Bool,1},Array{Int64,1},Array{Int64,1}}}}:
 (state = 0, action = 0, reward = 0.0, terminal = 0, next_state = 1, next_action = 1)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.VectorialTrajectory-Tuple{}" href="#ReinforcementLearningCore.VectorialTrajectory-Tuple{}"><code>ReinforcementLearningCore.VectorialTrajectory</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">VectorialTrajectory(;trace_name=trace_type ...)</code></pre><p>Use <code>Vector</code> to store the traces.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl">julia&gt; t = VectorialTrajectory(;a=Int, b=Symbol)
0-element Trajectory{(:a, :b),Tuple{Int64,Symbol},NamedTuple{(:a, :b),Tuple{Array{Int64,1},Array{Symbol,1}}}}

julia&gt; push!(t, a=0, b=:x)

julia&gt; push!(t, a=1, b=:y)

julia&gt; t
2-element Trajectory{(:a, :b),Tuple{Int64,Symbol},NamedTuple{(:a, :b),Tuple{Array{Int64,1},Array{Symbol,1}}}}:
 (a = 0, b = :x)
 (a = 1, b = :y)

julia&gt; get_trace(t, :b)
2-element Array{Symbol,1}:
 :x
 :y

julia&gt; pop!(t)

julia&gt; t
1-element Trajectory{(:a, :b),Tuple{Int64,Symbol},NamedTuple{(:a, :b),Tuple{Array{Int64,1},Array{Symbol,1}}}}:
 (a = 0, b = :x)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.WeightedExplorer" href="#ReinforcementLearningCore.WeightedExplorer"><code>ReinforcementLearningCore.WeightedExplorer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">WeightedExplorer(;is_normalized::Bool)</code></pre><p><code>is_normalized</code> is used to indicate if the feeded action values are alrady normalized to have a sum of <code>1.0</code>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.getindex-Tuple{Trajectory,Symbol}" href="#Base.getindex-Tuple{Trajectory,Symbol}"><code>Base.getindex</code></a> — <span class="docstring-category">Method</span></header><section><div><p>A helper function to access inner fields</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.pop!-Tuple{AbstractTrajectory,Vararg{Symbol,N} where N}" href="#Base.pop!-Tuple{AbstractTrajectory,Vararg{Symbol,N} where N}"><code>Base.pop!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">Base.pop!(t::AbstractTrajectory, s::Symbol...)</code></pre><p><code>pop!</code> out one element of the traces specified in <code>s</code></p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.pop!-Union{Tuple{AbstractTrajectory{names,types} where types}, Tuple{names}} where names" href="#Base.pop!-Union{Tuple{AbstractTrajectory{names,types} where types}, Tuple{names}} where names"><code>Base.pop!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">Base.pop!(t::AbstractTrajectory{names}) where {names}</code></pre><p><code>pop!</code> out one element of each trace in <code>t</code></p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.push!-Tuple{AbstractTrajectory}" href="#Base.push!-Tuple{AbstractTrajectory}"><code>Base.push!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">Base.push!(t::AbstractTrajectory; kwargs...)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="CUDA.device-Tuple{Any}" href="#CUDA.device-Tuple{Any}"><code>CUDA.device</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">device(model)</code></pre><p>Detect the suitable running device for the <code>model</code>. Return <code>Val(:cpu)</code> by default.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.get_priority-Tuple{AbstractLearner,Any}" href="#ReinforcementLearningBase.get_priority-Tuple{AbstractLearner,Any}"><code>ReinforcementLearningBase.get_priority</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">get_priority(p::AbstractLearner, experience)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.get_prob-Tuple{AbstractExplorer,Any,Any}" href="#ReinforcementLearningBase.get_prob-Tuple{AbstractExplorer,Any,Any}"><code>ReinforcementLearningBase.get_prob</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">get_prob(p::AbstractExplorer, x, mask)</code></pre><p>Similart to <code>get_prob(p::AbstractExplorer, x)</code>, but here only the <code>mask</code>ed elements are considered.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.get_prob-Tuple{AbstractExplorer,Any}" href="#ReinforcementLearningBase.get_prob-Tuple{AbstractExplorer,Any}"><code>ReinforcementLearningBase.get_prob</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">get_prob(p::AbstractExplorer, x) -&gt; AbstractDistribution</code></pre><p>Get the action distribution given action values.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.get_prob-Tuple{EpsilonGreedyExplorer{#s62,true,R} where R where #s62,Any}" href="#ReinforcementLearningBase.get_prob-Tuple{EpsilonGreedyExplorer{#s62,true,R} where R where #s62,Any}"><code>ReinforcementLearningBase.get_prob</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">get_prob(s::EpsilonGreedyExplorer, values) -&gt;Categorical
get_prob(s::EpsilonGreedyExplorer, values, mask) -&gt;Categorical</code></pre><p>Return the probability of selecting each action given the estimated <code>values</code> of each action.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.update!-Tuple{AbstractApproximator,Any}" href="#ReinforcementLearningBase.update!-Tuple{AbstractApproximator,Any}"><code>ReinforcementLearningBase.update!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">update!(a::AbstractApproximator, correction)</code></pre><p>Usually the <code>correction</code> is the gradient of inner parameters.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.update!-Tuple{AbstractEnvironmentModel,AbstractTrajectory,AbstractPolicy}" href="#ReinforcementLearningBase.update!-Tuple{AbstractEnvironmentModel,AbstractTrajectory,AbstractPolicy}"><code>ReinforcementLearningBase.update!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>By default, only use trajectory to update model</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.update!-Union{Tuple{N}, Tuple{T}, Tuple{CircularArrayBuffer{T,N},AbstractArray}} where N where T" href="#ReinforcementLearningBase.update!-Union{Tuple{N}, Tuple{T}, Tuple{CircularArrayBuffer{T,N},AbstractArray}} where N where T"><code>ReinforcementLearningBase.update!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">update!(cb::CircularArrayBuffer{T,N}, data::AbstractArray)</code></pre><p><code>update!</code> the last frame of <code>cb</code> with data.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.ApproximatorStyle-Tuple{AbstractApproximator}" href="#ReinforcementLearningCore.ApproximatorStyle-Tuple{AbstractApproximator}"><code>ReinforcementLearningCore.ApproximatorStyle</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Used to detect what an <a href="#ReinforcementLearningCore.AbstractApproximator"><code>AbstractApproximator</code></a> is approximating.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore._discount_rewards!-Tuple{Any,Any,Any,Any,Nothing}" href="#ReinforcementLearningCore._discount_rewards!-Tuple{Any,Any,Any,Any,Nothing}"><code>ReinforcementLearningCore._discount_rewards!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>assuming rewards and new_rewards are Vector</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore._generalized_advantage_estimation!-NTuple{6,Any}" href="#ReinforcementLearningCore._generalized_advantage_estimation!-NTuple{6,Any}"><code>ReinforcementLearningCore._generalized_advantage_estimation!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>assuming rewards and advantages are Vector</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.discount_rewards-Union{Tuple{T}, Tuple{Union{AbstractArray{T,2} where T, AbstractArray{T,1} where T},T}} where T&lt;:Number" href="#ReinforcementLearningCore.discount_rewards-Union{Tuple{T}, Tuple{Union{AbstractArray{T,2} where T, AbstractArray{T,1} where T},T}} where T&lt;:Number"><code>ReinforcementLearningCore.discount_rewards</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">discount_rewards(rewards::VectorOrMatrix, γ::Number;kwargs...)</code></pre><p>Calculate the gain started from the current step with discount rate of <code>γ</code>. <code>rewards</code> can be a matrix.</p><p><strong>Keyword argments</strong></p><ul><li><code>dims=:</code>, if <code>rewards</code> is a <code>Matrix</code>, then <code>dims</code> can only be <code>1</code> or <code>2</code>.</li><li><code>terminal=nothing</code>, specify if each reward follows by a terminal. <code>nothing</code> means the game is not terminated yet. If <code>terminal</code> is provided, then the size must be the same with <code>rewards</code>.</li><li><code>init=nothing</code>, <code>init</code> can be used to provide the the reward estimation of the last state.</li></ul><p><strong>Example</strong></p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.flatten_batch-Tuple{AbstractArray}" href="#ReinforcementLearningCore.flatten_batch-Tuple{AbstractArray}"><code>ReinforcementLearningCore.flatten_batch</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">flatten_batch(x::AbstractArray)</code></pre><p>Merge the last two dimension.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl">julia&gt; x = reshape(1:12, 2, 2, 3)
2×2×3 reshape(::UnitRange{Int64}, 2, 2, 3) with eltype Int64:
[:, :, 1] =
 1  3
 2  4

[:, :, 2] =
 5  7
 6  8

[:, :, 3] =
  9  11
 10  12

julia&gt; flatten_batch(x)
2×6 reshape(::UnitRange{Int64}, 2, 6) with eltype Int64:
 1  3  5  7   9  11
 2  4  6  8  10  12</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.generalized_advantage_estimation-Union{Tuple{T}, Tuple{Union{AbstractArray{T,2} where T, AbstractArray{T,1} where T},Union{AbstractArray{T,2} where T, AbstractArray{T,1} where T},T,T}} where T&lt;:Number" href="#ReinforcementLearningCore.generalized_advantage_estimation-Union{Tuple{T}, Tuple{Union{AbstractArray{T,2} where T, AbstractArray{T,1} where T},Union{AbstractArray{T,2} where T, AbstractArray{T,1} where T},T,T}} where T&lt;:Number"><code>ReinforcementLearningCore.generalized_advantage_estimation</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">generalized_advantage_estimation(rewards::VectorOrMatrix, values::VectorOrMatrix, γ::Number, λ::Number;kwargs...)</code></pre><p>Calculate the generalized advantage estimate started from the current step with discount rate of <code>γ</code> and a lambda for GAE-Lambda of &#39;λ&#39;. <code>rewards</code> and &#39;values&#39; can be a matrix.</p><p><strong>Keyword argments</strong></p><ul><li><code>dims=:</code>, if <code>rewards</code> is a <code>Matrix</code>, then <code>dims</code> can only be <code>1</code> or <code>2</code>.</li><li><code>terminal=nothing</code>, specify if each reward follows by a terminal. <code>nothing</code> means the game is not terminated yet. If <code>terminal</code> is provided, then the size must be the same with <code>rewards</code>.</li></ul><p><strong>Example</strong></p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.get_trace-Tuple{AbstractTrajectory,Vararg{Symbol,N} where N}" href="#ReinforcementLearningCore.get_trace-Tuple{AbstractTrajectory,Vararg{Symbol,N} where N}"><code>ReinforcementLearningCore.get_trace</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">get_trace(t::AbstractTrajectory, s::Symbol...)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.get_trace-Union{Tuple{AbstractTrajectory{names,types} where types}, Tuple{names}} where names" href="#ReinforcementLearningCore.get_trace-Union{Tuple{AbstractTrajectory{names,types} where types}, Tuple{names}} where names"><code>ReinforcementLearningCore.get_trace</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">get_trace(t::AbstractTrajectory{names}) where {names}</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.get_trace-Union{Tuple{N}, Tuple{AbstractTrajectory,Tuple{Vararg{Symbol,N}}}} where N" href="#ReinforcementLearningCore.get_trace-Union{Tuple{N}, Tuple{AbstractTrajectory,Tuple{Vararg{Symbol,N}}}} where N"><code>ReinforcementLearningCore.get_trace</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">get_trace(t::AbstractTrajectory, s::NTuple{N,Symbol}) where {N}</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.huber_loss-Tuple{Any,Any}" href="#ReinforcementLearningCore.huber_loss-Tuple{Any,Any}"><code>ReinforcementLearningCore.huber_loss</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">huber_loss(labels, predictions; δ = 1.0f0)</code></pre><p>See <a href="https://en.wikipedia.org/wiki/Huber_loss">Huber loss</a></p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningCore.huber_loss_unreduced-Tuple{Any,Any}" href="#ReinforcementLearningCore.huber_loss_unreduced-Tuple{Any,Any}"><code>ReinforcementLearningCore.huber_loss_unreduced</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">huber_loss_unreduced(labels, predictions; δ = 1.0f0)</code></pre><p>Similar to <a href="#ReinforcementLearningCore.huber_loss-Tuple{Any,Any}"><code>huber_loss</code></a>, but it doesn&#39;t do the <code>mean</code> operation in the last step.</p></div></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../rl_base/">« RLBase</a><a class="docs-footer-nextpage" href="../rl_envs/">RLEnvs »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Thursday 6 August 2020 01:06">Thursday 6 August 2020</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
