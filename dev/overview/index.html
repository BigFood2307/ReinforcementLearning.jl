<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Overview · ReinforcementLearning.jl</title><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-149861753-1', 'auto');
ga('send', 'pageview');
</script><link rel="canonical" href="https://juliareinforcementlearning.github.io/ReinforcementLearning.jl/latest/overview/index.html"/><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../assets/custom.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="../"><img class="logo" src="../assets/logo.png" alt="ReinforcementLearning.jl logo"/></a><h1>ReinforcementLearning.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li><a class="toctext" href="../a_quick_example/">A Quick Example</a></li><li class="current"><a class="toctext" href>Overview</a><ul class="internal"><li><a class="toctext" href="#Key-Concepts-1">Key Concepts</a></li></ul></li><li><span class="toctext">Manual</span><ul><li><span class="toctext">Components</span><ul><li><a class="toctext" href="../components/agents/">Agents</a></li><li><a class="toctext" href="../components/environments/">Environments</a></li><li><a class="toctext" href="../components/buffers/">Buffers</a></li><li><a class="toctext" href="../components/policies/">Policies</a></li><li><a class="toctext" href="../components/learners/">Learners</a></li><li><a class="toctext" href="../components/approximators/">Approximators</a></li><li><a class="toctext" href="../components/action_selectors/">Action Selectors</a></li><li><a class="toctext" href="../components/environment_models/">Environment Models</a></li></ul></li><li><a class="toctext" href="../core/">Core</a></li><li><a class="toctext" href="../utils/">Utils</a></li></ul></li><li><a class="toctext" href="../tips_for_developers/">Tips for Developers</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Overview</a></li></ul><a class="edit-page" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/master/docs/src/overview.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Overview</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Overview-1" href="#Overview-1">Overview</a></h1><p>Before diving into details, let&#39;s review some basic concepts in <strong>RL(Reinforcement Learning)</strong> first. Then we&#39;ll gradually introduce the relationship between those concepts and our implementations in this package.</p><h2><a class="nav-anchor" id="Key-Concepts-1" href="#Key-Concepts-1">Key Concepts</a></h2><h3><a class="nav-anchor" id="Agent-and-Environment-1" href="#Agent-and-Environment-1">Agent and Environment</a></h3><p><img src="../assets/img/agent_environment_relation.png" alt="agent_environment_relation"/></p><p>Generally speaking, RL is to learn how to take actions so as to maximize a numerical reward. Two core concepts in RL are <strong>Agent</strong> and <strong>Environment</strong>. In each step, the agent is provided with some observation of the environment and is required to take an action. Then the environment takes in that action and transites to another state, providing a numerical reward in the meantime.</p><p>In our package, <strong>Agent</strong> is an abstract type of <a href="../components/agents/#ReinforcementLearning.AbstractAgent"><code>AbstractAgent</code></a>. And <strong>Environment</strong> is an abstract type of <code>AbstractEnvironment</code> provided in another package named <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironments.jl">ReinforcementLearningEnvironments.jl</a>. We can <code>observe</code> an environment to get an <a href="@ref"><code>Observation</code></a> and <code>interact!</code> with an environment using an action. Usually, agents and environments are functional objects. So we can use the piping operator (<code>|&gt;</code>) to simulate the steps implied in the above picture: <code>env |&gt; observe |&gt; agent |&gt; env</code>. See <a href="../components/agents/#Agents-1">Agents</a> and <a href="../components/environments/#Environments-1">Environments</a> for more some concrete implementations.</p><p><img src="../assets/img/multi_agent.png" alt="multi_agent"/></p><p>For multi-agent environments, an <code>AgentManager</code> is introduced to manage the interactions between agents and environments (for now it&#39;s just a <code>Tuple</code>).</p><p>Now let&#39;s take a closer look at <a href="../components/agents/#ReinforcementLearning.Agent"><code>Agent</code></a>:</p><p><img src="../assets/img/agent_in_detail.png" alt="agent_in_detail"/></p><p>This is a typical hierarchical chart of different components. After getting an <a href="@ref"><code>Observation</code></a> from the envrionment, the <a href="../components/agents/#Agents-1">Agents</a> use it to fill <a href="../components/buffers/#Buffers-1">Buffers</a> and update <a href="../components/environment_models/#Environment-Models-1">Environment Models</a> and <a href="../components/policies/#Policies-1">Policies</a>. A policy is used to generate an action given an <a href="@ref"><code>Observation</code></a>. Usually a policy contains <a href="../components/learners/#Learners-1">Learners</a> to decide how to update internal <a href="../components/approximators/#Approximators-1">Approximators</a>. A typical approximator is <a href="../components/approximators/#ReinforcementLearning.NeuralNetworkQ"><code>NeuralNetworkQ</code></a>, which uses neural networks to approximator state-action values.</p><p>Notice that different components may have different implementations, so some steps are optional. Anyway, this picture should give you a perceptual knowledge of how those components are organized. You can move on and read the introduction of each components for more details.</p><footer><hr/><a class="previous" href="../a_quick_example/"><span class="direction">Previous</span><span class="title">A Quick Example</span></a><a class="next" href="../components/agents/"><span class="direction">Next</span><span class="title">Agents</span></a></footer></article></body></html>
