<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Learners · ReinforcementLearning.jl</title><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-149861753-1', 'auto');
ga('send', 'pageview');
</script><link rel="canonical" href="https://juliareinforcementlearning.github.io/ReinforcementLearning.jl/latest/components/learners/index.html"/><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/documenter.css" rel="stylesheet" type="text/css"/><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/custom.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="../../"><img class="logo" src="../../assets/logo.png" alt="ReinforcementLearning.jl logo"/></a><h1>ReinforcementLearning.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../../">Home</a></li><li><a class="toctext" href="../../a_quick_example/">A Quick Example</a></li><li><a class="toctext" href="../../overview/">Overview</a></li><li><span class="toctext">Manual</span><ul><li><span class="toctext">Components</span><ul><li><a class="toctext" href="../agents/">Agents</a></li><li><a class="toctext" href="../environments/">Environments</a></li><li><a class="toctext" href="../buffers/">Buffers</a></li><li><a class="toctext" href="../policies/">Policies</a></li><li class="current"><a class="toctext" href>Learners</a><ul class="internal"><li><a class="toctext" href="#Traditional-Learners-1">Traditional Learners</a></li><li><a class="toctext" href="#Q-Learners-1">Q Learners</a></li></ul></li><li><a class="toctext" href="../approximators/">Approximators</a></li><li><a class="toctext" href="../action_selectors/">Action Selectors</a></li><li><a class="toctext" href="../environment_models/">Environment Models</a></li></ul></li><li><a class="toctext" href="../../core/">Core</a></li><li><a class="toctext" href="../../utils/">Utils</a></li></ul></li><li><a class="toctext" href="../../tips_for_developers/">Tips for Developers</a></li><li><span class="toctext">Experiments</span><ul><li><a class="toctext" href="../../experiments/atari_dqn/">Play Atari Games with DQN</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Manual</li><li>Components</li><li><a href>Learners</a></li></ul><a class="edit-page" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/master/docs/src/components/learners.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Learners</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Learners-1" href="#Learners-1">Learners</a></h1><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.AbstractLearner" href="#ReinforcementLearning.AbstractLearner"><code>ReinforcementLearning.AbstractLearner</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>A learner is used to define:</p><ul><li>How to generate necessary training data?</li><li>How to update the inner <a href="../approximators/#Approximators-1">Approximators</a>?</li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/5c10dba7fd85b15c8e10e826425c5be614c6aeb0/src/components/learners/abstract_learner.jl#L3-L8">source</a></section><h2><a class="nav-anchor" id="Traditional-Learners-1" href="#Traditional-Learners-1">Traditional Learners</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.TDLearner" href="#ReinforcementLearning.TDLearner"><code>ReinforcementLearning.TDLearner</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">TDLearner(approximator::Tapp, γ::Float64, optimizer::Float64; n::Int=0) where {Tapp&lt;:AbstractVApproximator}
TDLearner(approximator::Tapp, γ::Float64, optimizer::Float64; n::Int=0, method::Symbol=:SARSA) where {Tapp&lt;:AbstractQApproximator}</code></pre><p>The <code>TDLearner</code>(Temporal Difference Learner) use the latest <code>n</code> step experiences to update the <code>approximator</code>. Note that <code>n</code> starts with <code>0</code>, which means looking forward for the next <code>n</code> steps. <code>γ</code> is the discount rate of experience. <code>optimizer</code> is the learning rate.</p><p>For <a href="../approximators/#ReinforcementLearning.AbstractVApproximator"><code>AbstractVApproximator</code></a>, the only supported update method is <code>:SRS</code>, which means only <strong>S</strong>tates, <strong>R</strong>ewards and next_<strong>S</strong>ates are used to update the <code>approximator</code>.</p><p>For <a href="../approximators/#ReinforcementLearning.AbstractQApproximator"><code>AbstractQApproximator</code></a>, the following methods are supported:</p><ul><li><code>:SARS</code> (aka Q-Learning)</li><li><code>:SARSA</code></li><li><code>:ExpectedSARSA</code></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/5c10dba7fd85b15c8e10e826425c5be614c6aeb0/src/components/learners/temporal_difference_learner.jl#L7-L24">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.DoubleLearner" href="#ReinforcementLearning.DoubleLearner"><code>ReinforcementLearning.DoubleLearner</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">DoubleLearner(;L1, L2)</code></pre><p>For now, this only supports <a href="#ReinforcementLearning.TDLearner"><code>TDLearner</code></a> for <code>L1</code> and <code>L2</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaLang/julia/blob/8c4656b97aaa0e2c7491c84cacbc70afc84d6601/base/#L0-L4">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.DifferentialTDLearner" href="#ReinforcementLearning.DifferentialTDLearner"><code>ReinforcementLearning.DifferentialTDLearner</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">DifferentialTDLearner(;approximator::A, α::Float64, β::Float64, R̄::Float64 = 0.0, n::Int = 0)</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaLang/julia/blob/8c4656b97aaa0e2c7491c84cacbc70afc84d6601/base/#L0-L2">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.TDλReturnLearner" href="#ReinforcementLearning.TDλReturnLearner"><code>ReinforcementLearning.TDλReturnLearner</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">TDλReturnLearner(;approximator::Tapp, γ::Float64 = 1.0, α::Float64, λ::Float64)</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaLang/julia/blob/8c4656b97aaa0e2c7491c84cacbc70afc84d6601/base/#L0-L2">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.MonteCarloLearner" href="#ReinforcementLearning.MonteCarloLearner"><code>ReinforcementLearning.MonteCarloLearner</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">MonteCarloLearner(; approximator::A, γ = 1.0, α = 1.0, kind = FIRST_VISIT, sampling = NO_SAMPLING, returns = CachedSampleAvg())</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/5c10dba7fd85b15c8e10e826425c5be614c6aeb0/src/components/learners/monte_carlo_learner.jl#L25-L27">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.GradientBanditLearner" href="#ReinforcementLearning.GradientBanditLearner"><code>ReinforcementLearning.GradientBanditLearner</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">GradientBanditLearner(;approximator::A, optimizer::O, baseline::B)</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaLang/julia/blob/8c4656b97aaa0e2c7491c84cacbc70afc84d6601/base/#L0-L2">source</a></section><h2><a class="nav-anchor" id="Q-Learners-1" href="#Q-Learners-1">Q Learners</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.BasicDQNLearner" href="#ReinforcementLearning.BasicDQNLearner"><code>ReinforcementLearning.BasicDQNLearner</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">BasicDQNLearner(;kwargs...)</code></pre><p>See paper: <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Playing Atari with Deep Reinforcement Learning</a></p><p>This is the very basic implementation of DQN. Compared to the traditional Q learning, the only difference is that, in the updating step it uses a batch of transitions sampled from an experience buffer instead of current transition. And the <code>approximator</code> is usually a <a href="../approximators/#ReinforcementLearning.NeuralNetworkQ"><code>NeuralNetworkQ</code></a>.</p><p>You can start from this implementation to understand how everything is organized and how to write your own customized algorithm.</p><p><strong>Keywords</strong></p><ul><li><code>approximator</code>::<a href="../approximators/#ReinforcementLearning.AbstractQApproximator"><code>AbstractQApproximator</code></a>: used to get Q-values of a state.</li><li><code>loss_fun</code>: the loss function to use. TODO: provide a default <a href="../../utils/#ReinforcementLearning.Utils.huber_loss-Tuple{Any,Any}"><code>huber_loss</code></a>?</li><li><code>γ::Float32=0.99f0</code>: discount rate.</li><li><code>batch_size::Int=32</code></li><li><code>update_horizon::Int=1</code>: length of update (&#39;n&#39; in n-step update).</li><li><code>min_replay_history::Int=32</code>: number of transitions that should be experienced before updating the <code>approximator</code>.</li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaLang/julia/blob/8c4656b97aaa0e2c7491c84cacbc70afc84d6601/base/#L0-L19">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.DQNLearner" href="#ReinforcementLearning.DQNLearner"><code>ReinforcementLearning.DQNLearner</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">DQNLearner(;kwargs...)</code></pre><p>See paper: <a href="https://www.nature.com/articles/nature14236">Human-level control through deep reinforcement learning</a></p><p><strong>Keywords</strong></p><ul><li><code>approximator</code>::<a href="../approximators/#ReinforcementLearning.AbstractQApproximator"><code>AbstractQApproximator</code></a>: used to get Q-values of a state.</li><li><code>target_approximator</code>::<a href="../approximators/#ReinforcementLearning.AbstractQApproximator"><code>AbstractQApproximator</code></a>: similar to <code>approximator</code>, but used to estimate the target (the next state).</li><li><code>loss_fun</code>: the loss function.</li><li><code>γ::Float32=0.99f0</code>: discount rate.</li><li><code>batch_size::Int=32</code></li><li><code>update_horizon::Int=1</code>: length of update (&#39;n&#39; in n-step update).</li><li><code>min_replay_history::Int=32</code>: number of transitions that should be experienced before updating the <code>approximator</code>.</li><li><code>update_freq::Int=4</code>: the frequency of updating the <code>approximator</code>.</li><li><code>target_update_freq::Int=100</code>: the frequency of syncing <code>target_approximator</code>.</li><li><code>stack_size::Union{Int, Nothing}=4</code>: use the recent <code>stack_size</code> frames to form a stacked state.</li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaLang/julia/blob/8c4656b97aaa0e2c7491c84cacbc70afc84d6601/base/#L0-L18">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.PrioritizedDQNLearner" href="#ReinforcementLearning.PrioritizedDQNLearner"><code>ReinforcementLearning.PrioritizedDQNLearner</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">PrioritizedDQNLearner(;kwargs...)</code></pre><p>See paper: <a href="https://arxiv.org/abs/1511.05952">Prioritized Experience Replay</a></p><p><strong>Keywords</strong></p><ul><li><code>approximator</code>::<a href="../approximators/#ReinforcementLearning.AbstractQApproximator"><code>AbstractQApproximator</code></a>: used to get Q-values of a state.</li><li><code>target_approximator</code>::<a href="../approximators/#ReinforcementLearning.AbstractQApproximator"><code>AbstractQApproximator</code></a>: similar to <code>approximator</code>, but used to estimate the target (the next state).</li><li><code>loss_fun</code>: the loss function.</li><li><code>γ::Float32=0.99f0</code>: discount rate.</li><li><code>batch_size::Int=32</code></li><li><code>update_horizon::Int=1</code>: length of update (&#39;n&#39; in n-step update).</li><li><code>min_replay_history::Int=32</code>: number of transitions that should be experienced before updating the <code>approximator</code>.</li><li><code>update_freq::Int=4</code>: the frequency of updating the <code>approximator</code>.</li><li><code>target_update_freq::Int=100</code>: the frequency of syncing <code>target_approximator</code>.</li><li><code>stack_size::Union{Int, Nothing}=4</code>: use the recent <code>stack_size</code> frames to form a stacked state.</li><li><code>default_priority::Float64=100.</code>: the default priority for newly added transitions.</li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaLang/julia/blob/8c4656b97aaa0e2c7491c84cacbc70afc84d6601/base/#L0-L18">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.RainbowLearner" href="#ReinforcementLearning.RainbowLearner"><code>ReinforcementLearning.RainbowLearner</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">RainbowLearner(;kwargs...)</code></pre><p>See paper: <a href="https://arxiv.org/abs/1710.02298">Rainbow: Combining Improvements in Deep Reinforcement Learning</a></p><p><strong>Keywords</strong></p><ul><li><code>approximator</code>::<a href="../approximators/#ReinforcementLearning.AbstractQApproximator"><code>AbstractQApproximator</code></a>: used to get Q-values of a state.</li><li><code>target_approximator</code>::<a href="../approximators/#ReinforcementLearning.AbstractQApproximator"><code>AbstractQApproximator</code></a>: similar to <code>approximator</code>, but used to estimate the target (the next state).</li><li><code>loss_fun</code>: the loss function.</li><li><code>Vₘₐₓ::Float32</code>: the maximum value of distribution.</li><li><code>Vₘᵢₙ::Float32</code>: the minimum value of distribution.</li><li><code>n_actions::Int</code>: number of possible actions.</li><li><code>γ::Float32=0.99f0</code>: discount rate.</li><li><code>batch_size::Int=32</code></li><li><code>update_horizon::Int=1</code>: length of update (&#39;n&#39; in n-step update).</li><li><code>min_replay_history::Int=32</code>: number of transitions that should be experienced before updating the <code>approximator</code>.</li><li><code>update_freq::Int=4</code>: the frequency of updating the <code>approximator</code>.</li><li><code>target_update_freq::Int=500</code>: the frequency of syncing <code>target_approximator</code>.</li><li><code>stack_size::Union{Int, Nothing}=4</code>: use the recent <code>stack_size</code> frames to form a stacked state.</li><li><code>default_priority::Float64=100.</code>: the default priority for newly added transitions.</li><li><code>n_atoms::Int=51</code>: the number of buckets of the value function distribution.</li><li><code>stack_size::Union{Int, Nothing}=4</code>: use the recent <code>stack_size</code> frames to form a stacked state.</li><li><code>default_priority::Float64=100.</code>: the default priority for newly added transitions.</li></ul><div class="admonition warning"><div class="admonition-title">Warning</div><div class="admonition-text"><p>The <code>Zygote_gpu</code> version is slow due to that the <code>argmax(A, dims=1)</code> falls back to the CPU version in CuArrays.</p></div></div></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaLang/julia/blob/8c4656b97aaa0e2c7491c84cacbc70afc84d6601/base/#L0-L27">source</a></section><footer><hr/><a class="previous" href="../policies/"><span class="direction">Previous</span><span class="title">Policies</span></a><a class="next" href="../approximators/"><span class="direction">Next</span><span class="title">Approximators</span></a></footer></article></body></html>
