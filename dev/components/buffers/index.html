<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Buffers · ReinforcementLearning.jl</title><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-149861753-1', 'auto');
ga('send', 'pageview', {'page': location.pathname + location.search + location.hash});
</script><link rel="canonical" href="https://juliareinforcementlearning.github.io/ReinforcementLearning.jl/latest/components/buffers/index.html"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/custom.css" rel="stylesheet" type="text/css"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="ReinforcementLearning.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">ReinforcementLearning.jl</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../a_quick_example/">A Quick Example</a></li><li><a class="tocitem" href="../../overview/">Overview</a></li><li><span class="tocitem">Manual</span><ul><li><input class="collapse-toggle" id="menuitem-4-1" type="checkbox" checked/><label class="tocitem" for="menuitem-4-1"><span class="docs-label">Components</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../agents/">Agents</a></li><li><a class="tocitem" href="../environments/">Environments</a></li><li class="is-active"><a class="tocitem" href>Buffers</a></li><li><a class="tocitem" href="../policies/">Policies</a></li><li><a class="tocitem" href="../learners/">Learners</a></li><li><a class="tocitem" href="../approximators/">Approximators</a></li><li><a class="tocitem" href="../action_selectors/">Action Selectors</a></li><li><a class="tocitem" href="../environment_models/">Environment Models</a></li></ul></li><li><a class="tocitem" href="../../core/">Core</a></li><li><a class="tocitem" href="../../utils/">Utils</a></li></ul></li><li><a class="tocitem" href="../../tips_for_developers/">Tips for Developers</a></li><li><span class="tocitem">Experiments</span><ul><li><a class="tocitem" href="../../experiments/atari_dqn/">Play Atari Games with DQN</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Manual</a></li><li><a class="is-disabled">Components</a></li><li class="is-active"><a href>Buffers</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Buffers</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/master/docs/src/components/buffers.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Buffers-1"><a class="docs-heading-anchor" href="#Buffers-1">Buffers</a><a class="docs-heading-anchor-permalink" href="#Buffers-1" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearning.AbstractTurnBuffer" href="#ReinforcementLearning.AbstractTurnBuffer"><code>ReinforcementLearning.AbstractTurnBuffer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">AbstractTurnBuffer{names, types} &lt;: AbstractArray{NamedTuple{names, types}, 1}</code></pre><p><code>AbstractTurnBuffer</code> is supertype of a collection of buffers to store the interactions between agents and environments. It is a subtype of <code>AbstractArray{NamedTuple{names, types}, 1}</code> where <code>names</code> specifies which fields are to store and <code>types</code> is the coresponding types of the <code>names</code>.</p><table><tr><th style="text-align: left">Required Methods</th><th style="text-align: left">Brief Description</th></tr><tr><td style="text-align: left"><code>Base.push!(b::AbstractTurnBuffer{names, types}, s[, a, r, d, s′, a′])</code></td><td style="text-align: left">Push a turn info into the buffer. According to different <code>names</code> and <code>types</code> of the buffer <code>b</code>, it may accept different number of arguments</td></tr><tr><td style="text-align: left"><code>isfull(b)</code></td><td style="text-align: left">Check whether the buffer is full or not</td></tr><tr><td style="text-align: left"><code>Base.length(b)</code></td><td style="text-align: left">Return the length of buffer</td></tr><tr><td style="text-align: left"><code>Base.getindex(b::AbstractTurnBuffer{names, types})</code></td><td style="text-align: left">Return a turn of type <code>NamedTuple{names, types}</code></td></tr><tr><td style="text-align: left"><code>Base.empty!(b)</code></td><td style="text-align: left">Reset the buffer</td></tr><tr><td style="text-align: left"><strong>Optional Methods</strong></td><td style="text-align: left"></td></tr><tr><td style="text-align: left"><code>Base.size(b)</code></td><td style="text-align: left">Return <code>(length(b),)</code> by default</td></tr><tr><td style="text-align: left"><code>Base.isempty(b)</code></td><td style="text-align: left">Check whether the buffer is empty or not. Return <code>length(b) == 0</code> by default</td></tr><tr><td style="text-align: left"><code>Base.lastindex(b)</code></td><td style="text-align: left">Return <code>length(b)</code> by default</td></tr></table></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/aae24d5e107939048440561e912c73ca0e8519f9/src/components/buffers/abstract_buffer.jl#L3-L21">source</a></section></article><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>CircularArrayBuffer</code>. Check Documenter&#39;s build log for details.</p></div></div><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearning.CircularTurnBuffer" href="#ReinforcementLearning.CircularTurnBuffer"><code>ReinforcementLearning.CircularTurnBuffer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">CircularTurnBuffer{names,types,Tbs}</code></pre><p>Contain a collection of buffers (mainly are <a href="components/@ref"><code>CircularArrayBuffer</code></a>, but not restriced to it) to represent the interractions between agents and environments. This struct itself is very simple, some commonly used buffers are provided by:</p><ul><li><a href="#ReinforcementLearning.circular_PRTSA_buffer"><code>circular_PRTSA_buffer</code></a></li><li><a href="#ReinforcementLearning.circular_PRTSA_buffer"><code>circular_PRTSA_buffer</code></a></li></ul><p><strong>Fields</strong></p><ul><li><code>buffers</code>: a tuple of inner buffers</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/aae24d5e107939048440561e912c73ca0e8519f9/src/components/buffers/circular_turn_buffer.jl#L6-L18">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearning.circular_RTSA_buffer" href="#ReinforcementLearning.circular_RTSA_buffer"><code>ReinforcementLearning.circular_RTSA_buffer</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">circular_RTSA_buffer(;kwargs...) -&gt; CircularTurnBuffer</code></pre><p>A helper function to help generate a <a href="#ReinforcementLearning.CircularTurnBuffer"><code>CircularTurnBuffer</code></a> with <strong>RTSA</strong> (<strong>R</strong>eward, <strong>T</strong>erminal, <strong>S</strong>tate, <strong>A</strong>ction) fields as buffers.</p><p><strong>Keywords</strong></p><p><strong>Necessary</strong></p><ul><li><code>capacity::Int</code>: the maximum length of the buffer</li></ul><p><strong>Optional</strong></p><ul><li><code>state_eltype::Type=Int</code>: the type of the state field in an <a href="components/@ref"><code>Observation</code></a>, <code>Int</code> by default</li><li><code>state_size::NTuple{N, Int}=()</code>: the size of the state field in an <a href="components/@ref"><code>Observation</code></a>, the <code>N</code> must match <code>ndims(state_eltype)</code>. Since the default <code>state_eltype</code> is <code>Int</code>, it is an empty tuple here by default.</li><li><code>action_eltype::Type=Int</code>: similar to <code>state_eltype</code></li><li><code>action_size::NTuple{N, Int}=()</code>: similar to <code>state_size</code></li><li><code>reward_eltype::Type=Float32</code>: similar to <code>state_eltype</code></li><li><code>reward_size::NTuple{N, Int}=()</code>: similar to <code>state_size</code></li><li><code>terminal_eltype::Type=Bool</code>: similar to <code>state_eltype</code></li><li><code>terminal_size::NTuple{N, Int}=()</code>: similar to <code>state_size</code></li></ul><p>The following picture will help you understand how the data are organized.</p><p><img src="../../assets/img/circular_RTSA_buffer.png" alt/></p><p><strong>Examples</strong></p><pre><code class="language-">julia&gt; using ReinforcementLearning

julia&gt; b = circular_RTSA_buffer(;capacity=2, state_eltype=Array{Float32, 2}, state_size=(2, 2))
0-element CircularTurnBuffer{(:reward, :terminal, :state, :action),Tuple{Float32,Bool,Array{Float32,2},Int64},NamedTuple{(:reward, :terminal, :state, :action),Tuple{CircularArrayBuffer{Float32,Float32,1},CircularArrayBuffer{Bool,Bool,1},CircularArrayBuffer{Array{Float32,2},Float32,3},CircularArrayBuffer{Int64,Int64,1}}}}

julia&gt; push!(b; reward = 0.0, terminal = true, state = Float32[0 0; 0 0], action = 0)

julia&gt; length(b)
0

julia&gt; b.buffers.reward
1-element CircularArrayBuffer{Float32,Float32,1}:
 0.0

julia&gt; b.buffers.terminal
1-element CircularArrayBuffer{Bool,Bool,1}:
 1

julia&gt; b.buffers.state
2×2×1 CircularArrayBuffer{Array{Float32,2},Float32,3}:
[:, :, 1] =
 0.0  0.0
 0.0  0.0

julia&gt; b.buffers.action
1-element CircularArrayBuffer{Int64,Int64,1}:
 0

julia&gt; push!(b; reward = 1.0, terminal = false, state = Float32[1 1; 1 1], action = 1)

julia&gt; b
1-element CircularTurnBuffer{(:reward, :terminal, :state, :action),Tuple{Float32,Bool,Array{Float32,2},Int64},NamedTuple{(:reward, :terminal, :state, :action),Tuple{CircularArrayBuffer{Float32,Float32,1},CircularArrayBuffer{Bool,Bool,1},CircularArrayBuffer{Array{Float32,2},Float32,3},CircularArrayBuffer{Int64,Int64,1}}}}:
 (state = Float32[0.0 0.0; 0.0 0.0], action = 0, reward = 1.0f0, terminal = false, next_state = Float32[1.0 1.0; 1.0 1.0], next_action = 1)

julia&gt; push!(b; reward = 2.0, terminal = true, state = Float32[2 2; 2 2], action = 2)

julia&gt; b
2-element CircularTurnBuffer{(:reward, :terminal, :state, :action),Tuple{Float32,Bool,Array{Float32,2},Int64},NamedTuple{(:reward, :terminal, :state, :action),Tuple{CircularArrayBuffer{Float32,Float32,1},CircularArrayBuffer{Bool,Bool,1},CircularArrayBuffer{Array{Float32,2},Float32,3},CircularArrayBuffer{Int64,Int64,1}}}}:
 (state = Float32[0.0 0.0; 0.0 0.0], action = 0, reward = 1.0f0, terminal = false, next_state = Float32[1.0 1.0; 1.0 1.0], next_action = 1)
 (state = Float32[1.0 1.0; 1.0 1.0], action = 1, reward = 2.0f0, terminal = true, next_state = Float32[2.0 2.0; 2.0 2.0], next_action = 2) 

julia&gt; push!(b; reward = 3.0, terminal = false, state = Float32[3 3; 3 3], action = 3)

julia&gt; b
2-element CircularTurnBuffer{(:reward, :terminal, :state, :action),Tuple{Float32,Bool,Array{Float32,2},Int64},NamedTuple{(:reward, :terminal, :state, :action),Tuple{CircularArrayBuffer{Float32,Float32,1},CircularArrayBuffer{Bool,Bool,1},CircularArrayBuffer{Array{Float32,2},Float32,3},CircularArrayBuffer{Int64,Int64,1}}}}:
 (state = Float32[1.0 1.0; 1.0 1.0], action = 1, reward = 2.0f0, terminal = true, next_state = Float32[2.0 2.0; 2.0 2.0], next_action = 2) 
 (state = Float32[2.0 2.0; 2.0 2.0], action = 2, reward = 3.0f0, terminal = false, next_state = Float32[3.0 3.0; 3.0 3.0], next_action = 3)

julia&gt; b.buffers.state
2×2×3 CircularArrayBuffer{Array{Float32,2},Float32,3}:
[:, :, 1] =
 1.0  1.0
 1.0  1.0

[:, :, 2] =
 2.0  2.0
 2.0  2.0

[:, :, 3] =
 3.0  3.0
 3.0  3.0

julia&gt; b.buffers.reward
3-element CircularArrayBuffer{Float32,Float32,1}:
 1.0
 2.0
 3.0

julia&gt; length(b)
2</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/aae24d5e107939048440561e912c73ca0e8519f9/src/components/buffers/circular_turn_buffer.jl#L41-L141">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearning.circular_PRTSA_buffer" href="#ReinforcementLearning.circular_PRTSA_buffer"><code>ReinforcementLearning.circular_PRTSA_buffer</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">circular_PRTSA_buffer(;kwargs...) -&gt; CircularTurnBuffer</code></pre><p>The only difference compared to <a href="#ReinforcementLearning.circular_RTSA_buffer"><code>circular_RTSA_buffer</code></a> is that a new field named <code>priority</code> is added. Notice that the struct of <code>priority</code> is not a <a href="components/@ref"><code>CircularArrayBuffer</code></a> but a <a href="../../utils/#ReinforcementLearning.Utils.SumTree"><code>SumTree</code></a>.</p><p><strong>Keywords</strong></p><p><strong>Necessary</strong></p><ul><li><code>capacity::Int</code>: the maximum length of the buffer</li></ul><p><strong>Optional</strong></p><ul><li><code>state_eltype::Type=Int</code>: the type of the state field in an <a href="components/@ref"><code>Observation</code></a>, <code>Int</code> by default</li><li><code>state_size::NTuple{N, Int}=()</code>: the size of the state field in an <a href="components/@ref"><code>Observation</code></a>, the <code>N</code> must match <code>ndims(state_eltype)</code>. Since the default <code>state_eltype</code> is <code>Int</code>, it is an empty tuple here by default.</li><li><code>action_eltype::Type=Int</code>: similar to <code>state_eltype</code></li><li><code>action_size::NTuple{N, Int}=()</code>: similar to <code>state_size</code></li><li><code>reward_eltype::Type=Float32</code>: similar to <code>state_eltype</code></li><li><code>reward_size::NTuple{N, Int}=()</code>: similar to <code>state_size</code></li><li><code>terminal_eltype::Type=Bool</code>: similar to <code>state_eltype</code></li><li><code>terminal_size::NTuple{N, Int}=()</code>: similar to <code>state_size</code></li><li><code>priority_eltype::Type=Float64</code>: the type of the <code>priority</code> field</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/aae24d5e107939048440561e912c73ca0e8519f9/src/components/buffers/circular_turn_buffer.jl#L175-L198">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearning.EpisodeTurnBuffer" href="#ReinforcementLearning.EpisodeTurnBuffer"><code>ReinforcementLearning.EpisodeTurnBuffer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">EpisodeTurnBuffer{names, types, Tbs} &lt;: AbstractTurnBuffer{names, types}
EpisodeTurnBuffer{names, types}() where {names, types}</code></pre><p>Similar to <a href="#ReinforcementLearning.CircularTurnBuffer"><code>CircularTurnBuffer</code></a>, but instead of using <a href="components/@ref"><code>CircularArrayBuffer</code></a>, it uses a vector to store each element specified by <code>names</code> and <code>types</code>. And when it reaches the end of an episode, the buffer is emptied first when a new observation is pushed.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Notice that, before emptifying the <code>EpisodeTurnBuffer</code>, the last element of each field is exracted and then pushed at the head of the buffer. Without this step, the first transition of the new episode will be lost!</p></div></div><p>See also: <a href="#ReinforcementLearning.episode_RTSA_buffer"><code>episode_RTSA_buffer</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/aae24d5e107939048440561e912c73ca0e8519f9/src/components/buffers/episode_turn_buffer.jl#L3-L13">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearning.episode_RTSA_buffer" href="#ReinforcementLearning.episode_RTSA_buffer"><code>ReinforcementLearning.episode_RTSA_buffer</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">episode_RTSA_buffer(;kwargs...) -&gt; EpisodeTurnBuffer</code></pre><p>Initialize an <code>EpisodeTurnBuffer</code> with fields of <strong>R</strong>eward, <strong>T</strong>erminal, <strong>S</strong>tate, <strong>A</strong>ction.</p><p><strong>Keywords</strong></p><ul><li><code>state_eltype::Type=Int</code>: the type of state.</li><li><code>action_eltype::Type=Int</code>: the type of action.</li><li><code>reward_eltype::Type=Float32</code>: the type of reward.</li><li><code>terminal_eltype::Type=Bool</code>: the type of terminal.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/aae24d5e107939048440561e912c73ca0e8519f9/src/components/buffers/episode_turn_buffer.jl#L28-L39">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../environments/">« Environments</a><a class="docs-footer-nextpage" href="../policies/">Policies »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Saturday 7 December 2019 12:47">Saturday 7 December 2019</span>. Using Julia version 1.3.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
