<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Buffers · ReinforcementLearning.jl</title><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-149861753-1', 'auto');
ga('send', 'pageview');
</script><link rel="canonical" href="https://juliareinforcementlearning.github.io/ReinforcementLearning.jl/latest/components/buffers/index.html"/><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/documenter.css" rel="stylesheet" type="text/css"/><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/custom.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="../../"><img class="logo" src="../../assets/logo.png" alt="ReinforcementLearning.jl logo"/></a><h1>ReinforcementLearning.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../../">Home</a></li><li><a class="toctext" href="../../a_quick_example/">A Quick Example</a></li><li><a class="toctext" href="../../overview/">Overview</a></li><li><span class="toctext">Manual</span><ul><li><span class="toctext">Components</span><ul><li><a class="toctext" href="../agents/">Agents</a></li><li><a class="toctext" href="../environments/">Environments</a></li><li class="current"><a class="toctext" href>Buffers</a><ul class="internal"></ul></li><li><a class="toctext" href="../policies/">Policies</a></li><li><a class="toctext" href="../learners/">Learners</a></li><li><a class="toctext" href="../approximators/">Approximators</a></li><li><a class="toctext" href="../action_selectors/">Action Selectors</a></li><li><a class="toctext" href="../environment_models/">Environment Models</a></li></ul></li><li><a class="toctext" href="../../core/">Core</a></li><li><a class="toctext" href="../../utils/">Utils</a></li></ul></li><li><a class="toctext" href="../../tips_for_developers/">Tips for Developers</a></li><li><span class="toctext">Experiments</span><ul><li><a class="toctext" href="../../experiments/atari_dqn/">Play Atari Games with DQN</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Manual</li><li>Components</li><li><a href>Buffers</a></li></ul><a class="edit-page" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/master/docs/src/components/buffers.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Buffers</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Buffers-1" href="#Buffers-1">Buffers</a></h1><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.AbstractTurnBuffer" href="#ReinforcementLearning.AbstractTurnBuffer"><code>ReinforcementLearning.AbstractTurnBuffer</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">AbstractTurnBuffer{names, types} &lt;: AbstractArray{NamedTuple{names, types}, 1}</code></pre><p><code>AbstractTurnBuffer</code> is supertype of a collection of buffers to store the interactions between agents and environments. It is a subtype of <code>AbstractArray{NamedTuple{names, types}, 1}</code> where <code>names</code> specifies which fields are to store and <code>types</code> is the coresponding types of the <code>names</code>.</p><table><tr><th style="text-align: left">Required Methods</th><th style="text-align: left">Brief Description</th></tr><tr><td style="text-align: left"><code>Base.push!(b::AbstractTurnBuffer{names, types}, s[, a, r, d, s′, a′])</code></td><td style="text-align: left">Push a turn info into the buffer. According to different <code>names</code> and <code>types</code> of the buffer <code>b</code>, it may accept different number of arguments</td></tr><tr><td style="text-align: left"><code>isfull(b)</code></td><td style="text-align: left">Check whether the buffer is full or not</td></tr><tr><td style="text-align: left"><code>Base.length(b)</code></td><td style="text-align: left">Return the length of buffer</td></tr><tr><td style="text-align: left"><code>Base.getindex(b::AbstractTurnBuffer{names, types})</code></td><td style="text-align: left">Return a turn of type <code>NamedTuple{names, types}</code></td></tr><tr><td style="text-align: left"><code>Base.empty!(b)</code></td><td style="text-align: left">Reset the buffer</td></tr><tr><td style="text-align: left"><strong>Optional Methods</strong></td><td style="text-align: left"></td></tr><tr><td style="text-align: left"><code>Base.size(b)</code></td><td style="text-align: left">Return <code>(length(b),)</code> by default</td></tr><tr><td style="text-align: left"><code>Base.isempty(b)</code></td><td style="text-align: left">Check whether the buffer is empty or not. Return <code>length(b) == 0</code> by default</td></tr><tr><td style="text-align: left"><code>Base.lastindex(b)</code></td><td style="text-align: left">Return <code>length(b)</code> by default</td></tr></table></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/76c2542d6e10eb0a5d727a737d11dd0ef03051c3/src/components/buffers/abstract_buffer.jl#L3-L21">source</a></section><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>CircularArrayBuffer</code>. Check Documenter&#39;s build log for details.</p></div></div><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.CircularTurnBuffer" href="#ReinforcementLearning.CircularTurnBuffer"><code>ReinforcementLearning.CircularTurnBuffer</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">CircularTurnBuffer{names,types,Tbs}</code></pre><p>Contain a collection of buffers (mainly are <a href="components/@ref"><code>CircularArrayBuffer</code></a>, but not restriced to it) to represent the interractions between agents and environments. This struct itself is very simple, some commonly used buffers are provided by:</p><ul><li><a href="#ReinforcementLearning.circular_PRTSA_buffer"><code>circular_PRTSA_buffer</code></a></li><li><a href="#ReinforcementLearning.circular_PRTSA_buffer"><code>circular_PRTSA_buffer</code></a></li></ul><p><strong>Fields</strong></p><ul><li><code>buffers</code>: a tuple of inner buffers</li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/76c2542d6e10eb0a5d727a737d11dd0ef03051c3/src/components/buffers/circular_turn_buffer.jl#L6-L18">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.circular_RTSA_buffer" href="#ReinforcementLearning.circular_RTSA_buffer"><code>ReinforcementLearning.circular_RTSA_buffer</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">circular_RTSA_buffer(;kwargs...) -&gt; CircularTurnBuffer</code></pre><p>A helper function to help generate a <a href="#ReinforcementLearning.CircularTurnBuffer"><code>CircularTurnBuffer</code></a> with <strong>RTSA</strong> (<strong>R</strong>eward, <strong>T</strong>erminal, <strong>S</strong>tate, <strong>A</strong>ction) fields as buffers.</p><p><strong>Keywords</strong></p><p><strong>Necessary</strong></p><ul><li><code>capacity::Int</code>: the maximum length of the buffer</li></ul><p><strong>Optional</strong></p><ul><li><code>state_eltype::Type=Int</code>: the type of the state field in an <a href="components/@ref"><code>Observation</code></a>, <code>Int</code> by default</li><li><code>state_size::NTuple{N, Int}=()</code>: the size of the state field in an <a href="components/@ref"><code>Observation</code></a>, the <code>N</code> must match <code>ndims(state_eltype)</code>. Since the default <code>state_eltype</code> is <code>Int</code>, it is an empty tuple here by default.</li><li><code>action_eltype::Type=Int</code>: similar to <code>state_eltype</code></li><li><code>action_size::NTuple{N, Int}=()</code>: similar to <code>state_size</code></li><li><code>reward_eltype::Type=Float32</code>: similar to <code>state_eltype</code></li><li><code>reward_size::NTuple{N, Int}=()</code>: similar to <code>state_size</code></li><li><code>terminal_eltype::Type=Bool</code>: similar to <code>state_eltype</code></li><li><code>terminal_size::NTuple{N, Int}=()</code>: similar to <code>state_size</code></li></ul><p>The following picture will help you understand how the data are organized.</p><p><img src="../../assets/img/circular_RTSA_buffer.png" alt/></p><p><strong>Examples</strong></p><pre><code class="language-">julia&gt; using ReinforcementLearning

julia&gt; b = circular_RTSA_buffer(;capacity=2, state_eltype=Array{Float32, 2}, state_size=(2, 2))
0-element CircularTurnBuffer{(:reward, :terminal, :state, :action),Tuple{Float32,Bool,Array{Float32,2},Int64},NamedTuple{(:reward, :terminal, :state, :action),Tuple{CircularArrayBuffer{Float32,Float32,1},CircularArrayBuffer{Bool,Bool,1},CircularArrayBuffer{Array{Float32,2},Float32,3},CircularArrayBuffer{Int64,Int64,1}}}}

julia&gt; push!(b; reward = 0.0, terminal = true, state = Float32[0 0; 0 0], action = 0)

julia&gt; length(b)
0

julia&gt; b.buffers.reward
1-element CircularArrayBuffer{Float32,Float32,1}:
 0.0

julia&gt; b.buffers.terminal
1-element CircularArrayBuffer{Bool,Bool,1}:
 1

julia&gt; b.buffers.state
2×2×1 CircularArrayBuffer{Array{Float32,2},Float32,3}:
[:, :, 1] =
 0.0  0.0
 0.0  0.0

julia&gt; b.buffers.action
1-element CircularArrayBuffer{Int64,Int64,1}:
 0

julia&gt; push!(b; reward = 1.0, terminal = false, state = Float32[1 1; 1 1], action = 1)

julia&gt; b
1-element CircularTurnBuffer{(:reward, :terminal, :state, :action),Tuple{Float32,Bool,Array{Float32,2},Int64},NamedTuple{(:reward, :terminal, :state, :action),Tuple{CircularArrayBuffer{Float32,Float32,1},CircularArrayBuffer{Bool,Bool,1},CircularArrayBuffer{Array{Float32,2},Float32,3},CircularArrayBuffer{Int64,Int64,1}}}}:
 (state = Float32[0.0 0.0; 0.0 0.0], action = 0, reward = 1.0f0, terminal = false, next_state = Float32[1.0 1.0; 1.0 1.0], next_action = 1)

julia&gt; push!(b; reward = 2.0, terminal = true, state = Float32[2 2; 2 2], action = 2)

julia&gt; b
2-element CircularTurnBuffer{(:reward, :terminal, :state, :action),Tuple{Float32,Bool,Array{Float32,2},Int64},NamedTuple{(:reward, :terminal, :state, :action),Tuple{CircularArrayBuffer{Float32,Float32,1},CircularArrayBuffer{Bool,Bool,1},CircularArrayBuffer{Array{Float32,2},Float32,3},CircularArrayBuffer{Int64,Int64,1}}}}:
 (state = Float32[0.0 0.0; 0.0 0.0], action = 0, reward = 1.0f0, terminal = false, next_state = Float32[1.0 1.0; 1.0 1.0], next_action = 1)
 (state = Float32[1.0 1.0; 1.0 1.0], action = 1, reward = 2.0f0, terminal = true, next_state = Float32[2.0 2.0; 2.0 2.0], next_action = 2) 

julia&gt; push!(b; reward = 3.0, terminal = false, state = Float32[3 3; 3 3], action = 3)

julia&gt; b
2-element CircularTurnBuffer{(:reward, :terminal, :state, :action),Tuple{Float32,Bool,Array{Float32,2},Int64},NamedTuple{(:reward, :terminal, :state, :action),Tuple{CircularArrayBuffer{Float32,Float32,1},CircularArrayBuffer{Bool,Bool,1},CircularArrayBuffer{Array{Float32,2},Float32,3},CircularArrayBuffer{Int64,Int64,1}}}}:
 (state = Float32[1.0 1.0; 1.0 1.0], action = 1, reward = 2.0f0, terminal = true, next_state = Float32[2.0 2.0; 2.0 2.0], next_action = 2) 
 (state = Float32[2.0 2.0; 2.0 2.0], action = 2, reward = 3.0f0, terminal = false, next_state = Float32[3.0 3.0; 3.0 3.0], next_action = 3)

julia&gt; b.buffers.state
2×2×3 CircularArrayBuffer{Array{Float32,2},Float32,3}:
[:, :, 1] =
 1.0  1.0
 1.0  1.0

[:, :, 2] =
 2.0  2.0
 2.0  2.0

[:, :, 3] =
 3.0  3.0
 3.0  3.0

julia&gt; b.buffers.reward
3-element CircularArrayBuffer{Float32,Float32,1}:
 1.0
 2.0
 3.0

julia&gt; length(b)
2</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/76c2542d6e10eb0a5d727a737d11dd0ef03051c3/src/components/buffers/circular_turn_buffer.jl#L41-L141">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.circular_PRTSA_buffer" href="#ReinforcementLearning.circular_PRTSA_buffer"><code>ReinforcementLearning.circular_PRTSA_buffer</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">circular_PRTSA_buffer(;kwargs...) -&gt; CircularTurnBuffer</code></pre><p>The only difference compared to <a href="#ReinforcementLearning.circular_RTSA_buffer"><code>circular_RTSA_buffer</code></a> is that a new field named <code>priority</code> is added. Notice that the struct of <code>priority</code> is not a <a href="components/@ref"><code>CircularArrayBuffer</code></a> but a <a href="../../utils/#ReinforcementLearning.Utils.SumTree"><code>SumTree</code></a>.</p><p><strong>Keywords</strong></p><p><strong>Necessary</strong></p><ul><li><code>capacity::Int</code>: the maximum length of the buffer</li></ul><p><strong>Optional</strong></p><ul><li><code>state_eltype::Type=Int</code>: the type of the state field in an <a href="components/@ref"><code>Observation</code></a>, <code>Int</code> by default</li><li><code>state_size::NTuple{N, Int}=()</code>: the size of the state field in an <a href="components/@ref"><code>Observation</code></a>, the <code>N</code> must match <code>ndims(state_eltype)</code>. Since the default <code>state_eltype</code> is <code>Int</code>, it is an empty tuple here by default.</li><li><code>action_eltype::Type=Int</code>: similar to <code>state_eltype</code></li><li><code>action_size::NTuple{N, Int}=()</code>: similar to <code>state_size</code></li><li><code>reward_eltype::Type=Float32</code>: similar to <code>state_eltype</code></li><li><code>reward_size::NTuple{N, Int}=()</code>: similar to <code>state_size</code></li><li><code>terminal_eltype::Type=Bool</code>: similar to <code>state_eltype</code></li><li><code>terminal_size::NTuple{N, Int}=()</code>: similar to <code>state_size</code></li><li><code>priority_eltype::Type=Float64</code>: the type of the <code>priority</code> field</li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/76c2542d6e10eb0a5d727a737d11dd0ef03051c3/src/components/buffers/circular_turn_buffer.jl#L175-L198">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.EpisodeTurnBuffer" href="#ReinforcementLearning.EpisodeTurnBuffer"><code>ReinforcementLearning.EpisodeTurnBuffer</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">EpisodeTurnBuffer{names, types, Tbs} &lt;: AbstractTurnBuffer{names, types}
EpisodeTurnBuffer{names, types}() where {names, types}</code></pre><p>Similar to <a href="#ReinforcementLearning.CircularTurnBuffer"><code>CircularTurnBuffer</code></a>, but instead of using <a href="components/@ref"><code>CircularArrayBuffer</code></a>, it uses a vector to store each element specified by <code>names</code> and <code>types</code>. And when it reaches the end of an episode, the buffer is emptied first when a new observation is pushed.</p><div class="admonition note"><div class="admonition-title">Note</div><div class="admonition-text"><p>Notice that, before emptifying the <code>EpisodeTurnBuffer</code>, the last element of each field is exracted and then pushed at the head of the buffer. Without this step, the first transition of the new episode will be lost!</p></div></div><p>See also: <a href="#ReinforcementLearning.episode_RTSA_buffer"><code>episode_RTSA_buffer</code></a></p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/76c2542d6e10eb0a5d727a737d11dd0ef03051c3/src/components/buffers/episode_turn_buffer.jl#L3-L13">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.episode_RTSA_buffer" href="#ReinforcementLearning.episode_RTSA_buffer"><code>ReinforcementLearning.episode_RTSA_buffer</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">episode_RTSA_buffer(;kwargs...) -&gt; EpisodeTurnBuffer</code></pre><p>Initialize an <code>EpisodeTurnBuffer</code> with fields of <strong>R</strong>eward, <strong>T</strong>erminal, <strong>S</strong>tate, <strong>A</strong>ction.</p><p><strong>Keywords</strong></p><ul><li><code>state_eltype::Type=Int</code>: the type of state.</li><li><code>action_eltype::Type=Int</code>: the type of action.</li><li><code>reward_eltype::Type=Float32</code>: the type of reward.</li><li><code>terminal_eltype::Type=Bool</code>: the type of terminal.</li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/76c2542d6e10eb0a5d727a737d11dd0ef03051c3/src/components/buffers/episode_turn_buffer.jl#L28-L39">source</a></section><footer><hr/><a class="previous" href="../environments/"><span class="direction">Previous</span><span class="title">Environments</span></a><a class="next" href="../policies/"><span class="direction">Next</span><span class="title">Policies</span></a></footer></article></body></html>
