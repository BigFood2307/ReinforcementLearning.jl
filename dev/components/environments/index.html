<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Environments · ReinforcementLearning.jl</title><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-149861753-1', 'auto');
ga('send', 'pageview');
</script><link rel="canonical" href="https://juliareinforcementlearning.github.io/ReinforcementLearning.jl/latest/components/environments/index.html"/><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/documenter.css" rel="stylesheet" type="text/css"/><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/custom.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="../../"><img class="logo" src="../../assets/logo.png" alt="ReinforcementLearning.jl logo"/></a><h1>ReinforcementLearning.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../../">Home</a></li><li><a class="toctext" href="../../a_quick_example/">A Quick Example</a></li><li><a class="toctext" href="../../overview/">Overview</a></li><li><span class="toctext">Manual</span><ul><li><span class="toctext">Components</span><ul><li><a class="toctext" href="../agents/">Agents</a></li><li class="current"><a class="toctext" href>Environments</a><ul class="internal"><li><a class="toctext" href="#Preprocessors-1">Preprocessors</a></li></ul></li><li><a class="toctext" href="../buffers/">Buffers</a></li><li><a class="toctext" href="../policies/">Policies</a></li><li><a class="toctext" href="../learners/">Learners</a></li><li><a class="toctext" href="../approximators/">Approximators</a></li><li><a class="toctext" href="../action_selectors/">Action Selectors</a></li><li><a class="toctext" href="../environment_models/">Environment Models</a></li></ul></li><li><a class="toctext" href="../../core/">Core</a></li><li><a class="toctext" href="../../utils/">Utils</a></li></ul></li><li><a class="toctext" href="../../tips_for_developers/">Tips for Developers</a></li></ul></nav><article id="docs"><header><nav><ul><li>Manual</li><li>Components</li><li><a href>Environments</a></li></ul><a class="edit-page" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/master/docs/src/components/environments.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Environments</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Environments-1" href="#Environments-1">Environments</a></h1><p>This package relies on some interfaces provided by the <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironments.jl">ReinforcementLearningEnvironments.jl</a> (<strong>RLEnvs</strong>). For completeness, we will also give a short introduction to it here.</p><p>RLEnvs provides many interfaces similar to <a href="https://gym.openai.com/docs/">OpenAI Gym</a>. But also extends it a little bit to make things easier to interact with sync/async, single/multi agent environments.</p><p>Basically, an environment is a functional object which takes in an action and changes its internal state correspondly. For sync environments, both <code>env(action)</code> and <code>reset!(env)</code> should return <code>nothing</code>, and <code>observe(env)</code> should return an <a href="#ReinforcementLearningEnvironments.Observation"><code>Observation</code></a>. For async environments, they should all return a task.</p><p>A specially kind of environment is <a href="#ReinforcementLearning.WrappedEnv"><code>WrappedEnv</code></a>.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearningEnvironments.Observation" href="#ReinforcementLearningEnvironments.Observation"><code>ReinforcementLearningEnvironments.Observation</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">Observation(;reward, terminal, state, meta...)</code></pre><p>The observation of an environment from the perspective of an agent.</p><p><strong>Keywords &amp; Fields</strong></p><ul><li><code>reward</code>: the reward of an agent</li><li><code>terminal</code>: indicates that if the environment is terminated or not.</li><li><code>state</code>: the current state of the environment from the perspective of an agent</li><li><code>meta</code>: some other information, like <code>legal_actions</code>...</li></ul><div class="admonition note"><div class="admonition-title">Note</div><div class="admonition-text"><p>The <code>reward</code> and <code>terminal</code> of the first observation before interacting with an environment may not be valid.</p></div></div></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/c66bbc8e46d3715326d3f137250a93a1b8ad7636/src/extensions/ReinforcementLearningEnvironments.jl#L10-L24">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.WrappedEnv" href="#ReinforcementLearning.WrappedEnv"><code>ReinforcementLearning.WrappedEnv</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">WrappedEnv(;env, preprocessor)</code></pre><p>The observation of <code>env</code> is first processed by the <code>preprocessor</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaLang/julia/blob/8c4656b97aaa0e2c7491c84cacbc70afc84d6601/base/#L0-L4">source</a></section><h2><a class="nav-anchor" id="Preprocessors-1" href="#Preprocessors-1">Preprocessors</a></h2><p>Following are some built-in preprocessors. Notice that preprocessors can be chained (like <code>Chain(p1, p2, ps...)</code>) to get a composed preprocessor.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.AbstractPreprocessor" href="#ReinforcementLearning.AbstractPreprocessor"><code>ReinforcementLearning.AbstractPreprocessor</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>Preprocess an <a href="#ReinforcementLearningEnvironments.Observation"><code>Observation</code></a> and return a new observation. By default, the preprocessor is only applied to the state field of the <a href="#ReinforcementLearningEnvironments.Observation"><code>Observation</code></a> and other fields remain unchanged.</p><p>For customized preprocessors that inherit <code>AbstractPreprocessor</code>, you can change this behavior by rewriting <code>(p::AbstractPreprocessor)(obs::Observation)</code> method.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/c66bbc8e46d3715326d3f137250a93a1b8ad7636/src/components/preprocessors.jl#L19-L27">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.FourierPreprocessor" href="#ReinforcementLearning.FourierPreprocessor"><code>ReinforcementLearning.FourierPreprocessor</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">FourierPreprocessor(order::Int)</code></pre><p>Transform a scalar to a vector of <code>order+1</code> Fourier bases.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/c66bbc8e46d3715326d3f137250a93a1b8ad7636/src/components/preprocessors.jl#L38-L42">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.PolynomialPreprocessor" href="#ReinforcementLearning.PolynomialPreprocessor"><code>ReinforcementLearning.PolynomialPreprocessor</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">PolynomialPreprocessor(order::Int)</code></pre><p>Transform a scalar to vector of maximum <code>order</code> polynomial.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/c66bbc8e46d3715326d3f137250a93a1b8ad7636/src/components/preprocessors.jl#L49-L53">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.TilingPreprocessor" href="#ReinforcementLearning.TilingPreprocessor"><code>ReinforcementLearning.TilingPreprocessor</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">TilingPreprocessor(tilings::Vector{&lt;:Tiling})</code></pre><p>Use each <code>tilings</code> to encode the state and return a vector.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/c66bbc8e46d3715326d3f137250a93a1b8ad7636/src/components/preprocessors.jl#L60-L64">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.ImageCrop" href="#ReinforcementLearning.ImageCrop"><code>ReinforcementLearning.ImageCrop</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">struct ImageCrop
    xidx::UnitRange{Int64}
    yidx::UnitRange{Int64}
end</code></pre><p>Select indices <code>xidx</code> and <code>yidx</code> from a 2 or 3 dimensional array.</p><p><strong>Example:</strong></p><pre><code class="language-none">c = ImageCrop(2:5, 3:2:9)
c([10i + j for i in 1:10, j in 1:10])</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/c66bbc8e46d3715326d3f137250a93a1b8ad7636/src/components/preprocessors.jl#L71-L85">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.ImageResizeBilinear" href="#ReinforcementLearning.ImageResizeBilinear"><code>ReinforcementLearning.ImageResizeBilinear</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">struct ImageResizeBilinear
    outdim::Tuple{Int64, Int64}
end</code></pre><p>Resize any image to <code>outdim = (width, height)</code> with bilinear interpolation.</p><p><strong>Example:</strong></p><pre><code class="language-none">r = ImageResizeBilinear((50, 50))
r(rand(200, 200))
r(rand(UInt8, 3, 100, 100))</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/c66bbc8e46d3715326d3f137250a93a1b8ad7636/src/components/preprocessors.jl#L94-L108">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.ImageResizeNearestNeighbour" href="#ReinforcementLearning.ImageResizeNearestNeighbour"><code>ReinforcementLearning.ImageResizeNearestNeighbour</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">struct ImageResizeNearestNeighbour
    outdim::Tuple{Int64, Int64}
end</code></pre><p>Resize any image to <code>outdim = (width, height)</code> by nearest-neighbour interpolation (i.e. subsampling).</p><p><strong>Example:</strong></p><pre><code class="language-none">r = ImageResizeNearestNeighbour((50, 50))
r(rand(200, 200))
r(rand(UInt8, 3, 100, 100))</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/c66bbc8e46d3715326d3f137250a93a1b8ad7636/src/components/preprocessors.jl#L139-L154">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.ImagePreprocessor" href="#ReinforcementLearning.ImagePreprocessor"><code>ReinforcementLearning.ImagePreprocessor</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">struct ImagePreprocessor
    size
    chain
end</code></pre><p>Use <code>chain</code> to preprocess a grayscale or color image of <code>size = (width, height)</code>.</p><p><strong>Example:</strong></p><pre><code class="language-none">p = ImagePreprocessor((100, 100), 
                      [ImageResizeNearestNeighbour((50, 80)),
                       ImageCrop(1:30, 10:80),
                       x -&gt; x ./ 256])
x = rand(UInt8, 100, 100)
s = ReinforcementLearning.preprocessstate(p, x)</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/c66bbc8e46d3715326d3f137250a93a1b8ad7636/src/components/preprocessors.jl#L165-L183">source</a></section><footer><hr/><a class="previous" href="../agents/"><span class="direction">Previous</span><span class="title">Agents</span></a><a class="next" href="../buffers/"><span class="direction">Next</span><span class="title">Buffers</span></a></footer></article></body></html>
