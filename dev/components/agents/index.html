<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Agents · ReinforcementLearning.jl</title><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-149861753-1', 'auto');
ga('send', 'pageview');
</script><link rel="canonical" href="https://juliareinforcementlearning.github.io/ReinforcementLearning.jl/latest/components/agents/index.html"/><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/documenter.css" rel="stylesheet" type="text/css"/><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/custom.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="../../"><img class="logo" src="../../assets/logo.png" alt="ReinforcementLearning.jl logo"/></a><h1>ReinforcementLearning.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../../">Home</a></li><li><a class="toctext" href="../../a_quick_example/">A Quick Example</a></li><li><a class="toctext" href="../../overview/">Overview</a></li><li><span class="toctext">Manual</span><ul><li><span class="toctext">Components</span><ul><li class="current"><a class="toctext" href>Agents</a><ul class="internal"></ul></li><li><a class="toctext" href="../environments/">Environments</a></li><li><a class="toctext" href="../buffers/">Buffers</a></li><li><a class="toctext" href="../policies/">Policies</a></li><li><a class="toctext" href="../learners/">Learners</a></li><li><a class="toctext" href="../approximators/">Approximators</a></li><li><a class="toctext" href="../action_selectors/">Action Selectors</a></li><li><a class="toctext" href="../environment_models/">Environment Models</a></li></ul></li><li><a class="toctext" href="../../core/">Core</a></li><li><a class="toctext" href="../../utils/">Utils</a></li></ul></li><li><a class="toctext" href="../../tips_for_developers/">Tips for Developers</a></li></ul></nav><article id="docs"><header><nav><ul><li>Manual</li><li>Components</li><li><a href>Agents</a></li></ul><a class="edit-page" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/master/docs/src/components/agents.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Agents</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Agents-1" href="#Agents-1">Agents</a></h1><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.AbstractAgent" href="#ReinforcementLearning.AbstractAgent"><code>ReinforcementLearning.AbstractAgent</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>An agent is a functional object, which takes in an observation and returns an action. Agents must also implement the <code>update!(agent::AbstractAgent, obs_action::Pair)</code> method to indicate how to update the internal state of the agent.</p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/af0159254e112b8e546cc0a48f3909a1aa9b8fad/src/components/agents/abstract_agent.jl#L3-L5">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.Agent" href="#ReinforcementLearning.Agent"><code>ReinforcementLearning.Agent</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">Agent(;kwargs...)</code></pre><p>One of the most commonly used <a href="#ReinforcementLearning.AbstractAgent"><code>AbstractAgent</code></a>.</p><p>Generally speaking, it does nothing but</p><ol><li>Pass observation to the policy to generate an action</li><li>Update the buffer using the <code>observation =&gt; action</code> pair</li><li>Update the policy with the newly updated buffer</li></ol><p><strong>Keywords &amp; Fields</strong></p><ul><li><code>π</code>::<a href="../policies/#ReinforcementLearning.AbstractPolicy"><code>AbstractPolicy</code></a>: the policy to use</li><li><code>buffer</code>::<a href="../buffers/#ReinforcementLearning.AbstractTurnBuffer"><code>AbstractTurnBuffer</code></a>: used to store transitions between agent and environment</li><li><code>role=:DEFAULT</code>: used to distinguish different agents</li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaLang/julia/blob/8c4656b97aaa0e2c7491c84cacbc70afc84d6601/base/#L0-L16">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="ReinforcementLearning.DynaAgent" href="#ReinforcementLearning.DynaAgent"><code>ReinforcementLearning.DynaAgent</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">DynaAgent(;kwargs...)</code></pre><p><code>DynaAgent</code> is first introduced in: <em>Sutton, Richard S. &quot;Dyna, an integrated architecture for learning, planning, and reacting.&quot; ACM Sigart Bulletin 2.4 (1991): 160-163.</em></p><p><strong>Keywords &amp; Fields</strong></p><ul><li><code>π</code>::<a href="../policies/#ReinforcementLearning.AbstractPolicy"><code>AbstractPolicy</code></a>: the policy to use</li><li><code>model</code>::<a href="../environment_models/#ReinforcementLearning.AbstractEnvironmentModel"><code>AbstractEnvironmentModel</code></a>: describe the environment to interact with</li><li><code>buffer</code>::<a href="../buffers/#ReinforcementLearning.AbstractTurnBuffer"><code>AbstractTurnBuffer</code></a>: used to store transitions between agent and environment</li><li><code>role=:DEFAULT</code>: used to distinguish different agents</li><li><code>plan_step::Int=10</code>: the count of planning steps</li></ul><p>The main difference between <a href="#ReinforcementLearning.DynaAgent"><code>DynaAgent</code></a> and <a href="#ReinforcementLearning.Agent"><code>Agent</code></a> is that an environment model is involved. It is best described in the book: <em>Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</em></p><p><img src="../../assets/img/RL_book_fig_8_1.png" alt/></p><p><img src="../../assets/img/RL_book_fig_8_2.png" alt/></p></div></div><a class="source-link" target="_blank" href="https://github.com/JuliaLang/julia/blob/8c4656b97aaa0e2c7491c84cacbc70afc84d6601/base/#L0-L18">source</a></section><footer><hr/><a class="previous" href="../../overview/"><span class="direction">Previous</span><span class="title">Overview</span></a><a class="next" href="../environments/"><span class="direction">Next</span><span class="title">Environments</span></a></footer></article></body></html>
