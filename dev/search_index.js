var documenterSearchIndex = {"docs":
[{"location":"rl_zoo/#ReinforcementLearningZoo.jl-1","page":"RLZoo","title":"ReinforcementLearningZoo.jl","text":"","category":"section"},{"location":"rl_zoo/#","page":"RLZoo","title":"RLZoo","text":"ReinforcementLearningZoo.jl (RLZoo) provides some implementations of the most typical reinforcement learning algorithms.","category":"page"},{"location":"rl_zoo/#","page":"RLZoo","title":"RLZoo","text":"BasicDQNLearner\nDQNLearner\nPrioritizedDQNLearner\nRainbowLearner","category":"page"},{"location":"rl_zoo/#ReinforcementLearningZoo.BasicDQNLearner","page":"RLZoo","title":"ReinforcementLearningZoo.BasicDQNLearner","text":"BasicDQNLearner(;kwargs...)\n\nSee paper: Playing Atari with Deep Reinforcement Learning\n\nThis is the very basic implementation of DQN. Compared to the traditional Q learning, the only difference is that, in the updating step it uses a batch of transitions sampled from an experience buffer instead of current transition. And the approximator is usually a NeuralNetworkApproximator. You can start from this implementation to understand how everything is organized and how to write your own customized algorithm.\n\nKeywords\n\napproximator::AbstractApproximator: used to get Q-values of a state.\nloss_func: the loss function to use. TODO: provide a default huber_loss?\nγ::Float32=0.99f0: discount rate.\nbatch_size::Int=32\nmin_replay_history::Int=32: number of transitions that should be experienced before updating the approximator.\nseed=nothing.\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.DQNLearner","page":"RLZoo","title":"ReinforcementLearningZoo.DQNLearner","text":"DQNLearner(;kwargs...)\n\nSee paper: Human-level control through deep reinforcement learning\n\nKeywords\n\napproximator::AbstractApproximator: used to get Q-values of a state.\ntarget_approximator::AbstractApproximator: similar to approximator, but used to estimate the target (the next state).\nloss_func: the loss function.\nγ::Float32=0.99f0: discount rate.\nbatch_size::Int=32\nupdate_horizon::Int=1: length of update ('n' in n-step update).\nmin_replay_history::Int=32: number of transitions that should be experienced before updating the approximator.\nupdate_freq::Int=4: the frequency of updating the approximator.\ntarget_update_freq::Int=100: the frequency of syncing target_approximator.\nstack_size::Union{Int, Nothing}=4: use the recent stack_size frames to form a stacked state.\nseed = nothing\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.PrioritizedDQNLearner","page":"RLZoo","title":"ReinforcementLearningZoo.PrioritizedDQNLearner","text":"PrioritizedDQNLearner(;kwargs...)\n\nSee paper: Prioritized Experience Replay\n\nKeywords\n\napproximator::AbstractApproximator: used to get Q-values of a state.\ntarget_approximator::AbstractApproximator: similar to approximator, but used to estimate the target (the next state).\nloss_func: the loss function.\nγ::Float32=0.99f0: discount rate.\nbatch_size::Int=32\nupdate_horizon::Int=1: length of update ('n' in n-step update).\nmin_replay_history::Int=32: number of transitions that should be experienced before updating the approximator.\nupdate_freq::Int=4: the frequency of updating the approximator.\ntarget_update_freq::Int=100: the frequency of syncing target_approximator.\nstack_size::Union{Int, Nothing}=4: use the recent stack_size frames to form a stacked state.\ndefault_priority::Float64=100.: the default priority for newly added transitions.\nseed = nothing\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.RainbowLearner","page":"RLZoo","title":"ReinforcementLearningZoo.RainbowLearner","text":"RainbowLearner(;kwargs...)\n\nSee paper: Rainbow: Combining Improvements in Deep Reinforcement Learning\n\nKeywords\n\napproximator::AbstractApproximator: used to get Q-values of a state.\ntarget_approximator::AbstractApproximator: similar to approximator, but used to estimate the target (the next state).\nloss_func: the loss function.\nVₘₐₓ::Float32: the maximum value of distribution.\nVₘᵢₙ::Float32: the minimum value of distribution.\nn_actions::Int: number of possible actions.\nγ::Float32=0.99f0: discount rate.\nbatch_size::Int=32\nupdate_horizon::Int=1: length of update ('n' in n-step update).\nmin_replay_history::Int=32: number of transitions that should be experienced before updating the approximator.\nupdate_freq::Int=4: the frequency of updating the approximator.\ntarget_update_freq::Int=500: the frequency of syncing target_approximator.\nstack_size::Union{Int, Nothing}=4: use the recent stack_size frames to form a stacked state.\ndefault_priority::Float64=100.: the default priority for newly added transitions.\nn_atoms::Int=51: the number of buckets of the value function distribution.\nstack_size::Union{Int, Nothing}=4: use the recent stack_size frames to form a stacked state.\ndefault_priority::Float64=100.: the default priority for newly added transitions.\nseed = nothing\n\n\n\n\n\n","category":"type"},{"location":"experiments/atari_dqn/#Playing-the-Atari-Games-with-DQN-1","page":"Play Atari Games with DQN","title":"Playing the Atari Games with DQN","text":"","category":"section"},{"location":"experiments/atari_dqn/#","page":"Play Atari Games with DQN","title":"Play Atari Games with DQN","text":"Here we provide an example to show you how to play the Atari games with DQN. First, let's install some required packages:","category":"page"},{"location":"experiments/atari_dqn/#","page":"Play Atari Games with DQN","title":"Play Atari Games with DQN","text":"] add ArcadeLearningEnvironment. Please note that this package only works on Linux systems. For now, it doesn't use the BinaryBuilder.jl and you have to make sure that some necessary building tools are properly installed(Good First Issue!!!). If you are using Ubuntu, you are lucky. sudo apt-get install -y --no-install-recommends cmake build-essential libz-dev unzip should be enough.\n] add ReinforcementLearningEnvironments. This package provides some unified interfaces.\n] add ReinforcementLearning#master. This package is still under rapid development, so we need to install the master branch.\n] add CuArrays. To enable the GPU support.\n] add Flux. We need the latest Flux to use Zygote.jl for backprop.\n] add Plots. To plot the rewards.","category":"page"},{"location":"experiments/atari_dqn/#","page":"Play Atari Games with DQN","title":"Play Atari Games with DQN","text":"Now we can try to train a DQN agent.","category":"page"},{"location":"experiments/atari_dqn/#","page":"Play Atari Games with DQN","title":"Play Atari Games with DQN","text":"using ReinforcementLearning, ReinforcementLearningEnvironments, ArcadeLearningEnvironment, Flux, Plots\n\nusing Random\nRandom.seed!(11)\n\nstate_size = (84, 84)\nn_frames = 4\n\nenv = WrappedEnv(\n    env = AtariEnv(;\n        name=\"pong\",\n        grayscale_obs=true,\n        noop_max=30,\n        frame_skip=4,\n        terminal_on_life_loss=false,\n        repeat_action_probability=0.,\n        max_num_frames_per_episode=4 * 100000,\n        color_averaging=false,\n        full_action_space=false,\n        seed=(22, 33)\n        ),\n    preprocessor = Chain(\n        ImageResize(84, 84),\n        StackFrames(state_size..., n_frames)\n    )\n)\n\nna = length(action_space(env))\n\ndevice = :gpu\n\nagent = Agent(\n    π = QBasedPolicy(\n        learner = DQNLearner(\n            approximator = NeuralNetworkQ(\n                model = Chain(\n                    x -> x ./ 255,\n                    Conv((8,8), n_frames => 32, relu; stride=4),\n                    Conv((4,4), 32 => 64, relu; stride=2),\n                    Conv((3,3), 64 => 64, relu; stride=1),\n                    x -> reshape(x, :, size(x)[end]),\n                    Dense(7*7*64, 512, relu),\n                    Dense(512, na),\n                    ),\n                optimizer = ADAM(0.00001),\n                device = device\n            ),\n            target_approximator = NeuralNetworkQ(\n                model = Chain(\n                    x -> x ./ 255,\n                    Conv((8,8), n_frames => 32, relu; stride=4),\n                    Conv((4,4), 32 => 64, relu; stride=2),\n                    Conv((3,3), 64 => 64, relu; stride=1),\n                    x -> reshape(x, :, size(x)[end]),\n                    Dense(7*7*64, 512, relu),\n                    Dense(512, na),\n                    ),\n                optimizer = ADAM(0.00001),\n                device = device\n            ),\n            update_freq = 4,\n            γ = 0.99f0,\n            update_horizon = 1,\n            batch_size = 32,\n            stack_size = n_frames,\n            min_replay_history = 10000,\n            loss_fun = huber_loss,\n            target_update_freq = 1000,\n        ),\n        selector = EpsilonGreedySelector{:exp}(ϵ_init=1.0, ϵ_stable = 0.01, decay_steps = 30000),\n    ),\n    buffer = circular_RTSA_buffer(\n        capacity = 100000,\n        state_eltype = Float32,\n        state_size = state_size,\n    )\n)\n\nhook = ComposedHook(\n    TotalRewardPerEpisode(),\n    TimePerStep()\n);\n\nrun(agent, env, StopAfterStep(3000000; is_show_progress=true); hook = hook)","category":"page"},{"location":"experiments/atari_dqn/#","page":"Play Atari Games with DQN","title":"Play Atari Games with DQN","text":"Finally we can plot the rewards of each episode:","category":"page"},{"location":"experiments/atari_dqn/#","page":"Play Atari Games with DQN","title":"Play Atari Games with DQN","text":"plot(hook[1].rewards, xlabel=\"Episode\", ylabel=\"Reward\", label=\"\")","category":"page"},{"location":"experiments/atari_dqn/#","page":"Play Atari Games with DQN","title":"Play Atari Games with DQN","text":"(Image: )","category":"page"},{"location":"rl_envs/#ReinforcementLearningEnvironments.jl-1","page":"RLEnvs","title":"ReinforcementLearningEnvironments.jl","text":"","category":"section"},{"location":"rl_envs/#","page":"RLEnvs","title":"RLEnvs","text":"ReinforcementLearningEnvironments.jl (RLEnvs) provides some very common environments (for example: CartPoleEnv) together with some wrappers for 3-rd party environments.","category":"page"},{"location":"rl_envs/#","page":"RLEnvs","title":"RLEnvs","text":"To use those extra environments with ReinforcementLearning.jl, you have to manually install those dependent packages listed here. And use it like this:","category":"page"},{"location":"rl_envs/#","page":"RLEnvs","title":"RLEnvs","text":"julia> using ArcadeLearningEnvironment\n┌ Warning: Package ReinforcementLearningEnvironments does not have ArcadeLearningEnvironment in its dependencies:\n│ - If you have ReinforcementLearningEnvironments checked out for development and have\n│   added ArcadeLearningEnvironment as a dependency but haven't updated your primary\n│   environment's manifest file, try `Pkg.resolve()`.\n│ - Otherwise you may need to report an issue with ReinforcementLearningEnvironments\n└ Loading ArcadeLearningEnvironment into ReinforcementLearningEnvironments from project dependency, future warnings for ReinforcementLearningEnvironments are suppressed.\n\njulia> using ReinforcementLearning.ReinforcementLearningEnvironments\n\njulia> env = AtariEnv()","category":"page"},{"location":"rl_envs/#","page":"RLEnvs","title":"RLEnvs","text":"The warning above can be safely ignored. Especially notice that you need to execute using ReinforcementLearning.ReinforcementLearningEnvironments.","category":"page"},{"location":"rl_envs/#","page":"RLEnvs","title":"RLEnvs","text":"CartPoleEnv\nMountainCarEnv\nPendulumEnv\nPendulumNonInteractiveEnv","category":"page"},{"location":"rl_envs/#ReinforcementLearningEnvironments.CartPoleEnv","page":"RLEnvs","title":"ReinforcementLearningEnvironments.CartPoleEnv","text":"CartPoleEnv(;kwargs...)\n\nKeyword arguments\n\nT = Float64\ngravity = T(9.8)\nmasscart = T(1.0)\nmasspole = T(0.1)\nhalflength = T(0.5)\nforcemag = T(10.0)\nmax_steps = 200\nseed = nothing\n\n\n\n\n\n","category":"type"},{"location":"rl_envs/#ReinforcementLearningEnvironments.MountainCarEnv","page":"RLEnvs","title":"ReinforcementLearningEnvironments.MountainCarEnv","text":"MountainCarEnv(;kwargs...)\n\nKeyword arguments\n\nT = Float64\ncontinuous = false\nseed = nothing\nmin_pos = -1.2\nmax_pos = 0.6\nmax_speed = 0.07\ngoal_pos = 0.5\nmax_steps = 200\ngoal_velocity = 0.0\npower = 0.001\ngravity = 0.0025\n\n\n\n\n\n","category":"type"},{"location":"rl_envs/#ReinforcementLearningEnvironments.PendulumEnv","page":"RLEnvs","title":"ReinforcementLearningEnvironments.PendulumEnv","text":"PwendulumEnv(;kwargs...)\n\nKeyword arguments\n\nT = Float64\nmax_speed = T(8)\nmax_torque = T(2)\ng = T(10)\nm = T(1)\nl = T(1)\ndt = T(0.05)\nmax_steps = 200\nseed = nothing\n\n\n\n\n\n","category":"type"},{"location":"rl_envs/#ReinforcementLearningEnvironments.PendulumNonInteractiveEnv","page":"RLEnvs","title":"ReinforcementLearningEnvironments.PendulumNonInteractiveEnv","text":"A non-interactive pendulum environment.\n\nAccepts only nothing actions, which result in the system being simulated for one time step. Sets env.done to true once maximum_time is reached. Resets to a random position and momentum. Always returns zero rewards.\n\nUseful for debugging and development purposes, particularly in model-based reinforcement learning.\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.jl-1","page":"RLBase","title":"ReinforcementLearningBase.jl","text":"","category":"section"},{"location":"rl_base/#","page":"RLBase","title":"RLBase","text":"RLBase","category":"page"},{"location":"rl_base/#ReinforcementLearningBase.RLBase","page":"RLBase","title":"ReinforcementLearningBase.RLBase","text":"ReinforcementLearningBase.jl (RLBase) provides some common constants, traits, abstractions and interfaces in developing reinforcement learning algorithms in  Julia. From the concept level, they can be organized in the following parts:\n\nGeneral\nAgent\nPolicy\nLearner\nApproximator\nExplorer\nEnvironmentModel\nEnvironment\nPreprocessor\nObservation\nTrajectory\nSpace\n\n\n\n\n\n","category":"module"},{"location":"rl_base/#General-1","page":"RLBase","title":"General","text":"","category":"section"},{"location":"rl_base/#","page":"RLBase","title":"RLBase","text":"AbstractStage","category":"page"},{"location":"rl_base/#ReinforcementLearningBase.AbstractStage","page":"RLBase","title":"ReinforcementLearningBase.AbstractStage","text":"In the simplest case, we just need to run env |> observe |> agent |> env repeatly. But usually we would also like to control when/how to update the agent. So we defined the following stages for the help:\n\nPRE_EXPERIMENT_STAGE\nPRE_EPISODE_STAGE\nPRE_ACT_STAGE\nPOST_ACT_STAGE\nPOST_EPISODE_STAGE\nPOST_EXPERIMENT_STAGE\n\n                      +-----------------------------------------------------------+                      \n                      |Episode                                                    |                      \n                      |                                                           |                      \nPRE_EXPERIMENT_STAGE  |            PRE_ACT_STAGE    POST_ACT_STAGE                | POST_EXPERIMENT_STAGE\n         |            |                  |                |                       |          |           \n         v            |        +-----+   v   +-------+    v   +-----+             |          v           \n         --------------------->+ env +------>+ agent +------->+ env +---> ... ------->......             \n                      |  ^     +-----+  obs  +-------+ action +-----+          ^  |                      \n                      |  |                                                     |  |                      \n                      |  +--PRE_EPISODE_STAGE            POST_EPISODE_STAGE----+  |                      \n                      |                                                           |                      \n                      |                                                           |                      \n                      +-----------------------------------------------------------+                      \n\n\n\n\n\n","category":"type"},{"location":"rl_base/#Agent-1","page":"RLBase","title":"Agent","text":"","category":"section"},{"location":"rl_base/#","page":"RLBase","title":"RLBase","text":"AbstractAgent\nget_role\nDEFAULT_PLAYER","category":"page"},{"location":"rl_base/#ReinforcementLearningBase.AbstractAgent","page":"RLBase","title":"ReinforcementLearningBase.AbstractAgent","text":"(agent::AbstractAgent)(obs) = agent(PRE_ACT_STAGE, obs) -> action\n(agent::AbstractAgent)(stage::AbstractStage, obs)\n\nAn agent is a functional object which takes in an observation and returns an action. In different stages, the behavior may be different.\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.get_role","page":"RLBase","title":"ReinforcementLearningBase.get_role","text":"return DEFAULT_PLAYER by default\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.DEFAULT_PLAYER","page":"RLBase","title":"ReinforcementLearningBase.DEFAULT_PLAYER","text":"Default player instance for all the environments\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#Policy-1","page":"RLBase","title":"Policy","text":"","category":"section"},{"location":"rl_base/#","page":"RLBase","title":"RLBase","text":"AbstractPolicy\nupdate!(p::AbstractPolicy, experience)\nget_prob(p::AbstractPolicy, obs)\nget_prob(p::AbstractPolicy, obs, a)\nget_priority","category":"page"},{"location":"rl_base/#ReinforcementLearningBase.AbstractPolicy","page":"RLBase","title":"ReinforcementLearningBase.AbstractPolicy","text":"(p::AbstractPolicy)(obs) = p(obs, ActionStyle(obs)) -> action\n\nSimilar to AbstractAgent, a policy is aslo a functional object which defines how to generate action(s) given an observation of the environment. However, in the concept level, a policy is more lower and usually it doesn't need to care about how/when to interact with an environment.\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.update!-Tuple{AbstractPolicy,Any}","page":"RLBase","title":"ReinforcementLearningBase.update!","text":"update!(p::AbstractPolicy, experience)\n\nUpdate the policy p with experience\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.get_prob-Tuple{AbstractPolicy,Any}","page":"RLBase","title":"ReinforcementLearningBase.get_prob","text":"get_prob(p::AbstractPolicy, obs) = get_prob(p, obs, ActionStyle(obs))\nget_prob(p::AbstractPolicy, obs, ::AbstractActionStyle)\n\nGet the probability distribution of actions from the policy p given an observation obs\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.get_prob-Tuple{AbstractPolicy,Any,Any}","page":"RLBase","title":"ReinforcementLearningBase.get_prob","text":"get_prob(p::AbstractPolicy, obs, a) = get_prob(p, obs, ActionStyle(obs), a)\n\nGet the probability of to take action a from the policy p given an observation obs\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.get_priority","page":"RLBase","title":"ReinforcementLearningBase.get_priority","text":"get_priority(p::AbstractLearner, experience)\n\n\n\n\n\nget_priority(p::AbstractPolicy, experience)\n\nCalculate the priority of the experience to policy p\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#Learner-1","page":"RLBase","title":"Learner","text":"","category":"section"},{"location":"rl_base/#","page":"RLBase","title":"RLBase","text":"AbstractLearner\nupdate!(learner::AbstractLearner, experience)\nget_priority(p::AbstractLearner, experience)\nextract_experience(trajectory::AbstractTrajectory, learner::AbstractLearner)","category":"page"},{"location":"rl_base/#ReinforcementLearningBase.AbstractLearner","page":"RLBase","title":"ReinforcementLearningBase.AbstractLearner","text":"(learner::AbstractLearner)(obs)\n\nA learner is usually a wrapper around AbstractApproximators. It defines the expected inputs and how to update inner approximators. From the concept level, it assumes that the necessary training data is  already cooked (usually by AbstractPolicy with the extract_experience method).\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.update!-Tuple{AbstractLearner,Any}","page":"RLBase","title":"ReinforcementLearningBase.update!","text":"update!(learner::AbstractLearner, experience)\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.get_priority-Tuple{AbstractLearner,Any}","page":"RLBase","title":"ReinforcementLearningBase.get_priority","text":"get_priority(p::AbstractLearner, experience)\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.extract_experience-Tuple{AbstractTrajectory,AbstractLearner}","page":"RLBase","title":"ReinforcementLearningBase.extract_experience","text":"extract_experience(trajectory::AbstractTrajectory, learner::AbstractLearner)\n\nExtract necessary data from the trajectory given a learner\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#Approximator-1","page":"RLBase","title":"Approximator","text":"","category":"section"},{"location":"rl_base/#","page":"RLBase","title":"RLBase","text":"AbstractApproximator\nbatch_estimate\nupdate!(a::AbstractApproximator, correction)\nBase.copyto!(dest::AbstractApproximator, src::AbstractApproximator)\nApproximatorStyle\nQ_APPROXIMATOR \nV_APPROXIMATOR\nHYBRID_APPROXIMATOR ","category":"page"},{"location":"rl_base/#ReinforcementLearningBase.AbstractApproximator","page":"RLBase","title":"ReinforcementLearningBase.AbstractApproximator","text":"(app::AbstractApproximator)(obs) = app(get_state(obs))\n\nAn approximator is a functional object for value estimation. It serves as a black box to provides an abstraction over different  kinds of approximate methods (for example DNN provided by Flux or Knet).\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.batch_estimate","page":"RLBase","title":"ReinforcementLearningBase.batch_estimate","text":"batch_estimate(app::AbstractApproximator, states)\n\nThe states is assume to be a batch of states\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.update!-Tuple{AbstractApproximator,Any}","page":"RLBase","title":"ReinforcementLearningBase.update!","text":"update!(a::AbstractApproximator, correction)\n\nUsually the correction is the gradient of inner parameters.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#Base.copyto!-Tuple{AbstractApproximator,AbstractApproximator}","page":"RLBase","title":"Base.copyto!","text":"Base.copyto!(dest::AbstractApproximator, src::AbstractApproximator)\n\nCopy the internal params from src approximator to dest approximator\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.ApproximatorStyle","page":"RLBase","title":"ReinforcementLearningBase.ApproximatorStyle","text":"ApproximatorStyle(x::AbstractApproximator)\n\nDetect which kind of approximator it is\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.Q_APPROXIMATOR","page":"RLBase","title":"ReinforcementLearningBase.Q_APPROXIMATOR","text":"For Q_APPROXIMATOR, we assume that the following methods are implemented:\n\n(Q::AbstractApproximator)(s, a), estimate the Q value.\n(Q::AbstractApproximator)(s), estimate the Q value among all possible actions.\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.V_APPROXIMATOR","page":"RLBase","title":"ReinforcementLearningBase.V_APPROXIMATOR","text":"For V_APPROXIMATOR, we assume that (V::AbstractApproximator)(s) is implemented.\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.HYBRID_APPROXIMATOR","page":"RLBase","title":"ReinforcementLearningBase.HYBRID_APPROXIMATOR","text":"For HYBRID_APPROXIMATOR, the following methods are assumed to be implemented:\n\n(app::AbstractApproximator)(s, ::Val{:Q}), estimate the action values of state s.\n(app::AbstractApproximator)(s, ::Val{:V}), estimate the state values.\n(app::AbstractApproximator)(s, a), estimate state-action values.\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#Explorer-1","page":"RLBase","title":"Explorer","text":"","category":"section"},{"location":"rl_base/#","page":"RLBase","title":"RLBase","text":"AbstractExplorer\nreset!(p::AbstractExplorer)\nBase.copy(p::AbstractExplorer)\nget_prob(p::AbstractExplorer, x)\nget_prob(p::AbstractExplorer, x, mask)","category":"page"},{"location":"rl_base/#ReinforcementLearningBase.AbstractExplorer","page":"RLBase","title":"ReinforcementLearningBase.AbstractExplorer","text":"(p::AbstractExplorer)(x)\n(p::AbstractExplorer)(x, mask)\n\nDefine how to select an action based on action values.\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.reset!-Tuple{AbstractExplorer}","page":"RLBase","title":"ReinforcementLearningBase.reset!","text":"Reset the internal state of an explorer p\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#Base.copy-Tuple{AbstractExplorer}","page":"RLBase","title":"Base.copy","text":"The internal state is also copied\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.get_prob-Tuple{AbstractExplorer,Any}","page":"RLBase","title":"ReinforcementLearningBase.get_prob","text":"get_prob(p::AbstractExplorer, x) -> AbstractDistribution\n\nGet the action distribution given action values\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.get_prob-Tuple{AbstractExplorer,Any,Any}","page":"RLBase","title":"ReinforcementLearningBase.get_prob","text":"get_prob(p::AbstractExplorer, x, mask)\n\nSimilart to get_prob(p::AbstractExplorer, x), but here only the masked elements are considered.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#EnvironmentModel-1","page":"RLBase","title":"EnvironmentModel","text":"","category":"section"},{"location":"rl_base/#","page":"RLBase","title":"RLBase","text":"AbstractEnvironmentModel","category":"page"},{"location":"rl_base/#ReinforcementLearningBase.AbstractEnvironmentModel","page":"RLBase","title":"ReinforcementLearningBase.AbstractEnvironmentModel","text":"Describe how to model a reinforcement learning environment.\n\nTODO: need more investigation\n\nRef: https://bair.berkeley.edu/blog/2019/12/12/mbpo/\n\nAnalytic gradient computation\nSampling-based planning\nModel-based data generation\nValue-equivalence prediction\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#Environment-1","page":"RLBase","title":"Environment","text":"","category":"section"},{"location":"rl_base/#","page":"RLBase","title":"RLBase","text":"AbstractEnv \nDynamicStyle\nSIMULTANEOUS\nSEQUENTIAL \nget_action_space\nget_observation_space\nget_current_player\nget_num_players\nrender\nreset!(::AbstractEnv)","category":"page"},{"location":"rl_base/#ReinforcementLearningBase.AbstractEnv","page":"RLBase","title":"ReinforcementLearningBase.AbstractEnv","text":"(env::AbstractEnv)(action) = env(DEFAULT_PLAYER, action) -> nothing\n(env::AbstractEnv)(player, action) -> nothing\n\nSuper type of all reinforcement learning environments. An environments is a functional object which takes in an action and returns nothing.\n\nnote: Note\nSo why don't we adopt the step! method like OpenAI Gym here? The reason is that the async manner will simplify a lot of things here.\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.DynamicStyle","page":"RLBase","title":"ReinforcementLearningBase.DynamicStyle","text":"DynamicStyle(x::AbstractEnv) = SEQUENTIAL\n\nDetermine whether the players can play simultaneous or not. Default value is SEQUENTIAL\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.SIMULTANEOUS","page":"RLBase","title":"ReinforcementLearningBase.SIMULTANEOUS","text":"Environment with the DynamicStyle of SIMULTANEOUS must take in actions from some (or all) players at one time\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.SEQUENTIAL","page":"RLBase","title":"ReinforcementLearningBase.SEQUENTIAL","text":"Environment with the DynamicStyle of SEQUENTIAL must takes actions from different players one-by-one.\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.get_action_space","page":"RLBase","title":"ReinforcementLearningBase.get_action_space","text":"get_action_space(env::AbstractEnv) -> AbstractSpace\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.get_observation_space","page":"RLBase","title":"ReinforcementLearningBase.get_observation_space","text":"get_observation_space(env::AbstractEnv) -> AbstractSpace\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.get_current_player","page":"RLBase","title":"ReinforcementLearningBase.get_current_player","text":"Return DEFAULT_PLAYER by default\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.get_num_players","page":"RLBase","title":"ReinforcementLearningBase.get_num_players","text":"get_num_players(env::AbstractEnv) -> Int\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.render","page":"RLBase","title":"ReinforcementLearningBase.render","text":"Show the environment in a user-friendly manner\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.reset!-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.reset!","text":"Reset the internal state of an environment\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#","page":"RLBase","title":"RLBase","text":"Several meta-environments are also provided:","category":"page"},{"location":"rl_base/#","page":"RLBase","title":"RLBase","text":"WrappedEnv","category":"page"},{"location":"rl_base/#ReinforcementLearningBase.WrappedEnv","page":"RLBase","title":"ReinforcementLearningBase.WrappedEnv","text":"WrappedEnv(;preprocessor, env, postprocessor=identity)\n\nThe observation of the inner env is first transformed by the preprocessor. And the action is transformed by postprocessor and then send to the inner env.\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#Preprocessor-1","page":"RLBase","title":"Preprocessor","text":"","category":"section"},{"location":"rl_base/#","page":"RLBase","title":"RLBase","text":"AbstractPreprocessor","category":"page"},{"location":"rl_base/#ReinforcementLearningBase.AbstractPreprocessor","page":"RLBase","title":"ReinforcementLearningBase.AbstractPreprocessor","text":"Preprocess an observation and return a new observation.\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#Observation-1","page":"RLBase","title":"Observation","text":"","category":"section"},{"location":"rl_base/#","page":"RLBase","title":"RLBase","text":"observe\nget_legal_actions_mask\nget_legal_actions\nget_terminal\nget_reward\nget_state\nget_invalid_action\nINVALID_ACTION\nActionStyle\nMINIMAL_ACTION_SET\nFULL_ACTION_SET\nRTSA\nSARTSA","category":"page"},{"location":"rl_base/#ReinforcementLearningBase.observe","page":"RLBase","title":"ReinforcementLearningBase.observe","text":"observe(env::AbstractEnv) = observe(env, get_current_player(env))\nobserve(::AbstractEnv, player)\n\nGet an observation of the env from the perspective of an player.\n\nnote: Note\nThis is a very deliberate decision to adopt the duck-typing here to describe an observation from an environment. By default, we assume an observation is a NamedTuple, which is the most common case. But of course it can be of any type, as long as it implemented  the necessay methods described in this section.\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.get_legal_actions_mask","page":"RLBase","title":"ReinforcementLearningBase.get_legal_actions_mask","text":"get_legal_actions_mask(x) -> Bool[]\n\nOnly valid for observations of FULL_ACTION_SET.\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.get_legal_actions","page":"RLBase","title":"ReinforcementLearningBase.get_legal_actions","text":"get_legal_actions(x)\n\nOnly valid for observations of FULL_ACTION_SET.\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.get_terminal","page":"RLBase","title":"ReinforcementLearningBase.get_terminal","text":"get_terminal(x) -> bool\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.get_reward","page":"RLBase","title":"ReinforcementLearningBase.get_reward","text":"get_reward(x) -> Number\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.get_state","page":"RLBase","title":"ReinforcementLearningBase.get_state","text":"get_state(x) -> Array\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.get_invalid_action","page":"RLBase","title":"ReinforcementLearningBase.get_invalid_action","text":"By default INVALID_ACTION is returned\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.INVALID_ACTION","page":"RLBase","title":"ReinforcementLearningBase.INVALID_ACTION","text":"The default value of invalid action for all the environments. However, one can still specify the value for a specific environment or observation.\n\nSee also: get_invalid_action\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.ActionStyle","page":"RLBase","title":"ReinforcementLearningBase.ActionStyle","text":"ActionStyle(x)\n\nSpecify whether the observation contains a full action set or a minimal action set. By default the MINIMAL_ACTION_SET is returned.\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.MINIMAL_ACTION_SET","page":"RLBase","title":"ReinforcementLearningBase.MINIMAL_ACTION_SET","text":"All actions in the action space of the environment are legal\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.FULL_ACTION_SET","page":"RLBase","title":"ReinforcementLearningBase.FULL_ACTION_SET","text":"The action space of the environment may contains illegal actions\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.RTSA","page":"RLBase","title":"ReinforcementLearningBase.RTSA","text":"An alias of (:reward, :terminal, :state, :action)\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.SARTSA","page":"RLBase","title":"ReinforcementLearningBase.SARTSA","text":"An alias of (:state, :action, :reward, :terminal, :next_state, :next_action)\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#","page":"RLBase","title":"RLBase","text":"Some meta-observations are also provided:","category":"page"},{"location":"rl_base/#","page":"RLBase","title":"RLBase","text":"StateOverriddenObs","category":"page"},{"location":"rl_base/#ReinforcementLearningBase.StateOverriddenObs","page":"RLBase","title":"ReinforcementLearningBase.StateOverriddenObs","text":"StateOverriddenObs(;obs, state)\n\nReplace the internal state of obs with state.\n\nExample\n\njulia> old_obs = (reward=1.0, terminal=false, state=1)\n(reward = 1.0, terminal = false, state = 1)\n\njulia> new_obs = StateOverriddenObs(;obs=old_obs, state=nothing)\nStateOverriddenObs{NamedTuple{(:reward, :terminal, :state),Tuple{Float64,Bool,Int64}},Nothing}((reward = 1.0, terminal = false, state = 1), nothing)\n\njulia> get_state(new_obs) === nothing\ntrue\n\njulia> get_reward(new_obs) === get_reward(old_obs)\ntrue\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#Trajectory-1","page":"RLBase","title":"Trajectory","text":"","category":"section"},{"location":"rl_base/#","page":"RLBase","title":"RLBase","text":"AbstractTrajectory\nget_trace\npush!\npop!","category":"page"},{"location":"rl_base/#ReinforcementLearningBase.AbstractTrajectory","page":"RLBase","title":"ReinforcementLearningBase.AbstractTrajectory","text":"AbstractTrajectory{names,types} <: AbstractArray{NamedTuple{names,types},1}\n\nA trajectory is used to record some useful information during the interactions between agents and environments.\n\nParameters\n\nnames::NTuple{Symbol}, indicate what fields to be recorded.\ntypes::Tuple{DataType...}, the datatypes of names.\n\nThe length of names and types must match.\n\nRequired Methods:\n\nget_trace\nBase.push!(t::AbstractTrajectory, kv::Pair{Symbol})\nBase.pop!(t::AbstractTrajectory, s::Symbol)\n\nOptional Methods:\n\nBase.length\nBase.size\nBase.lastindex\nBase.isempty\nBase.empty!\nBase.push!\nBase.pop!\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.get_trace","page":"RLBase","title":"ReinforcementLearningBase.get_trace","text":"get_trace(t::AbstractTrajectory, s::Symbol)\n\n\n\n\n\nget_trace(t::AbstractTrajectory, s::NTuple{N,Symbol}) where {N}\n\n\n\n\n\nget_trace(t::AbstractTrajectory, s::Symbol...)\n\n\n\n\n\nget_trace(t::AbstractTrajectory{names}) where {names}\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#Base.push!","page":"RLBase","title":"Base.push!","text":"Base.push!(t::AbstractTrajectory; kwargs...)\n\n\n\n\n\nBase.push!(t::AbstractTrajectory, kv::Pair{Symbol})\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#Base.pop!","page":"RLBase","title":"Base.pop!","text":"Base.pop!(t::AbstractTrajectory{names}) where {names}\n\npop! out one element of each trace in t\n\n\n\n\n\nBase.pop!(t::AbstractTrajectory, s::Symbol...)\n\npop! out one element of the traces specified in s\n\n\n\n\n\nBase.pop!(t::AbstractTrajectory, s::Symbol)\n\npop! out one element of the trace s in t\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#Space-1","page":"RLBase","title":"Space","text":"","category":"section"},{"location":"rl_base/#","page":"RLBase","title":"RLBase","text":"AbstractSpace","category":"page"},{"location":"rl_base/#ReinforcementLearningBase.AbstractSpace","page":"RLBase","title":"ReinforcementLearningBase.AbstractSpace","text":"Describe the span of observations and actions. Usually the following methods are implemented:\n\nBase.length\nBase.in\nRandom.rand\nBase.eltype\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#","page":"RLBase","title":"RLBase","text":"Some typical implementations of AbstractSpace are also included in the RLBase.","category":"page"},{"location":"rl_base/#","page":"RLBase","title":"RLBase","text":"EmptySpace\nDiscreteSpace\nMultiDiscreteSpace\nContinuousSpace\nMultiContinuousSpace\nTupleSpace\nDictSpace","category":"page"},{"location":"rl_base/#ReinforcementLearningBase.EmptySpace","page":"RLBase","title":"ReinforcementLearningBase.EmptySpace","text":"EmptySpace()\n\nThere's nothing in the EmptySpace!\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.DiscreteSpace","page":"RLBase","title":"ReinforcementLearningBase.DiscreteSpace","text":"DiscreteSpace(span)\n\nThe span can be of any iterators.\n\nExample\n\njulia> s = DiscreteSpace([1,2,3])\nDiscreteSpace{Array{Int64,1}}([1, 2, 3])\n\njulia> 0 ∉ s\ntrue\n\njulia> 2 ∈ s\ntrue\n\njulia> s = DiscreteSpace(Set([:a, :c, :a, :b]))\nDiscreteSpace{Set{Symbol}}(Set(Symbol[:a, :b, :c]))\n\njulia> s = DiscreteSpace(3)\nDiscreteSpace{UnitRange{Int64}}(1:3)\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.MultiDiscreteSpace","page":"RLBase","title":"ReinforcementLearningBase.MultiDiscreteSpace","text":"MultiDiscreteSpace(low::T, high::T) where {T<:AbstractArray}\n\nSimilar to DiscreteSpace, but scaled to multi-dimension.\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.ContinuousSpace","page":"RLBase","title":"ReinforcementLearningBase.ContinuousSpace","text":"ContinuousSpace(low, high)\n\nSimilar to DiscreteSpace, but the span is continuous.\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.MultiContinuousSpace","page":"RLBase","title":"ReinforcementLearningBase.MultiContinuousSpace","text":"MultiContinuousSpace(low, high)\n\nSimilar to ContinuousSpace, but scaled to multi-dimension.\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.TupleSpace","page":"RLBase","title":"ReinforcementLearningBase.TupleSpace","text":"TupleSpace(args::AbstractSpace...)\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.DictSpace","page":"RLBase","title":"ReinforcementLearningBase.DictSpace","text":"DictSpace(ps::Pair{<:Union{Symbol,AbstractString},<:AbstractSpace}...)\n\n\n\n\n\n","category":"type"},{"location":"a_quick_example/#A-Quick-Example-1","page":"A Quick Example","title":"A Quick Example","text":"","category":"section"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"Welcome to the world of reinforcement learning in Julia! Here's a quick example to show you how to train an agent with to play the CartPoleEnv.","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"First, let's make sure that this package is properly installed.","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"using ReinforcementLearning","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"Cartpole is considered to be one of the simplest environments for DRL(Deep Reinforcement Learning) algorithms testing. The state of the Cartpole environment can be described with 4 numbers and the actions are two integers(1 and 2). Before game terminates, agent can gain a reward of +1 for each step. And the game will be forced to end after 200 steps, thus the maximum reward of an episode is 200. ","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"env = CartPoleEnv(;T=Float32, seed=123)","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"Then we create an agent to play with the cartpole environment.","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"agent = Agent(\n    policy = RandomPolicy(env;seed=456),\n    trajectory = CircularCompactSARTSATrajectory(; capacity=3, state_type=Float32, state_size = (4,)),\n)","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"An agent is usually constructed by a policy and a trajectory. A policy is a mapping from an observation to an action. And a trajectory is used to store some important information of the interactions between agents and environments. The RandomPolicy used here will do nothing but select an action randomly. And the CircularCompactSARTSATrajectory here will store the State, Action, Reward, Terminal, next-State and next-Action in each step of the latest 3 episodes.","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"Now we can start to run simulations:","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"run(agent, env, StopAfterEpisode(1))","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"Here the StopAfterEpisode(1) is a stop condition, which means stop after 1 episode here. Then we can take a look at the trajectory in the agent.","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"agent.trajectory\n# 3-element Trajectory{(:state, :action, :reward, :terminal, :next_state, :next_action),Tuple{Float32,Int64,Float32,Bool,Float32,Int64},NamedTuple{(:reward, :terminal, :state, :action),Tuple{CircularArrayBuffer{Float32,1},CircularArrayBuffer{Bool,1},CircularArrayBuffer{Float32,2},CircularArrayBuffer{Int64,1}}}}:\n#  (state = Float32[-0.116456345, -0.57231975, 0.16624497, 1.1284109], action = 2, reward = 1.0f0, terminal = false, next_state = Float32[-0.12790275, -0.37971866, 0.18881318, 0.89214355], next_action = 2)\n#  (state = Float32[-0.12790275, -0.37971866, 0.18881318, 0.89214355], action = 2, reward = 1.0f0, terminal = false, next_state = Float32[-0.13549712, -0.18759018, 0.20665605, 0.6642545], next_action = 1) \n#  (state = Float32[-0.13549712, -0.18759018, 0.20665605, 0.6642545], action = 1, reward = 0.0f0, terminal = true, next_state = Float32[-0.13924892, -0.38489604, 0.21994114, 1.0142413], next_action = 2)","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"note: Note\nSince we have set the random seed of the RandomPolicy and the CartPoleEnv, you should see the exactly same result as above.","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"To record the total reward of each episode, we can add a hook during each run.","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"hook = TotalRewardPerEpisode()\nrun(agent, env, StopAfterEpisode(10000), hook)\nsum(hook.rewards)/10000  # 21.0591","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"Playing the cartpole environment with a RandomPolicy is not very interesting. Next we will use a DQN to solve the problem.","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"Because this package relies on Flux.jl to build deep learning models, you need to manually install Flux to run the following example. To show the rewards of each episode and the fps, Plots.jl and StatsBase.jl are also required.","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"using Flux\nusing ReinforcementLearning\nusing StatsBase:mean\n\nenv = CartPoleEnv(; T = Float32, seed = 11)\nns, na = length(rand(get_observation_space(env))), length(get_action_space(env))\nagent = Agent(\n    policy = QBasedPolicy(\n        learner = BasicDQNLearner(\n            approximator = NeuralNetworkApproximator(\n                model = Chain(\n                    Dense(ns, 128, relu; initW = seed_glorot_uniform(seed = 17)),\n                    Dense(128, 128, relu; initW = seed_glorot_uniform(seed = 23)),\n                    Dense(128, na; initW = seed_glorot_uniform(seed = 39)),\n                ) |> gpu,\n                optimizer = ADAM(),\n            ),\n            batch_size = 32,\n            min_replay_history = 100,\n            loss_func = huber_loss,\n            seed = 22,\n        ),\n        explorer = EpsilonGreedyExplorer(\n            kind = :exp,\n            ϵ_stable = 0.01,\n            decay_steps = 500,\n            seed = 33,\n        ),\n    ),\n    trajectory = CircularCompactSARTSATrajectory(\n        capacity = 1000,\n        state_type = Float32,\n        state_size = (ns,),\n    ),\n)\nhook = ComposedHook(TotalRewardPerEpisode(), TimePerStep())\nrun(agent, env, StopAfterStep(10000), hook)\n\n@info \"stats for BasicDQNLearner\" avg_reward = mean(hook[1].rewards) avg_fps = 1 / mean(hook[2].times)\n# ┌ Info: stats for BasicDQNLearner\n# │   avg_reward = 107.43478260869566\n# └   avg_fps = 531.283841452491","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"The main difference here is that, now we are using a QBasedPolicy instead of a RandomPolicy. A model of three Dense layers is used to calculate the Q values.","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"Relax! We promise that all the new concepts above will be explained in detail later.","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"Now we can also plot the rewards stored in our hook:","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"using Plots\nplot(hook[1].rewards, xlabel=\"Episode\", ylabel=\"Reward\", label=\"\")","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"(Image: )","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"That's fantastic!","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"\"But I'm new to Julia and RL. Can I learn RL by using this package?\"\nYes! One of this package's main goals is to be educational. Reinforcement Learning: An Introduction is a good introductory book. And we reproduce almost all the examples mentioned in that book by using this package here.\n\"What if I have a solid background in RL but new to Julia?\"\nProgramming isn't hard. Programming well is very hard!  - CS 3110\nFortunately, Julia provides some amazing features together with many awesome packages to make things much easier. We provide a Tips for Developers section to help you grasp Julia in depth.\n\"I'm experienced in both Julia and RL. But I find it hard to use this package...\"\nAlthough we tried our best to make concepts and codes as simple as possible, it is still possible that they are not very intuitive enough. So do not hesitate to JOIN US (create an issue or a PR). We need YOU to improve all this stuff together!","category":"page"},{"location":"overview/#Overview-1","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"overview/#","page":"Overview","title":"Overview","text":"Before diving into details, let's review some basic concepts in RL(Reinforcement Learning) first. Then we'll gradually introduce the relationship between those concepts and our implementations in this package.","category":"page"},{"location":"overview/#Key-Concepts-1","page":"Overview","title":"Key Concepts","text":"","category":"section"},{"location":"overview/#Agent-and-Environment-1","page":"Overview","title":"Agent and Environment","text":"","category":"section"},{"location":"overview/#","page":"Overview","title":"Overview","text":"<img src=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/raw/master/docs/src/assets/img/agent_env_relation.png\" width=\"640px\">","category":"page"},{"location":"overview/#","page":"Overview","title":"Overview","text":"Generally speaking, RL is to learn how to take actions so as to maximize a numerical reward. Two core concepts in RL are Agent and Environment. In each step, the agent is provided with the observation of the environment and is required to take an action. Then the environment consumes that action and transites to another state, providing a numerical reward in the meantime.","category":"page"},{"location":"overview/#","page":"Overview","title":"Overview","text":"note: Note\nSometimes people are confused about the concept of state and observation (see also the discussion here). In this package, we treat all the information we can get from the perspective of an agent in each step as an observation, including state, reward and some other extra info.Here we adopt the idea of Duck typing to describe the observation from an environment. See Environments for more details.","category":"page"},{"location":"overview/#","page":"Overview","title":"Overview","text":"In this package, Agent is one of the most typical subtypes of AbstractAgent. And you may find different kinds of Environments provided in ReinforcementLearningEnvironments.jl. Agents and environments are required to be functional objects. So we can use the piping operator (|>) to simulate the steps implied in the above picture: env |> observe |> agent |> env. See Agents and Environments for more some concrete implementations.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"<div align=\"center\">\n  <p>\n  <img src=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/raw/master/docs/src/assets/logo.svg?sanitize=true\" width=\"320px\">\n  </p>\n</div>","category":"page"},{"location":"#","page":"Home","title":"Home","text":"ReinforcementLearning.jl, as the name says, is a package for reinforcement learning research in Julia.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Our design principles are:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Reusability and extensibility: Provide elaborately designed components and interfaces to help users implement new algorithms.\nEasy experimentation: Make it easy for new users to run benchmark experiments, compare different algorithms, evaluate and diagnose agents.\nReproducibility: Facilitate reproducibility from traditional tabular methods to modern deep reinforcement learning algorithms.","category":"page"},{"location":"#Installation-1","page":"Home","title":"Installation","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"This package can be installed from the package manager in Julia's REPL:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"] add ReinforcementLearning","category":"page"},{"location":"#Project-Structure-1","page":"Home","title":"Project Structure","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"ReinforcementLearning.jl itself is just a wrapper around several other packages inside the JuliaReinforcementLearning org. The relationship between different packages is described below:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"<pre>+-------------------------------------------------------------------------------------------+\n|                                                                                           |\n|  <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl\">ReinforcementLearning.jl</a>                                                                 |\n|                                                                                           |\n|      +------------------------------+                                                     |\n|      | <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearningBase.jl\">ReinforcementLearningBase.jl</a> |                                                     |\n|      +--------|---------------------+                                                     |\n|               |                                                                           |\n|               |         +--------------------------------------+                          |\n|               |         | <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironments.jl\">ReinforcementLearningEnvironments.jl</a> |                          |\n|               |         |                                      |                          |\n|               |         |     (Conditionally depends on)       |                          |\n|               |         |                                      |                          |\n|               |         |     <a href=\"https://github.com/JuliaReinforcementLearning/ArcadeLearningEnvironment.jl\">ArcadeLearningEnvironment.jl</a>     |                          |\n|               +--------&gt;+     <a href=\"https://github.com/JuliaReinforcementLearning/OpenSpiel.jl\">OpenSpiel.jl</a>                     |                          |\n|               |         |     <a href=\"https://github.com/JuliaPOMDP/POMDPs.jl\">POMDPs.jl</a>                        |                          |\n|               |         |     <a href=\"https://github.com/JuliaPy/PyCall.jl\">PyCall.jl</a>                        |                          |\n|               |         |     <a href=\"https://github.com/JuliaReinforcementLearning/ViZDoom.jl\">ViZDoom.jl</a>                       |                          |\n|               |         |     Maze.jl(WIP)                     |                          |\n|               |         +--------------------------------------+                          |\n|               |                                                                           |\n|               |         +------------------------------+                                  |\n|               +--------&gt;+ <a href=\"\">ReinforcementLearningCore.jl</a> |                                  |\n|                         +--------|---------------------+                                  |\n|                                  |                                                        |\n|                                  |          +-----------------------------+               |\n|                                  |---------&gt;+ <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearningZoo.jl\">ReinforcementLearningZoo.jl</a> |               |\n|                                  |          +-----------------------------+               |\n|                                  |                                                        |\n|                                  |          +----------------------------------------+    |\n|                                  +---------&gt;+ <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearningAnIntroduction.jl\">ReinforcementLearningAnIntroduction.jl</a> |    |\n|                                             +----------------------------------------+    |\n+-------------------------------------------------------------------------------------------+\n</pre>","category":"page"},{"location":"#","page":"Home","title":"Home","text":"note: Note\nReinforcementLearningAnIntroduction.jl contains some traditional reinforcement algorithms and it is not registered yet. So it is not included in ReinforcementLearning.jl. The reason to do so is to ease the burden of maintainance.","category":"page"},{"location":"rl_core/#ReinforcementLearningCore.jl-1","page":"RLCore","title":"ReinforcementLearningCore.jl","text":"","category":"section"},{"location":"rl_core/#","page":"RLCore","title":"RLCore","text":"RLCore","category":"page"},{"location":"rl_core/#ReinforcementLearningCore.RLCore","page":"RLCore","title":"ReinforcementLearningCore.RLCore","text":"ReinforcementLearningCore.jl (RLCore) provides some standard and reusable components defined by RLBase, hoping that they are useful for people to implement and experiment with different kinds of algorithms.\n\n\n\n\n\n","category":"module"},{"location":"rl_core/#Core-1","page":"RLCore","title":"Core","text":"","category":"section"},{"location":"rl_core/#","page":"RLCore","title":"RLCore","text":"The most important function in RLCore is run(agent, env, stop_condition, hook). In practice, it will be dispatched to different implementations based on the type of agent and env. For a stop_condition, it can be arbitrary function which accepts agent, env, obs as arguments and return a Bool value indicates whether to stop or not. For a hook, it can should be instances of AbstractHook.","category":"page"},{"location":"rl_core/#Stop-Conditions-1","page":"RLCore","title":"Stop Conditions","text":"","category":"section"},{"location":"rl_core/#","page":"RLCore","title":"RLCore","text":"ComposedStopCondition\nStopAfterStep\nStopAfterEpisode\nStopWhenDone","category":"page"},{"location":"rl_core/#ReinforcementLearningCore.ComposedStopCondition","page":"RLCore","title":"ReinforcementLearningCore.ComposedStopCondition","text":"ComposedStopCondition(stop_conditions; reducer = any)\n\nThe result of stop_conditions is reduced by reducer.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.StopAfterStep","page":"RLCore","title":"ReinforcementLearningCore.StopAfterStep","text":"StopAfterStep(step; cur = 1, is_show_progress = true, tag = \"TRAINING\")\n\nReturn true after being called step times.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.StopAfterEpisode","page":"RLCore","title":"ReinforcementLearningCore.StopAfterEpisode","text":"StopAfterEpisode(episode; cur = 0, is_show_progress = true, tag = \"TRAINING\")\n\nReturn true after being called episode. If is_show_progress is true, the ProgressMeter will be used to show progress.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.StopWhenDone","page":"RLCore","title":"ReinforcementLearningCore.StopWhenDone","text":"StopWhenDone()\n\nReturn true if the terminal field of an observation is true.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#Hooks-1","page":"RLCore","title":"Hooks","text":"","category":"section"},{"location":"rl_core/#","page":"RLCore","title":"RLCore","text":"AbstractHook\nEmptyHook\nComposedHook\nStepsPerEpisode\nRewardsPerEpisode\nTotalRewardPerEpisode\nCumulativeReward\nTimePerStep","category":"page"},{"location":"rl_core/#ReinforcementLearningCore.AbstractHook","page":"RLCore","title":"ReinforcementLearningCore.AbstractHook","text":"A hook is called at different stage duiring a run to allow users to inject customized runtime logic. By default, a AbstractHook will do nothing. One can override the behavior by implementing the following methods:\n\n(hook::YourHook)(::PreActStage, agent, env, obs, action), note that there's an extra argument of action.\n(hook::YourHook)(::PostActStage, agent, env, obs)\n(hook::YourHook)(::PreEpisodeStage, agent, env, obs)\n(hook::YourHook)(::PostEpisodeStage, agent, env, obs)\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.EmptyHook","page":"RLCore","title":"ReinforcementLearningCore.EmptyHook","text":"Do nothing\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.ComposedHook","page":"RLCore","title":"ReinforcementLearningCore.ComposedHook","text":"ComposedHook(hooks::AbstractHook...)\n\nCompose different hooks into a single hook.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.StepsPerEpisode","page":"RLCore","title":"ReinforcementLearningCore.StepsPerEpisode","text":"StepsPerEpisode(; steps = Int[], count = 0, tag = \"TRAINING\")\n\nStore steps of each episode in the field of steps.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.RewardsPerEpisode","page":"RLCore","title":"ReinforcementLearningCore.RewardsPerEpisode","text":"RewardsPerEpisode(; rewards = Vector{Vector{Float64}}(), tag = \"TRAINING\")\n\nStore each reward of each step in every episode in the field of rewards.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.TotalRewardPerEpisode","page":"RLCore","title":"ReinforcementLearningCore.TotalRewardPerEpisode","text":"TotalRewardPerEpisode(; rewards = Float64[], reward = 0.0, tag = \"TRAINING\")\n\nStore the total rewards of each episode in the field of rewards.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.CumulativeReward","page":"RLCore","title":"ReinforcementLearningCore.CumulativeReward","text":"CumulativeReward(rewards::Vector{Float64} = [0.0], tag::String = \"TRAINING\")\n\nStore cumulative rewards since the beginning to the field of rewards.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.TimePerStep","page":"RLCore","title":"ReinforcementLearningCore.TimePerStep","text":"TimePerStep(;max_steps=100)\nTimePerStep(times::CircularArrayBuffer{Float64}, t::UInt64)\n\nStore time cost of the latest max_steps in the times field.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#Agents-1","page":"RLCore","title":"Agents","text":"","category":"section"},{"location":"rl_core/#","page":"RLCore","title":"RLCore","text":"Agent\nDynaAgent","category":"page"},{"location":"rl_core/#ReinforcementLearningCore.Agent","page":"RLCore","title":"ReinforcementLearningCore.Agent","text":"Agent(;kwargs...)\n\nOne of the most commonly used AbstractAgent.\n\nGenerally speaking, it does nothing but update the trajectory and policy appropriately in different stages.\n\nKeywords & Fields\n\npolicy::AbstractPolicy: the policy to use\ntrajectory::AbstractTrajectory: used to store transitions between an agent and an environment\nrole=DEFAULT_PLAYER: used to distinguish different agents\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.DynaAgent","page":"RLCore","title":"ReinforcementLearningCore.DynaAgent","text":"DynaAgent(;kwargs...)\n\nDynaAgent is first introduced in: Sutton, Richard S. \"Dyna, an integrated architecture for learning, planning, and reacting.\" ACM Sigart Bulletin 2.4 (1991): 160-163.\n\nKeywords & Fields\n\npolicy::AbstractPolicy: the policy to use\nmodel::AbstractEnvironmentModel: describe the environment to interact with\ntrajectory::AbstractTrajectory: used to store transitions between agent and environment\nrole=:DEFAULT: used to distinguish different agents\nplan_step::Int=10: the count of planning steps\n\nThe main difference between DynaAgent and Agent is that an environment model is involved. It is best described in the book: Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\n(Image: ) (Image: )\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#Policies-1","page":"RLCore","title":"Policies","text":"","category":"section"},{"location":"rl_core/#","page":"RLCore","title":"RLCore","text":"OffPolicy\nQBasedPolicy\nRandomPolicy\nVBasedPolicy\nWeightedRandomPolicy","category":"page"},{"location":"rl_core/#ReinforcementLearningCore.OffPolicy","page":"RLCore","title":"ReinforcementLearningCore.OffPolicy","text":"OffPolicy(π_target::P, π_behavior::B) -> OffPolicy{P,B}\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.QBasedPolicy","page":"RLCore","title":"ReinforcementLearningCore.QBasedPolicy","text":"QBasedPolicy(;learner::Q, explorer::S)\n\nUse a Q-learner to generate the estimations of actions and use explorer to get the action.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.RandomPolicy","page":"RLCore","title":"ReinforcementLearningCore.RandomPolicy","text":"RandomPolicy(action_space, rng)\n\nRandomly return a valid action.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.VBasedPolicy","page":"RLCore","title":"ReinforcementLearningCore.VBasedPolicy","text":"VBasedPolicy(;kwargs...)\n\nKey words & Fields\n\nlearner::AbstractLearner, learn how to estimate state values.\nmapping, a customized function (obs, learner) -> action_values\nexplorer::AbstractExplorer, decide which action to take based on action values.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.WeightedRandomPolicy","page":"RLCore","title":"ReinforcementLearningCore.WeightedRandomPolicy","text":"WeightedRandomPolicy(actions, weight, sums, rng)\n\nSimilar to RandomPolicy, but the probability of each action is set in advance instead of a uniform distribution.\n\nactions are all possible actions.\nweight can be an 1-D or 2-D array. If it's 1-D, then the weight applies to all states. If it's 2-D, then the state is assume to be of Int and for different state, the corresponding weight is selected.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#Trajectories-1","page":"RLCore","title":"Trajectories","text":"","category":"section"},{"location":"rl_core/#","page":"RLCore","title":"RLCore","text":"Trajectory\nVectorialTrajectory\nCircularTrajectory\nVectorialCompactSARTSATrajectory\nEpisodicCompactSARTSATrajectory\nCircularCompactSARTSATrajectory\nCircularCompactPSARTSATrajectory","category":"page"},{"location":"rl_core/#ReinforcementLearningCore.Trajectory","page":"RLCore","title":"ReinforcementLearningCore.Trajectory","text":"Trajectory{names,types,Tbs}(trajectories::Tbs)\n\nA container of different trajectories. Usually you won't use it directly.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.VectorialTrajectory","page":"RLCore","title":"ReinforcementLearningCore.VectorialTrajectory","text":"VectorialTrajectory(;trace_name=trace_type ...)\n\nUse Vector to store the traces.\n\nExample\n\njulia> t = VectorialTrajectory(;a=Int, b=Symbol)\n0-element Trajectory{(:a, :b),Tuple{Int64,Symbol},NamedTuple{(:a, :b),Tuple{Array{Int64,1},Array{Symbol,1}}}}\n\njulia> push!(t, a=0, b=:x)\n\njulia> push!(t, a=1, b=:y)\n\njulia> t\n2-element Trajectory{(:a, :b),Tuple{Int64,Symbol},NamedTuple{(:a, :b),Tuple{Array{Int64,1},Array{Symbol,1}}}}:\n (a = 0, b = :x)\n (a = 1, b = :y)\n\njulia> get_trace(t, :b)\n2-element Array{Symbol,1}:\n :x\n :y\n\njulia> pop!(t)\n\njulia> t\n1-element Trajectory{(:a, :b),Tuple{Int64,Symbol},NamedTuple{(:a, :b),Tuple{Array{Int64,1},Array{Symbol,1}}}}:\n (a = 0, b = :x)\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.CircularTrajectory","page":"RLCore","title":"ReinforcementLearningCore.CircularTrajectory","text":"CircularTrajectory(; capacity, trace_name=eltype=>size...)\n\nSimilar to VectorialTrajectory, but we use the CircularArrayBuffer to store the traces. The capacity here is used to specify the maximum length of the trajectory.\n\nExample\n\njulia> t = CircularTrajectory(capacity=10, state=Float64=>(3,3), reward=Int=>tuple())\n0-element Trajectory{(:state, :reward),Tuple{Float64,Int64},NamedTuple{(:state, :reward),Tuple{CircularArrayBuffer{Float64,3},CircularArrayBuffer{Int64,1}}}}\n\njulia> push!(t,state=rand(3,3), reward=1)\n\njulia> push!(t,state=rand(3,3), reward=2)\n\njulia> get_trace(t, :reward)\n2-element CircularArrayBuffer{Int64,1}:\n 1\n 2\n\njulia> get_trace(t, :state)\n3×3×2 CircularArrayBuffer{Float64,3}:\n[:, :, 1] =\n 0.699906  0.382396   0.927411\n 0.269807  0.0581324  0.239609\n 0.222304  0.514408   0.318905\n\n[:, :, 2] =\n 0.956228  0.992505  0.109743\n 0.763497  0.381387  0.540566\n 0.223081  0.834308  0.634759\n\njulia> pop!(t)\n\njulia> get_trace(t, :state)\n3×3×1 CircularArrayBuffer{Float64,3}:\n[:, :, 1] =\n 0.699906  0.382396   0.927411\n 0.269807  0.0581324  0.239609\n 0.222304  0.514408   0.318905\n\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.VectorialCompactSARTSATrajectory","page":"RLCore","title":"ReinforcementLearningCore.VectorialCompactSARTSATrajectory","text":"VectorialCompactSARTSATrajectory(; state_type = Int, action_type = Int, reward_type = Float32, terminal_type = Bool)\n\nThis function creates a VectorialTrajectory of RTSA fields. Here the Compact in the function name means that, state and next_state, action and next_action reuse a same vector underlying.\n\nExample\n\njulia> t = VectorialCompactSARTSATrajectory()\n0-element Trajectory{(:state, :action, :reward, :terminal, :next_state, :next_action),Tuple{Int64,Int64,Float32,Bool,Int64,Int64},NamedTuple{(:reward, :terminal, :state, :action),Tuple{Array{Float32,1},Array{Bool,1},Array{Int64,1},Array{Int64,1}}}}\n\njulia> push!(t, state=0, action=0)\n\njulia> push!(t, reward=0.f0, terminal=false, state=1, action=1)\n\njulia> t\n1-element Trajectory{(:state, :action, :reward, :terminal, :next_state, :next_action),Tuple{Int64,Int64,Float32,Bool,Int64,Int64},NamedTuple{(:reward, :terminal, :state, :action),Tuple{Array{Float32,1},Array{Bool,1},Array{Int64,1},Array{Int64,1}}}}:\n (state = 0, action = 0, reward = 0.0, terminal = 0, next_state = 1, next_action = 1)\n\njulia> push!(t, reward=1.f0, terminal=true, state=2, action=2)\n\njulia> t\n2-element Trajectory{(:state, :action, :reward, :terminal, :next_state, :next_action),Tuple{Int64,Int64,Float32,Bool,Int64,Int64},NamedTuple{(:reward, :terminal, :state, :action),Tuple{Array{Float32,1},Array{Bool,1},Array{Int64,1},Array{Int64,1}}}}:\n (state = 0, action = 0, reward = 0.0, terminal = 0, next_state = 1, next_action = 1)\n (state = 1, action = 1, reward = 1.0, terminal = 1, next_state = 2, next_action = 2)\n\njulia> get_trace(t, :state, :action)\n(state = [0, 1], action = [0, 1])\n\njulia> get_trace(t, :next_state, :next_action)\n(next_state = [1, 2], next_action = [1, 2])\n\njulia> pop!(t)\n1-element Trajectory{(:state, :action, :reward, :terminal, :next_state, :next_action),Tuple{Int64,Int64,Float32,Bool,Int64,Int64},NamedTuple{(:reward, :terminal, :state, :action),Tuple{Array{Float32,1},Array{Bool,1},Array{Int64,1},Array{Int64,1}}}}:\n (state = 0, action = 0, reward = 0.0, terminal = 0, next_state = 1, next_action = 1)\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.EpisodicCompactSARTSATrajectory","page":"RLCore","title":"ReinforcementLearningCore.EpisodicCompactSARTSATrajectory","text":"EpisodicCompactSARTSATrajectory(; state_type = Int, action_type = Int, reward_type = Float32, terminal_type = Bool)\n\nExactly the same with VectorialCompactSARTSATrajectory. It only exists for multiple dispatch purpose.\n\nwarning: Warning\nThe EpisodicCompactSARTSATrajectory will not be automatically emptified when reaching the end of an episode.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.CircularCompactSARTSATrajectory","page":"RLCore","title":"ReinforcementLearningCore.CircularCompactSARTSATrajectory","text":"CircularCompactSARTSATrajectory(;kwargs...)\n\nSimilar to VectorialCompactSARTSATrajectory, instead of using Vectors as containers, CircularArrayBuffers are used here.\n\nKey word arguments\n\ncapacity::Int, the maximum length of each trace.\nstate_type = Int\nstate_size = ()\naction_type = Int\naction_size = ()\nreward_type = Float32\nreward_size = ()\nterminal_type = Bool\nterminal_size = ()\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.CircularCompactPSARTSATrajectory","page":"RLCore","title":"ReinforcementLearningCore.CircularCompactPSARTSATrajectory","text":"CircularCompactPSARTSATrajectory(;kwargs)\n\nSimilar to CircularCompactSARTSATrajectory, except that another trace named priority is added.\n\nKey word arguments\n\ncapacity::Int, the maximum length of each trace.\nstate_type = Int\nstate_size = ()\naction_type = Int\naction_size = ()\nreward_type = Float32\nreward_size = ()\nterminal_type = Bool\nterminal_size = ()\npriority_type = Float32\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#Preprocessors-1","page":"RLCore","title":"Preprocessors","text":"","category":"section"},{"location":"rl_core/#","page":"RLCore","title":"RLCore","text":"ComposedPreprocessor\nCloneStatePreprocessor","category":"page"},{"location":"rl_core/#ReinforcementLearningCore.ComposedPreprocessor","page":"RLCore","title":"ReinforcementLearningCore.ComposedPreprocessor","text":"ComposedPreprocessor(p::AbstractPreprocessor...)\n\nCompose multiple preprocessors.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.CloneStatePreprocessor","page":"RLCore","title":"ReinforcementLearningCore.CloneStatePreprocessor","text":"CloneStatePreprocessor()\n\nDo deepcopy for the state in an observation.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#Learners-1","page":"RLCore","title":"Learners","text":"","category":"section"},{"location":"rl_core/#","page":"RLCore","title":"RLCore","text":"DoubleLearner","category":"page"},{"location":"rl_core/#ReinforcementLearningCore.DoubleLearner","page":"RLCore","title":"ReinforcementLearningCore.DoubleLearner","text":"DoubleLearner(;L1, L2, rng=MersenneTwister())\n\nThis is a meta-learner, it will randomly select one learner and update another learner. The estimation of an observation is the sum of result from two learners.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#Approximators-1","page":"RLCore","title":"Approximators","text":"","category":"section"},{"location":"rl_core/#","page":"RLCore","title":"RLCore","text":"TabularApproximator\nNeuralNetworkApproximator","category":"page"},{"location":"rl_core/#ReinforcementLearningCore.TabularApproximator","page":"RLCore","title":"ReinforcementLearningCore.TabularApproximator","text":"TabularApproximator(table<:AbstractArray)\n\nFor table of 1-d, it will create a V_APPROXIMATOR. For table of 2-d, it will create a [QApproximator].\n\nwarning: Warning\nFor table of 2-d, the first dimension is action and the second dimension is state.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.NeuralNetworkApproximator","page":"RLCore","title":"ReinforcementLearningCore.NeuralNetworkApproximator","text":"NeuralNetworkApproximator(;kwargs)\n\nUse a DNN model for value estimation.\n\nKeyword arguments\n\nmodel, a Flux based DNN model.\noptimizer\nparameters=params(model)\nkind=Q_APPROXIMATOR, specify the type of model.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#Explorers-1","page":"RLCore","title":"Explorers","text":"","category":"section"},{"location":"rl_core/#","page":"RLCore","title":"RLCore","text":"EpsilonGreedyExplorer\nUCBExplorer\nWeightedExplorer","category":"page"},{"location":"rl_core/#ReinforcementLearningCore.EpsilonGreedyExplorer","page":"RLCore","title":"ReinforcementLearningCore.EpsilonGreedyExplorer","text":"EpsilonGreedyExplorer{T}(;kwargs...)\nEpsilonGreedyExplorer(ϵ) -> EpsilonGreedyExplorer{:linear}(; ϵ_stable = ϵ)\n\nEpsilon-greedy strategy: The best lever is selected for a proportion 1 - epsilon of the trials, and a lever is selected at random (with uniform probability) for a proportion epsilon . Multi-armed_bandit\n\nTwo kinds of epsilon-decreasing strategy are implmented here (linear and exp).\n\nEpsilon-decreasing strategy: Similar to the epsilon-greedy strategy, except that the value of epsilon decreases as the experiment progresses, resulting in highly explorative behaviour at the start and highly exploitative behaviour at the finish.  - Multi-armed_bandit\n\nKeywords\n\nT::Symbol: defines how to calculate the epsilon in the warmup steps. Supported values are linear and exp.\nstep::Int = 1: record the current step.\nϵ_init::Float64 = 1.0: initial epsilon.\nwarmup_steps::Int=0: the number of steps to use ϵ_init.\ndecay_steps::Int=0: the number of steps for epsilon to decay from ϵ_init to ϵ_stable.\nϵ_stable::Float64: the epsilon after warmup_steps + decay_steps.\n\nExample\n\ns = EpsilonGreedyExplorer{:linear}(ϵ_init=0.9, ϵ_stable=0.1, warmup_steps=100, decay_steps=100)\nplot([RL.get_ϵ(s, i) for i in 1:500], label=\"linear epsilon\")\n\n(Image: )\n\ns = EpsilonGreedyExplorer{:exp}(ϵ_init=0.9, ϵ_stable=0.1, warmup_steps=100, decay_steps=100)\nplot([RL.get_ϵ(s, i) for i in 1:500], label=\"exp epsilon\")\n\n(Image: )\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.UCBExplorer","page":"RLCore","title":"ReinforcementLearningCore.UCBExplorer","text":"UCBExplorer(na; c=2.0, ϵ=1e-10)\n\nArguments\n\nna is the number of actions used to create a internal counter.\nt is used to store current time step.\nc is used to control the degree of exploration.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.WeightedExplorer","page":"RLCore","title":"ReinforcementLearningCore.WeightedExplorer","text":"WeightedExplorer(;is_normalized::Bool)\n\nis_normalized is used to indicate if the feeded action values are alrady normalized to have a sum of 1.0.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#Utils-1","page":"RLCore","title":"Utils","text":"","category":"section"},{"location":"rl_core/#","page":"RLCore","title":"RLCore","text":"huber_loss\nhuber_loss_unreduced\nCircularArrayBuffer\ndevice\nSumTree\nfind_all_max","category":"page"},{"location":"rl_core/#ReinforcementLearningCore.huber_loss","page":"RLCore","title":"ReinforcementLearningCore.huber_loss","text":"huber_loss(labels, predictions; δ = 1.0f0)\n\nSee Huber loss\n\n\n\n\n\n","category":"function"},{"location":"rl_core/#ReinforcementLearningCore.huber_loss_unreduced","page":"RLCore","title":"ReinforcementLearningCore.huber_loss_unreduced","text":"huber_loss_unreduced(labels, predictions; δ = 1.0f0)\n\nSimilar to huber_loss, but it doesn't do the mean operation in the last step.\n\n\n\n\n\n","category":"function"},{"location":"rl_core/#ReinforcementLearningCore.CircularArrayBuffer","page":"RLCore","title":"ReinforcementLearningCore.CircularArrayBuffer","text":"CircularArrayBuffer{T}(d::Integer...) -> CircularArrayBuffer{T, N}\n\nCircularArrayBuffer uses a N-dimention Array of size d to serve as a buffer for N-1-dimention Arrays with the same size.\n\nExamples\n\njulia> b = CircularArrayBuffer{Float64}(2, 2, 3)\n2×2×0 CircularArrayBuffer{Float64,3}\n\njulia> capacity(b)\n3\n\njulia> length(b)\n0\n\njulia> push!(b, [1. 1.; 2. 2.])\n2×2×1 CircularArrayBuffer{Float64,3}:\n[:, :, 1] =\n 1.0  1.0\n 2.0  2.0\n\njulia> b\n2×2×1 CircularArrayBuffer{Float64,3}:\n[:, :, 1] =\n 1.0  1.0\n 2.0  2.0\n\njulia> length(b)\n4\n\njulia> nframes(cb::CircularArrayBuffer) = cb.length\nnframes (generic function with 1 method)\n\njulia> nframes(b)\n1\n\njulia> ones(2,2)\n2×2 Array{Float64,2}:\n 1.0  1.0\n 1.0  1.0\n\njulia> 3 .* ones(2,2)\n2×2 Array{Float64,2}:\n 3.0  3.0\n 3.0  3.0\n\njulia> 3 * ones(2,2)\n2×2 Array{Float64,2}:\n 3.0  3.0\n 3.0  3.0\n\njulia> b = CircularArrayBuffer{Float64}(2, 2, 3)\n2×2×0 CircularArrayBuffer{Float64,3}\n\njulia> capacity(b)\n3\n\njulia> nframes(b)\n0\n\njulia> push!(b, 1 .* ones(2,2))\n2×2×1 CircularArrayBuffer{Float64,3}:\n[:, :, 1] =\n 1.0  1.0\n 1.0  1.0\n\njulia> b\n2×2×1 CircularArrayBuffer{Float64,3}:\n[:, :, 1] =\n 1.0  1.0\n 1.0  1.0\n\njulia> nframes(b)\n1\n\njulia> for i in 2:4\n           push!(b, i .* ones(2,2))\n       end\n\njulia> b\n2×2×3 CircularArrayBuffer{Float64,3}:\n[:, :, 1] =\n 2.0  2.0\n 2.0  2.0\n\n[:, :, 2] =\n 3.0  3.0\n 3.0  3.0\n\n[:, :, 3] =\n 4.0  4.0\n 4.0  4.0\n\njulia> isfull(b)\ntrue\n\njulia> nframes(b)\n3\n\njulia> size(b)\n(2, 2, 3)\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.device","page":"RLCore","title":"ReinforcementLearningCore.device","text":"device(model)\n\nDetect the suitable running device for the model. Return Val(:cpu) by default.\n\n\n\n\n\n","category":"function"},{"location":"rl_core/#ReinforcementLearningCore.SumTree","page":"RLCore","title":"ReinforcementLearningCore.SumTree","text":"SumTree(capacity::Int)\n\nEfficiently sample and update weights. For more detals, see the post at here. Here we use a vector to represent the binary tree. Suppose we will have capacity leaves at most. Every time we push! new node into the tree, only the recent capacity node and their sum will be updated! [––––––Parent nodes––––––][––––leaves––––] [size: 2^ceil(Int, log2(capacity))-1 ][     size: capacity   ]\n\nExample\n\njulia> t = SumTree(8)\n0-element SumTree\njulia> for i in 1:16\n       push!(t, i)\n       end\njulia> t\n8-element SumTree:\n  9.0\n 10.0\n 11.0\n 12.0\n 13.0\n 14.0\n 15.0\n 16.0\njulia> sample(t)\n(2, 10.0)\njulia> sample(t)\n(1, 9.0)\njulia> inds, ps = sample(t,100000)\n([8, 4, 8, 1, 5, 2, 2, 7, 6, 6  …  1, 1, 7, 1, 6, 1, 5, 7, 2, 7], [16.0, 12.0, 16.0, 9.0, 13.0, 10.0, 10.0, 15.0, 14.0, 14.0  …  9.0, 9.0, 15.0, 9.0, 14.0, 9.0, 13.0, 15.0, 10.0, 15.0])\njulia> countmap(inds)\nDict{Int64,Int64} with 8 entries:\n  7 => 14991\n  4 => 12019\n  2 => 10003\n  3 => 11027\n  5 => 12971\n  8 => 16052\n  6 => 13952\n  1 => 8985\njulia> countmap(ps)\nDict{Float64,Int64} with 8 entries:\n  9.0  => 8985\n  13.0 => 12971\n  10.0 => 10003\n  14.0 => 13952\n  16.0 => 16052\n  11.0 => 11027\n  15.0 => 14991\n  12.0 => 12019\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.find_all_max","page":"RLCore","title":"ReinforcementLearningCore.find_all_max","text":"find_all_max(A::AbstractArray)\n\nLike find_max, but all the indices of the maximum value are returned.\n\nwarning: Warning\nAll elements of value NaN in A will be ignored, unless all elements are NaN. In that case, the returned maximum value will be NaN and the returned indices will be collect(1:length(A))\n\nExamples\n\njulia> find_all_max([-Inf, -Inf, -Inf])\n(-Inf, [1, 2, 3])\njulia> find_all_max([Inf, Inf, Inf])\n(Inf, [1, 2, 3])\njulia> find_all_max([Inf, 0, Inf])\n(Inf, [1, 3])\njulia> find_all_max([0,1,2,1,2,1,0])\n(2, [3, 5])\n\n\n\n\n\nfind_all_max(A, mask)\n\nSimilar to find_all_max(A), but only the masked elements in A will be considered.\n\n\n\n\n\n","category":"function"},{"location":"tips_for_developers/#Tips-for-Developers-1","page":"Tips for Developers","title":"Tips for Developers","text":"","category":"section"},{"location":"tips_for_developers/#","page":"Tips for Developers","title":"Tips for Developers","text":"This page aims to provide a short introduction to Julia and some related packages for those who are interested in making contributions to this package.","category":"page"},{"location":"tips_for_developers/#Basic-1","page":"Tips for Developers","title":"Basic","text":"","category":"section"},{"location":"tips_for_developers/#","page":"Tips for Developers","title":"Tips for Developers","text":"Go through the latest doc. Especially keep an eye on the following sections if you come from other programming languages:","category":"page"},{"location":"tips_for_developers/#","page":"Tips for Developers","title":"Tips for Developers","text":"Noteworthy Differences from other Languages.\nYou don't need to remember all those details, but giving a glimp of them will make things easier to transfer to Julia.\nFunction like objects\nIt shouldn't surprise you that many important components in this package are functional objects.\nParametric Types\nIt may be a little struggling to create a flexible type if you have little background in parameteric types. See also Design Patterns with Parameteric Methods\nTraits and Multiple Dispatch\nYou may have heard a lot of Julia users talking about Traits and Multiple Dispatch. This blog provides a general introduction to them.\nSubArrays\nTo avoid unnecessary memory allocation, views are widely used in this package.\nPerformance Tips\nAlways keep those suggestions in mind if you want to make your algorithms run fast.\nMulti-Threading\nMulti-Threading will be heavily use in some async algorithms.","category":"page"},{"location":"tips_for_developers/#Some-Important-Packages-1","page":"Tips for Developers","title":"Some Important Packages","text":"","category":"section"},{"location":"tips_for_developers/#[Flux.jl](https://github.com/FluxML/Flux.jl)-1","page":"Tips for Developers","title":"Flux.jl","text":"","category":"section"},{"location":"tips_for_developers/#","page":"Tips for Developers","title":"Tips for Developers","text":"Flux.jl is a very lightweight package for deep learning. Its source code is really worth reading.","category":"page"},{"location":"tips_for_developers/#[Zygote.jl](https://github.com/FluxML/Zygote.jl)-1","page":"Tips for Developers","title":"Zygote.jl","text":"","category":"section"},{"location":"tips_for_developers/#","page":"Tips for Developers","title":"Tips for Developers","text":"The next-gen AD system for Flux. Basically you only need to understand how to implement a customized @adjoin.","category":"page"},{"location":"tips_for_developers/#[CuArrays.jl](https://github.com/JuliaGPU/CuArrays.jl)-1","page":"Tips for Developers","title":"CuArrays.jl","text":"","category":"section"},{"location":"tips_for_developers/#","page":"Tips for Developers","title":"Tips for Developers","text":"A basic understanding of how CuArrays.jl works will help you to write some efficient customized GPU kernels. An Introduction to GPU Programming in Julia  is also a very helpful resource.","category":"page"},{"location":"tips_for_developers/#[Knet.jl](https://github.com/denizyuret/Knet.jl)-1","page":"Tips for Developers","title":"Knet.jl","text":"","category":"section"},{"location":"tips_for_developers/#","page":"Tips for Developers","title":"Tips for Developers","text":"Another excellent julia package for deep learning. You'll find it easy to write algorithms that run on both Flux and Knet in this package.","category":"page"},{"location":"tips_for_developers/#Comparison-with-Other-RL-Packages-1","page":"Tips for Developers","title":"Comparison with Other RL Packages","text":"","category":"section"},{"location":"tips_for_developers/#Comparison-with-[Dopamine](https://github.com/google/dopamine)-1","page":"Tips for Developers","title":"Comparison with Dopamine","text":"","category":"section"},{"location":"tips_for_developers/#","page":"Tips for Developers","title":"Tips for Developers","text":"TODO:","category":"page"},{"location":"tips_for_developers/#Comparison-with-[Ray/RLlib](https://ray.readthedocs.io/en/latest/rllib.html)-1","page":"Tips for Developers","title":"Comparison with Ray/RLlib","text":"","category":"section"},{"location":"tips_for_developers/#","page":"Tips for Developers","title":"Tips for Developers","text":"TODO:","category":"page"},{"location":"tips_for_developers/#Q-and-A-1","page":"Tips for Developers","title":"Q&A","text":"","category":"section"},{"location":"tips_for_developers/#","page":"Tips for Developers","title":"Tips for Developers","text":"Your questions will appear here.","category":"page"}]
}
