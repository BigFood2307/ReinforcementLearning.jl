var documenterSearchIndex = {"docs":
[{"location":"rl_zoo/#ReinforcementLearningZoo.jl","page":"RLZoo","title":"ReinforcementLearningZoo.jl","text":"","category":"section"},{"location":"rl_zoo/","page":"RLZoo","title":"RLZoo","text":"Modules = [ReinforcementLearningZoo]","category":"page"},{"location":"rl_zoo/#ReinforcementLearningZoo.A2CGAELearner","page":"RLZoo","title":"ReinforcementLearningZoo.A2CGAELearner","text":"A2CGAELearner(;kwargs...)\n\nKeyword arguments\n\napproximator, an ActorCritic based NeuralNetworkApproximator\nγ::Float32, reward discount rate.\nλ::Float32, lambda for GAE-lambda\nactor_loss_weight::Float32\ncritic_loss_weight::Float32\nentropy_loss_weight::Float32\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.A2CLearner","page":"RLZoo","title":"ReinforcementLearningZoo.A2CLearner","text":"A2CLearner(;kwargs...)\n\nKeyword arguments\n\napproximator::ActorCritic\nγ::Float32, reward discount rate.\nactor_loss_weight::Float32\ncritic_loss_weight::Float32\nentropy_loss_weight::Float32\nupdate_freq::Int, usually set to the same with the length of trajectory.\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.BasicDQNLearner","page":"RLZoo","title":"ReinforcementLearningZoo.BasicDQNLearner","text":"BasicDQNLearner(;kwargs...)\n\nSee paper: Playing Atari with Deep Reinforcement Learning\n\nThis is the very basic implementation of DQN. Compared to the traditional Q learning, the only difference is that, in the updating step it uses a batch of transitions sampled from an experience buffer instead of current transition. And the approximator is usually a NeuralNetworkApproximator. You can start from this implementation to understand how everything is organized and how to write your own customized algorithm.\n\nKeywords\n\napproximator::AbstractApproximator: used to get Q-values of a state.\nloss_func: the loss function to use.\nγ::Float32=0.99f0: discount rate.\nbatch_size::Int=32\nmin_replay_history::Int=32: number of transitions that should be experienced before updating the approximator.\nrng=Random.GLOBAL_RNG\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.BehaviorCloningPolicy","page":"RLZoo","title":"ReinforcementLearningZoo.BehaviorCloningPolicy","text":"BehaviorCloningPolicy(;kw...)\n\nKeyword Arguments\n\napproximator: calculate the logits of possible actions directly\nexplorer=GreedyExplorer() \n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.BestResponsePolicy-Tuple{Any, Any, Any}","page":"RLZoo","title":"ReinforcementLearningZoo.BestResponsePolicy","text":"BestResponsePolicy(policy, env, best_responder)\n\npolicy, the original policy to be wrapped in the best response policy.\nenv, the environment to handle.\nbest_responder, the player to choose best response action.\n\n\n\n\n\n","category":"method"},{"location":"rl_zoo/#ReinforcementLearningZoo.DDPGPolicy-Tuple{}","page":"RLZoo","title":"ReinforcementLearningZoo.DDPGPolicy","text":"DDPGPolicy(;kwargs...)\n\nKeyword arguments\n\nbehavior_actor,\nbehavior_critic,\ntarget_actor,\ntarget_critic,\nstart_policy,\nγ = 0.99f0,\nρ = 0.995f0,\nbatch_size = 32,\nstart_steps = 10000,\nupdate_after = 1000,\nupdate_every = 50,\nact_limit = 1.0,\nact_noise = 0.1,\nstep = 0,\nrng = Random.GLOBAL_RNG,\n\n\n\n\n\n","category":"method"},{"location":"rl_zoo/#ReinforcementLearningZoo.DQNLearner-Union{Tuple{}, Tuple{Tf}, Tuple{Tt}, Tuple{Tq}} where {Tq, Tt, Tf}","page":"RLZoo","title":"ReinforcementLearningZoo.DQNLearner","text":"DQNLearner(;kwargs...)\n\nSee paper: Human-level control through deep reinforcement learning\n\nKeywords\n\napproximator::AbstractApproximator: used to get Q-values of a state.\ntarget_approximator::AbstractApproximator: similar to approximator, but used to estimate the target (the next state).\nloss_func: the loss function.\nγ::Float32=0.99f0: discount rate.\nbatch_size::Int=32\nupdate_horizon::Int=1: length of update ('n' in n-step update).\nmin_replay_history::Int=32: number of transitions that should be experienced before updating the approximator.\nupdate_freq::Int=4: the frequency of updating the approximator.\ntarget_update_freq::Int=100: the frequency of syncing target_approximator.\nstack_size::Union{Int, Nothing}=4: use the recent stack_size frames to form a stacked state.\ntraces = SARTS: set to SLARTSL if you are to apply to an environment of FULL_ACTION_SET.\nrng = Random.GLOBAL_RNG\nis_enable_double_DQN = Bool: enable double dqn, enabled by default.\n\n\n\n\n\n","category":"method"},{"location":"rl_zoo/#ReinforcementLearningZoo.DeepCFR","page":"RLZoo","title":"ReinforcementLearningZoo.DeepCFR","text":"DeepCFR(;kwargs...)\n\nSymbols used here follow the paper: Deep Counterfactual Regret Minimization\n\nKeyword arguments\n\nK, number of traverrsal.\nt, number of iteration.\nΠ, the policy network.\nV, a dictionary of each player's advantage network.\nMΠ, a strategy memory.\nMV, a dictionary of each player's advantage memory.\nreinitialize_freq=1, the frequency of reinitializing the value networks.\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.DoubleLearner","page":"RLZoo","title":"ReinforcementLearningZoo.DoubleLearner","text":"DoubleLearner(;L1, L2, rng=Random.GLOBAL_RNG)\n\nThis is a meta-learner, it will randomly select one learner and update another learner. The estimation of an observation is the sum of result from two learners.\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.DuelingNetwork","page":"RLZoo","title":"ReinforcementLearningZoo.DuelingNetwork","text":"DuelingNetwork(;base, val, adv)\n\nDueling network automatically produces separate estimates of the state value function network and advantage function network. The expected output size of val is 1, and adv is the size of the action space.\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.EnrichedAction","page":"RLZoo","title":"ReinforcementLearningZoo.EnrichedAction","text":"EnrichedAction(action;kwargs...)\n\nInject some runtime info into the action\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.ExperienceBasedSamplingModel","page":"RLZoo","title":"ReinforcementLearningZoo.ExperienceBasedSamplingModel","text":"ExperienceBasedSamplingModel\n\nRandomly generate a transition of (s, a, r, t, s′) based on previous experiences in each sampling.\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.ExternalSamplingMCCFRPolicy","page":"RLZoo","title":"ReinforcementLearningZoo.ExternalSamplingMCCFRPolicy","text":"ExternalSamplingMCCFRPolicy\n\nThis implementation uses stochasticaly-weighted averaging.\n\nRef:\n\nMONTE CARLO SAMPLING AND REGRET MINIMIZATION FOR EQUILIBRIUM COMPUTATION AND DECISION-MAKING IN LARGE EXTENSIVE FORM GAMES\nMonte Carlo Sampling for Regret Minimization in Extensive Games\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.GaussianNetwork","page":"RLZoo","title":"ReinforcementLearningZoo.GaussianNetwork","text":"GaussianNetwork(;pre=identity, μ, logσ)\n\nReturns μ and logσ when called.  Create a distribution to sample from  using Normal.(μ, exp.(logσ)).\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.IQNLearner","page":"RLZoo","title":"ReinforcementLearningZoo.IQNLearner","text":"IQNLearner(;kwargs)\n\nSee paper\n\nKeyworkd arugments\n\napproximator, a ImplicitQuantileNet\ntarget_approximator, a ImplicitQuantileNet, must have the same structure as approximator\nκ = 1.0f0,\nN = 32,\nN′ = 32,\nNₑₘ = 64,\nK = 32,\nγ = 0.99f0,\nstack_size = 4,\nbatch_size = 32,\nupdate_horizon = 1,\nmin_replay_history = 20000,\nupdate_freq = 4,\ntarget_update_freq = 8000,\nupdate_step = 0,\ndefault_priority = 1.0f2,\nβ_priority = 0.5f0,\nrng = Random.GLOBAL_RNG,\ndevice_seed = nothing,\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.ImplicitQuantileNet","page":"RLZoo","title":"ReinforcementLearningZoo.ImplicitQuantileNet","text":"ImplicitQuantileNet(;ψ, ϕ, header)\n\n        quantiles (n_action, n_quantiles, batch_size)\n           ↑\n         header\n           ↑\nfeature ↱  ⨀   ↰ transformed embedding\n       ψ       ϕ\n       ↑       ↑\n       s        τ\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.MinimaxPolicy","page":"RLZoo","title":"ReinforcementLearningZoo.MinimaxPolicy","text":"MinimaxPolicy(;value_function, depth::Int)\n\nThe minimax algorithm with Alpha-beta pruning\n\nKeyword Arguments\n\nmaximum_depth::Int=30, the maximum depth of search.\nvalue_function=nothing, estimate the value of env. value_function(env) -> Number. It is only called after searching for maximum_depth and the env is not terminated yet.\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.MonteCarloLearner","page":"RLZoo","title":"ReinforcementLearningZoo.MonteCarloLearner","text":"MonteCarloLearner(;kwargs...)\n\nUse monte carlo method to estimate state value or state-action value.\n\nFields\n\napproximator::TabularApproximator, can be either TabularVApproximator or TabularQApproximator.\nγ=1.0, discount rate.\nkind=FIRST_VISIT. Optional values are FIRST_VISIT or EVERY_VISIT.\nsampling=NO_SAMPLING. Optional values are NO_SAMPLING, WEIGHTED_IMPORTANCE_SAMPLING or ORDINARY_IMPORTANCE_SAMPLING.\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.MultiThreadEnv","page":"RLZoo","title":"ReinforcementLearningZoo.MultiThreadEnv","text":"MultiThreadEnv(envs::Vector{<:AbstractEnv})\n\nWrap multiple instances of the same environment type into one environment. Each environment will run in parallel by leveraging Threads.@spawn. So remember to set the environment variable JULIA_NUM_THREADS!\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.MultiThreadEnv-Tuple{Any, Int64}","page":"RLZoo","title":"ReinforcementLearningZoo.MultiThreadEnv","text":"MultiThreadEnv(f, n::Int)\n\nf is a lambda function which creates an AbstractEnv by calling f().\n\n\n\n\n\n","category":"method"},{"location":"rl_zoo/#ReinforcementLearningZoo.OutcomeSamplingMCCFRPolicy","page":"RLZoo","title":"ReinforcementLearningZoo.OutcomeSamplingMCCFRPolicy","text":"OutcomeSamplingMCCFRPolicy\n\nThis implementation uses stochasticaly-weighted averaging.\n\nRef:\n\nMONTE CARLO SAMPLING AND REGRET MINIMIZATION FOR EQUILIBRIUM COMPUTATION AND DECISION-MAKING IN LARGE EXTENSIVE FORM GAMES\nMonte Carlo Sampling for Regret Minimization in Extensive Games\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.PPOPolicy","page":"RLZoo","title":"ReinforcementLearningZoo.PPOPolicy","text":"PPOPolicy(;kwargs)\n\nKeyword arguments\n\napproximator,\nγ = 0.99f0,\nλ = 0.95f0,\nclip_range = 0.2f0,\nmax_grad_norm = 0.5f0,\nn_microbatches = 4,\nn_epochs = 4,\nactor_loss_weight = 1.0f0,\ncritic_loss_weight = 0.5f0,\nentropy_loss_weight = 0.01f0,\ndist = Categorical,\nrng = Random.GLOBAL_RNG,\n\nBy default, dist is set to Categorical, which means it will only works on environments of discrete actions. To work with environments of continuous actions dist should be set to Normal and the actor in the approximator should be a GaussianNetwork. Using it with a GaussianNetwork supports  multi-dimensional action spaces, though it only supports it under the assumption that the dimensions are independent since the GaussianNetwork outputs a single μ and σ for each dimension which is used to simplify the calculations.\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.PrioritizedDQNLearner","page":"RLZoo","title":"ReinforcementLearningZoo.PrioritizedDQNLearner","text":"PrioritizedDQNLearner(;kwargs...)\n\nSee paper: Prioritized Experience Replay And also https://danieltakeshi.github.io/2019/07/14/per/\n\nKeywords\n\napproximator::AbstractApproximator: used to get Q-values of a state.\ntarget_approximator::AbstractApproximator: similar to approximator, but used to estimate the target (the next state).\nloss_func: the loss function.\nγ::Float32=0.99f0: discount rate.\nbatch_size::Int=32\nupdate_horizon::Int=1: length of update ('n' in n-step update).\nmin_replay_history::Int=32: number of transitions that should be experienced before updating the approximator.\nupdate_freq::Int=4: the frequency of updating the approximator.\ntarget_update_freq::Int=100: the frequency of syncing target_approximator.\nstack_size::Union{Int, Nothing}=4: use the recent stack_size frames to form a stacked state.\ndefault_priority::Float64=100.: the default priority for newly added transitions.\nrng = Random.GLOBAL_RNG\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.PrioritizedDQNLearner-Tuple{Any}","page":"RLZoo","title":"ReinforcementLearningZoo.PrioritizedDQNLearner","text":"note: Note\nThe state of the observation is assumed to have been stacked, if !isnothing(stack_size).\n\n\n\n\n\n","category":"method"},{"location":"rl_zoo/#ReinforcementLearningZoo.PrioritizedSweepingSamplingModel","page":"RLZoo","title":"ReinforcementLearningZoo.PrioritizedSweepingSamplingModel","text":"PrioritizedSweepingSamplingModel(θ::Float64=1e-4)\n\nSee more details at Section (8.4) on Page 168 of the book Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.QRDQNLearner-Tuple{}","page":"RLZoo","title":"ReinforcementLearningZoo.QRDQNLearner","text":"QRDQNLearner(;kwargs...)\n\nSee paper: Distributional Reinforcement Learning with Quantile Regression\n\nKeywords\n\napproximator::AbstractApproximator: used to get quantile-values of a batch of states. The output should be of size (n_quantile, n_action).\ntarget_approximator::AbstractApproximator: similar to approximator, but used to estimate the quantile values of the next state batch.\nγ::Float32=0.99f0: discount rate.\nbatch_size::Int=32\nupdate_horizon::Int=1: length of update ('n' in n-step update).\nmin_replay_history::Int=32: number of transitions that should be experienced before updating the approximator.\nupdate_freq::Int=1: the frequency of updating the approximator.\nn_quantile::Int=1: the number of quantiles.\ntarget_update_freq::Int=100: the frequency of syncing target_approximator.\nstack_size::Union{Int, Nothing}=4: use the recent stack_size frames to form a stacked state.\ntraces = SARTS, set to SLARTSL if you are to apply to an environment of FULL_ACTION_SET.\nloss_func=quantile_huber_loss.\n\n\n\n\n\n","category":"method"},{"location":"rl_zoo/#ReinforcementLearningZoo.REMDQNLearner-Union{Tuple{}, Tuple{Tf}, Tuple{Tt}, Tuple{Tq}} where {Tq, Tt, Tf}","page":"RLZoo","title":"ReinforcementLearningZoo.REMDQNLearner","text":"REMDQNLearner(;kwargs...)\n\nSee paper: An Optimistic Perspective on Offline Reinforcement Learning\n\nKeywords\n\napproximator::AbstractApproximator: used to get Q-values of a state.\ntarget_approximator::AbstractApproximator: similar to approximator, but used to estimate the target (the next state).\nloss_func: the loss function.\nγ::Float32=0.99f0: discount rate.\nbatch_size::Int=32\nupdate_horizon::Int=1: length of update ('n' in n-step update).\nmin_replay_history::Int=32: number of transitions that should be experienced before updating the approximator.\nupdate_freq::Int=4: the frequency of updating the approximator.\nensemble_num::Int=1: the number of ensemble approximators.\nensemble_method::Symbol=:rand: the method of combining Q values. ':rand' represents random ensemble mixture, and ':mean' is the average.\ntarget_update_freq::Int=100: the frequency of syncing target_approximator.\nstack_size::Union{Int, Nothing}=4: use the recent stack_size frames to form a stacked state.\ntraces = SARTS, set to SLARTSL if you are to apply to an environment of FULL_ACTION_SET.\nrng = Random.GLOBAL_RNG\n\n\n\n\n\n","category":"method"},{"location":"rl_zoo/#ReinforcementLearningZoo.RainbowLearner","page":"RLZoo","title":"ReinforcementLearningZoo.RainbowLearner","text":"RainbowLearner(;kwargs...)\n\nSee paper: Rainbow: Combining Improvements in Deep Reinforcement Learning\n\nKeywords\n\napproximator::AbstractApproximator: used to get Q-values of a state.\ntarget_approximator::AbstractApproximator: similar to approximator, but used to estimate the target (the next state).\nloss_func: the loss function. It is recommended to use Flux.Losses.logitcrossentropy. Flux.Losses.crossentropy will encounter the problem of negative numbers.\nVₘₐₓ::Float32: the maximum value of distribution.\nVₘᵢₙ::Float32: the minimum value of distribution.\nn_actions::Int: number of possible actions.\nγ::Float32=0.99f0: discount rate.\nbatch_size::Int=32\nupdate_horizon::Int=1: length of update ('n' in n-step update).\nmin_replay_history::Int=32: number of transitions that should be experienced before updating the approximator.\nupdate_freq::Int=4: the frequency of updating the approximator.\ntarget_update_freq::Int=500: the frequency of syncing target_approximator.\nstack_size::Union{Int, Nothing}=4: use the recent stack_size frames to form a stacked state.\ndefault_priority::Float32=1.0f2.: the default priority for newly added transitions. It must be >= 1.\nn_atoms::Int=51: the number of buckets of the value function distribution.\nstack_size::Union{Int, Nothing}=4: use the recent stack_size frames to form a stacked state.\nrng = Random.GLOBAL_RNG\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.SACPolicy-Tuple{}","page":"RLZoo","title":"ReinforcementLearningZoo.SACPolicy","text":"SACPolicy(;kwargs...)\n\nKeyword arguments\n\npolicy,\nqnetwork1,\nqnetwork2,\ntarget_qnetwork1,\ntarget_qnetwork2,\nstart_policy,\nγ = 0.99f0,\nρ = 0.995f0,\nα = 0.2f0,\nbatch_size = 32,\nstart_steps = 10000,\nupdate_after = 1000,\nupdate_every = 50,\nstep = 0,\nrng = Random.GLOBAL_RNG,\n\npolicy is expected to output a tuple (μ, logσ) of mean and log standard deviations for the desired action distributions, this can be implemented using a GaussianNetwork in a NeuralNetworkApproximator.\n\nImplemented based on http://arxiv.org/abs/1812.05905\n\n\n\n\n\n","category":"method"},{"location":"rl_zoo/#ReinforcementLearningZoo.TD3Policy-Tuple{}","page":"RLZoo","title":"ReinforcementLearningZoo.TD3Policy","text":"TD3Policy(;kwargs...)\n\nKeyword arguments\n\nbehavior_actor,\nbehavior_critic,\ntarget_actor,\ntarget_critic,\nstart_policy,\nγ = 0.99f0,\nρ = 0.995f0,\nbatch_size = 32,\nstart_steps = 10000,\nupdate_after = 1000,\nupdate_every = 50,\npolicy_freq = 2 # frequency in which the actor performs a gradient step and critic target is updated\ntarget_act_limit = 1.0, # noise added to actor target\ntarget_act_noise = 0.1, # noise added to actor target\nact_limit = 1.0, # noise added when outputing action\nact_noise = 0.1, # noise added when outputing action\nstep = 0,\nrng = Random.GLOBAL_RNG,\n\n\n\n\n\n","category":"method"},{"location":"rl_zoo/#ReinforcementLearningZoo.TabularCFRPolicy-Tuple{}","page":"RLZoo","title":"ReinforcementLearningZoo.TabularCFRPolicy","text":"TabularCFRPolicy(;kwargs...)\n\nSome useful papers while implementing this algorithm:\n\nAn Introduction to Counterfactual Regret Minimization\nMONTE CARLO SAMPLING AND REGRET MINIMIZATION FOR EQUILIBRIUM COMPUTATION AND DECISION-MAKING IN LARGE EXTENSIVE FORM GAMES\nSolving Large Imperfect Information Games Using CFR⁺\nRevisiting CFR⁺ and Alternating Updates\nSolving Imperfect-Information Games via Discounted Regret Minimization\n\nKeyword Arguments\n\nis_alternating_update=true: If true, we update the players alternatively.\nis_reset_neg_regrets=true: Whether to use regret matching⁺.\nis_linear_averaging=true\nweighted_averaging_delay=0. The averaging delay in number of iterations. Only valid when is_linear_averaging is set to true.\nstate_type=String, the data type of information set.\nrng=Random.GLOBAL_RNG\n\n\n\n\n\n","category":"method"},{"location":"rl_zoo/#ReinforcementLearningZoo.TimeBasedSamplingModel","page":"RLZoo","title":"ReinforcementLearningZoo.TimeBasedSamplingModel","text":"TimeBasedSamplingModel(n_actions::Int, κ::Float64 = 1e-4)\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.VPGPolicy","page":"RLZoo","title":"ReinforcementLearningZoo.VPGPolicy","text":"Vanilla Policy Gradient\n\nVPGPolicy(;kwargs)\n\nKeyword arguments\n\napproximator,\nbaseline,\ndist, distribution function of the action\nγ, discount factor\nα_θ, step size of policy parameter\nα_w, step size of baseline parameter\nbatch_size,\nrng,\nloss,\nbaseline_loss,\n\nif the action space is continuous, then the env should transform the action value, (such as using tanh), in order to make sure low ≤ value ≤ high\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningBase.update!-Tuple{AbstractTrajectory, Union{NamedPolicy{var\"#s12\", N} where {var\"#s12\"<:(VBasedPolicy{var\"#s11\", M} where {var\"#s11\"<:MonteCarloLearner, M}), N}, QBasedPolicy{var\"#s13\", E} where {var\"#s13\"<:MonteCarloLearner, E<:AbstractExplorer}, VBasedPolicy{var\"#s14\", M} where {var\"#s14\"<:MonteCarloLearner, M}}, AbstractEnv, PreEpisodeStage}","page":"RLZoo","title":"ReinforcementLearningBase.update!","text":"Empty the trajectory at the end of an episode\n\n\n\n\n\n","category":"method"},{"location":"rl_zoo/#ReinforcementLearningBase.update!-Tuple{DeepCFR, AbstractEnv}","page":"RLZoo","title":"ReinforcementLearningBase.update!","text":"Run one interation\n\n\n\n\n\n","category":"method"},{"location":"rl_zoo/#ReinforcementLearningBase.update!-Tuple{DeepCFR}","page":"RLZoo","title":"ReinforcementLearningBase.update!","text":"Update Π (policy network)\n\n\n\n\n\n","category":"method"},{"location":"rl_zoo/#ReinforcementLearningBase.update!-Tuple{ExternalSamplingMCCFRPolicy, AbstractEnv}","page":"RLZoo","title":"ReinforcementLearningBase.update!","text":"Run one interation\n\n\n\n\n\n","category":"method"},{"location":"rl_zoo/#ReinforcementLearningBase.update!-Tuple{OutcomeSamplingMCCFRPolicy, AbstractEnv}","page":"RLZoo","title":"ReinforcementLearningBase.update!","text":"Run one interation\n\n\n\n\n\n","category":"method"},{"location":"rl_zoo/#ReinforcementLearningBase.update!-Tuple{TabularCFRPolicy, AbstractEnv}","page":"RLZoo","title":"ReinforcementLearningBase.update!","text":"Run one interation\n\n\n\n\n\n","category":"method"},{"location":"rl_zoo/#ReinforcementLearningBase.update!-Tuple{TabularCFRPolicy}","page":"RLZoo","title":"ReinforcementLearningBase.update!","text":"Update the behavior_policy\n\n\n\n\n\n","category":"method"},{"location":"rl_zoo/#ReinforcementLearningBase.update!-Tuple{VBasedPolicy{var\"#s19\", M} where {var\"#s19\"<:MonteCarloLearner, M}, AbstractTrajectory, AbstractEnv, PostEpisodeStage}","page":"RLZoo","title":"ReinforcementLearningBase.update!","text":"Only update at the end of an episode\n\n\n\n\n\n","category":"method"},{"location":"rl_zoo/#ReinforcementLearningCore._run","page":"RLZoo","title":"ReinforcementLearningCore._run","text":"Many policy gradient based algorithms require that the env is a MultiThreadEnv to increase the diversity during training. So the training pipeline is different from the default one in RLCore.\n\n\n\n\n\n","category":"function"},{"location":"rl_zoo/#ReinforcementLearningZoo.cfr!","page":"RLZoo","title":"ReinforcementLearningZoo.cfr!","text":"Symbol meanings:\n\nπ: reach prob π′: new reach prob π₋ᵢ: opponents' reach prob p: player to update. nothing means simultaneous update. w: weight v: counterfactual value before weighted by opponent's reaching probability V: a vector containing the v after taking each action with current information set. Used to calculate the regret value\n\n\n\n\n\n","category":"function"},{"location":"rl_zoo/#ReinforcementLearningZoo.evaluate-Tuple{SACPolicy, Any}","page":"RLZoo","title":"ReinforcementLearningZoo.evaluate","text":"This function is compatible with a multidimensional action space.\n\n\n\n\n\n","category":"method"},{"location":"rl_zoo/#ReinforcementLearningZoo.external_sampling!-Tuple{DeepCFR, AbstractEnv, Any}","page":"RLZoo","title":"ReinforcementLearningZoo.external_sampling!","text":"CFR Traversal with External Sampling\n\n\n\n\n\n","category":"method"},{"location":"rl_zoo/#ReinforcementLearningZoo.masked_regret_matching-Tuple{Any, Any}","page":"RLZoo","title":"ReinforcementLearningZoo.masked_regret_matching","text":"This is the specific regret matching method used in DeepCFR\n\n\n\n\n\n","category":"method"},{"location":"rl_zoo/#ReinforcementLearningZoo.policy_evaluation!-Tuple{}","page":"RLZoo","title":"ReinforcementLearningZoo.policy_evaluation!","text":"policy_evaluation!(;V, π, model, γ, θ)\n\nKeyword arguments\n\nV, an AbstractApproximator.\nπ, an AbstractPolicy.\nmodel, a distribution based environment model(given a state and action pair, return all possible reward, next state, termination info and corresponding probability).\nγ::Float64, discount rate.\nθ::Float64, threshold to stop evaluation.\n\n\n\n\n\n","category":"method"},{"location":"rl_zoo/#ReinforcementLearningZoo.update_advantage_networks-Tuple{Any, Any}","page":"RLZoo","title":"ReinforcementLearningZoo.update_advantage_networks","text":"Update advantage network\n\n\n\n\n\n","category":"method"},{"location":"rl_envs/#ReinforcementLearningEnvironments.jl","page":"RLEnvs","title":"ReinforcementLearningEnvironments.jl","text":"","category":"section"},{"location":"rl_envs/","page":"RLEnvs","title":"RLEnvs","text":"Modules = [ReinforcementLearningEnvironments]","category":"page"},{"location":"rl_envs/#ReinforcementLearningEnvironments.KUHN_POKER_REWARD_TABLE","page":"RLEnvs","title":"ReinforcementLearningEnvironments.KUHN_POKER_REWARD_TABLE","text":"(Image: )\n\n\n\n\n\n","category":"constant"},{"location":"rl_envs/#ReinforcementLearningEnvironments.ActionTransformedEnv-Tuple{Any}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.ActionTransformedEnv","text":"ActionTransformedEnv(env;action_space_mapping=identity, action_mapping=identity)\n\naction_space_mapping will be applied to action_space(env) and legal_action_space(env). action_mapping will be applied to action before feeding it into env.\n\n\n\n\n\n","category":"method"},{"location":"rl_envs/#ReinforcementLearningEnvironments.CartPoleEnv-Tuple{}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.CartPoleEnv","text":"CartPoleEnv(;kwargs...)\n\nKeyword arguments\n\nT = Float64\ngravity = T(9.8)\nmasscart = T(1.0)\nmasspole = T(0.1)\nhalflength = T(0.5)\nforcemag = T(10.0)\nmax_steps = 200\n'dt = 0.02'\nrng = Random.GLOBAL_RNG\n\n\n\n\n\n","category":"method"},{"location":"rl_envs/#ReinforcementLearningEnvironments.DefaultStateStyleEnv-Union{Tuple{E}, Tuple{S}} where {S, E}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.DefaultStateStyleEnv","text":"DefaultStateStyleEnv{S}(env::E)\n\nReset the result of DefaultStateStyle without changing the original behavior.\n\n\n\n\n\n","category":"method"},{"location":"rl_envs/#ReinforcementLearningEnvironments.KuhnPokerEnv","page":"RLEnvs","title":"ReinforcementLearningEnvironments.KuhnPokerEnv","text":"KuhnPokerEnv()\n\nSee more detailed description here.\n\nHere we demonstrate how to write a typical ZERO_SUM, IMPERFECT_INFORMATION game. The implementation here has a explicit CHANCE_PLAYER.\n\nTODO: add public state for SPECTOR. Ref: https://arxiv.org/abs/1906.11110\n\n\n\n\n\n","category":"type"},{"location":"rl_envs/#ReinforcementLearningEnvironments.MaxTimeoutEnv-Union{Tuple{E}, Tuple{E, Int64}} where E<:AbstractEnv","page":"RLEnvs","title":"ReinforcementLearningEnvironments.MaxTimeoutEnv","text":"MaxTimeoutEnv(env::E, max_t::Int; current_t::Int = 1)\n\nForce is_terminated(env) return true after max_t interactions.\n\n\n\n\n\n","category":"method"},{"location":"rl_envs/#ReinforcementLearningEnvironments.MontyHallEnv-Tuple{}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.MontyHallEnv","text":"MontyHallEnv(;rng=Random.GLOBAL_RNG)\n\nQuoted from wiki:\n\nSuppose you're on a game show, and you're given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows what's behind the doors, opens another door, say No. 3, which has a goat. He then says to you, \"Do you want to pick door No. 2?\" Is it to your advantage to switch your choice?\n\nHere we'll introduce the first environment which is of FULL_ACTION_SET.\n\n\n\n\n\n","category":"method"},{"location":"rl_envs/#ReinforcementLearningEnvironments.MountainCarEnv-Tuple{}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.MountainCarEnv","text":"MountainCarEnv(;kwargs...)\n\nKeyword arguments\n\nT = Float64\ncontinuous = false\nrng = Random.GLOBAL_RNG\nmin_pos = -1.2\nmax_pos = 0.6\nmax_speed = 0.07\ngoal_pos = 0.5\nmax_steps = 200\ngoal_velocity = 0.0\npower = 0.001\ngravity = 0.0025\n\n\n\n\n\n","category":"method"},{"location":"rl_envs/#ReinforcementLearningEnvironments.MultiArmBanditsEnv-Tuple{Any}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.MultiArmBanditsEnv","text":"In our design, the return of taking an action in env is undefined. This is the main difference compared to those interfaces defined in OpenAI/Gym. We find that the async manner is more suitable to describe many complicated environments. However, one of the inconveniences is that we have to cache some intermediate data for future queries. Here we have to store reward and is_terminated in the instance of env for future queries.\n\n\n\n\n\n","category":"method"},{"location":"rl_envs/#ReinforcementLearningEnvironments.MultiArmBanditsEnv-Tuple{}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.MultiArmBanditsEnv","text":"MultiArmBanditsEnv(;true_reward=0., k = 10,rng=Random.GLOBAL_RNG)\n\ntrue_reward is the expected reward. k is the number of arms. See multi-armed bandit for more detailed explanation.\n\nThis is a one-shot game. The environment terminates immediately after taking in an action. Here we use it to demonstrate how to write a customized environment with only minimal interfaces defined.\n\n\n\n\n\n","category":"method"},{"location":"rl_envs/#ReinforcementLearningEnvironments.PendulumEnv-Tuple{}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.PendulumEnv","text":"PendulumEnv(;kwargs...)\n\nKeyword arguments\n\nT = Float64\nmax_speed = T(8)\nmax_torque = T(2)\ng = T(10)\nm = T(1)\nl = T(1)\ndt = T(0.05)\nmax_steps = 200\ncontinuous::Bool = true\nn_actions::Int = 3\nrng = Random.GLOBAL_RNG\n\n\n\n\n\n","category":"method"},{"location":"rl_envs/#ReinforcementLearningEnvironments.PendulumNonInteractiveEnv","page":"RLEnvs","title":"ReinforcementLearningEnvironments.PendulumNonInteractiveEnv","text":"A non-interactive pendulum environment.\n\nAccepts only nothing actions, which result in the system being simulated for one time step. Sets env.done to true once maximum_time is reached. Resets to a random position and momentum. Always returns zero rewards.\n\nUseful for debugging and development purposes, particularly in model-based reinforcement learning.\n\n\n\n\n\n","category":"type"},{"location":"rl_envs/#ReinforcementLearningEnvironments.PendulumNonInteractiveEnv-Tuple{}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.PendulumNonInteractiveEnv","text":"PendulumNonInteractiveEnv(;kwargs...)\n\nKeyword arguments\n\nfloat_type = Float64\ngravity = 9.8\nlength = 2.0\nmass = 1.0\nstep_size = 0.01\nmaximum_time = 10.0\nrng = Random.GLOBAL_RNG\n\n\n\n\n\n","category":"method"},{"location":"rl_envs/#ReinforcementLearningEnvironments.PigEnv-Tuple{}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.PigEnv","text":"PigEnv(;n_players=2)\n\nSee wiki for explanation of this game.\n\nHere we use it to demonstrate how to write a game with more than 2 players.\n\n\n\n\n\n","category":"method"},{"location":"rl_envs/#ReinforcementLearningEnvironments.RandomWalk1D","page":"RLEnvs","title":"ReinforcementLearningEnvironments.RandomWalk1D","text":"RandomWalk1D(;rewards=-1. => 1.0, N=7, start_pos=(N+1) ÷ 2, actions=[-1,1])\n\nAn agent is placed at the start_pos and can move left or right (stride is defined in actions). The game terminates when the agent reaches either end and receives a reward correspondingly.\n\nCompared to the MultiArmBanditsEnv:\n\nThe state space is more complicated (well, not that complicated though).\nIt's a sequential game of multiple action steps.\nIt's a deterministic game instead of stochastic game.\n\n\n\n\n\n","category":"type"},{"location":"rl_envs/#ReinforcementLearningEnvironments.RockPaperScissorsEnv","page":"RLEnvs","title":"ReinforcementLearningEnvironments.RockPaperScissorsEnv","text":"RockPaperScissorsEnv()\n\nRock Paper Scissors is a simultaneous, zero sum game.\n\n\n\n\n\n","category":"type"},{"location":"rl_envs/#ReinforcementLearningEnvironments.StateCachedEnv","page":"RLEnvs","title":"ReinforcementLearningEnvironments.StateCachedEnv","text":"Cache the state so that state(env) will always return the same result before the next interaction with env. This function is useful because some environments are stateful during each state(env). For example: StateOverriddenEnv(StackFrames(...)).\n\n\n\n\n\n","category":"type"},{"location":"rl_envs/#ReinforcementLearningEnvironments.StateOverriddenEnv","page":"RLEnvs","title":"ReinforcementLearningEnvironments.StateOverriddenEnv","text":"StateOverriddenEnv(f, env)\n\nApply f to override state(env).\n\nnote: Note\nIf the meaning of state space is changed after apply f, one should manually redefine the RLBase.state_space(env::YourSpecificEnv).\n\n\n\n\n\n","category":"type"},{"location":"rl_envs/#ReinforcementLearningEnvironments.TicTacToeEnv","page":"RLEnvs","title":"ReinforcementLearningEnvironments.TicTacToeEnv","text":"This is a typical two player, zero sum game. Here we'll also demonstrate how to implement an environment with multiple state representations.\n\nYou might be interested in this blog\n\n\n\n\n\n","category":"type"},{"location":"rl_envs/#ReinforcementLearningEnvironments.TigerProblemEnv","page":"RLEnvs","title":"ReinforcementLearningEnvironments.TigerProblemEnv","text":"TigerProblemEnv(;rng=Random>GLOBAL_RNG)\n\nHere we use the The Tiger Proglem to demonstrate how to write a POMDP problem.\n\n\n\n\n\n","category":"type"},{"location":"rl_envs/#ReinforcementLearningEnvironments.ZeroTo","page":"RLEnvs","title":"ReinforcementLearningEnvironments.ZeroTo","text":"Similar to Base.OneTo. Useful when wrapping third-party environments.\n\n\n\n\n\n","category":"type"},{"location":"rl_envs/#Random.seed!-Tuple{MultiArmBanditsEnv, Any}","page":"RLEnvs","title":"Random.seed!","text":"The multi-arm bandits environment is a stochastic environment. The resulted reward may be different even after taking the same actions each time. So for this kind of environments, the Random.seed!(env) must be implemented to help increase reproducibility without creating a new instance of the same rng.\n\n\n\n\n\n","category":"method"},{"location":"rl_envs/#ReinforcementLearningBase.action_space-Tuple{MultiArmBanditsEnv}","page":"RLEnvs","title":"ReinforcementLearningBase.action_space","text":"First we need to define the action space. In the MultiArmBanditsEnv environment, the possible actions are 1 to k (which equals to length(env.true_values)).\n\nnote: Note\nAlthough we decide to return an action space of Base.OneTo  here, it is not a hard requirement. You can return anything else (Tuple, Distribution, etc) that is more suitable to describe your problem and handle it correctly in the you_env(action) function. Some algorithms may require that the action space must be of Base.OneTo. However, it's the algorithm designer's job to do the checking and conversion.\n\n\n\n\n\n","category":"method"},{"location":"rl_envs/#ReinforcementLearningBase.current_player-Tuple{RockPaperScissorsEnv}","page":"RLEnvs","title":"ReinforcementLearningBase.current_player","text":"Note that although this is a two player game. The current player is always a dummy simultaneous player.\n\n\n\n\n\n","category":"method"},{"location":"rl_envs/#ReinforcementLearningBase.legal_action_space-Tuple{MontyHallEnv}","page":"RLEnvs","title":"ReinforcementLearningBase.legal_action_space","text":"In the first round, the guest has 3 options, in the second round only two options are valid, those different then the host's action.\n\n\n\n\n\n","category":"method"},{"location":"rl_envs/#ReinforcementLearningBase.legal_action_space_mask-Tuple{MontyHallEnv}","page":"RLEnvs","title":"ReinforcementLearningBase.legal_action_space_mask","text":"For environments of [FULL_ACTION_SET], this function must be implemented.\n\n\n\n\n\n","category":"method"},{"location":"rl_envs/#ReinforcementLearningBase.reward-Tuple{MultiArmBanditsEnv}","page":"RLEnvs","title":"ReinforcementLearningBase.reward","text":"warn: Warn\nIf the env is not started yet, the returned value is meaningless. The reason why we don't throw an exception here is to simplify the code logic to keep type consistency when storing the value in buffers.\n\n\n\n\n\n","category":"method"},{"location":"rl_envs/#ReinforcementLearningBase.state-Tuple{MultiArmBanditsEnv}","page":"RLEnvs","title":"ReinforcementLearningBase.state","text":"Since MultiArmBanditsEnv is just a one-shot game, it doesn't matter what the state is after each action. So here we can simply set it to a constant 1.\n\n\n\n\n\n","category":"method"},{"location":"rl_envs/#ReinforcementLearningBase.state-Tuple{RockPaperScissorsEnv, Observation, Any}","page":"RLEnvs","title":"ReinforcementLearningBase.state","text":"For multi-agent environments, we usually implement the most detailed one.\n\n\n\n\n\n","category":"method"},{"location":"rl_envs/#ReinforcementLearningBase.state-Tuple{TigerProblemEnv}","page":"RLEnvs","title":"ReinforcementLearningBase.state","text":"The main difference compared to other environments is that, now we have two kinds of states. The observation and the internal state. By default we return the observation.\n\n\n\n\n\n","category":"method"},{"location":"rl_envs/#ReinforcementLearningBase.state_space-Tuple{RockPaperScissorsEnv, Observation, Any}","page":"RLEnvs","title":"ReinforcementLearningBase.state_space","text":"Since it's a one-shot game, the state space doesn't have much meaning.\n\n\n\n\n\n","category":"method"},{"location":"rl_envs/#ReinforcementLearningEnvironments.discrete2standard_discrete-Tuple{AbstractEnv}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.discrete2standard_discrete","text":"discrete2standard_discrete(env)\n\nConvert an env with a discrete action space to a standard form:\n\nThe action space is of type Base.OneTo\nIf the env is of FULL_ACTION_SET, then each action in the legal_action_space(env) is also an Int in the action space.\n\nThe standard form is useful for some algorithms (like Q-learning).\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.jl","page":"RLBase","title":"ReinforcementLearningBase.jl","text":"","category":"section"},{"location":"rl_base/","page":"RLBase","title":"RLBase","text":"Modules = [ReinforcementLearningBase]","category":"page"},{"location":"rl_base/#ReinforcementLearningBase.RLBase","page":"RLBase","title":"ReinforcementLearningBase.RLBase","text":"ReinforcementLearningBase.jl (RLBase) provides some common constants, traits, abstractions and interfaces in developing reinforcement learning algorithms in Julia. \n\nBasically, we defined the following two main concepts in reinforcement learning:\n\nAbstractPolicy\nAbstractEnv\n\n\n\n\n\n","category":"module"},{"location":"rl_base/#ReinforcementLearningBase.CONSTANT_SUM","page":"RLBase","title":"ReinforcementLearningBase.CONSTANT_SUM","text":"Rewards of all players sum to a constant\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.DETERMINISTIC","page":"RLBase","title":"ReinforcementLearningBase.DETERMINISTIC","text":"No chance player in the environment. And the game is fully deterministic.\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.EXPLICIT_STOCHASTIC","page":"RLBase","title":"ReinforcementLearningBase.EXPLICIT_STOCHASTIC","text":"Usually used to describe extensive-form game. The environment contains a chance player and the corresponding probability is known. Therefore, prob(env, player=chance_player(env)) must be defined.\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.FULL_ACTION_SET","page":"RLBase","title":"ReinforcementLearningBase.FULL_ACTION_SET","text":"The action space of the environment may contains illegal actions\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.GENERAL_SUM","page":"RLBase","title":"ReinforcementLearningBase.GENERAL_SUM","text":"Total rewards of all players may be different in each step\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.IDENTICAL_UTILITY","page":"RLBase","title":"ReinforcementLearningBase.IDENTICAL_UTILITY","text":"Every player gets the same reward\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.IMPERFECT_INFORMATION","page":"RLBase","title":"ReinforcementLearningBase.IMPERFECT_INFORMATION","text":"The inner state of some players' observations may be different\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.MINIMAL_ACTION_SET","page":"RLBase","title":"ReinforcementLearningBase.MINIMAL_ACTION_SET","text":"All actions in the action space of the environment are legal\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.PERFECT_INFORMATION","page":"RLBase","title":"ReinforcementLearningBase.PERFECT_INFORMATION","text":"All players observe the same state\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.SAMPLED_STOCHASTIC","page":"RLBase","title":"ReinforcementLearningBase.SAMPLED_STOCHASTIC","text":"Environment contains chance player and the probability is unknown. Usually only a dummy action is allowed in this case.\n\nnote: Note\nThe chance player (chance_player(env)) must appears in the result of players(env). The result of action_space(env, chance_player) should only contains one dummy action.\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.SEQUENTIAL","page":"RLBase","title":"ReinforcementLearningBase.SEQUENTIAL","text":"Environment with the DynamicStyle of SEQUENTIAL must takes actions from different players one-by-one.\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.SIMULTANEOUS","page":"RLBase","title":"ReinforcementLearningBase.SIMULTANEOUS","text":"Environment with the DynamicStyle of SIMULTANEOUS must take in actions from some (or all) players at one time\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.STEP_REWARD","page":"RLBase","title":"ReinforcementLearningBase.STEP_REWARD","text":"We can get reward after each step\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.STOCHASTIC","page":"RLBase","title":"ReinforcementLearningBase.STOCHASTIC","text":"No chance player in the environment. And the game is stochastic. To help increase reproducibility, these environments should generally accept a AbstractRNG as a keyword argument. For some third-party environments, at least a seed is exposed in the constructor.\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.TERMINAL_REWARD","page":"RLBase","title":"ReinforcementLearningBase.TERMINAL_REWARD","text":"Only get reward at the end of environment\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.ZERO_SUM","page":"RLBase","title":"ReinforcementLearningBase.ZERO_SUM","text":"Rewards of all players sum to 0. A special case of [CONSTANT_SUM].\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.AbstractEnv","page":"RLBase","title":"ReinforcementLearningBase.AbstractEnv","text":"(env::AbstractEnv)(action, player=current_player(env))\n\nSuper type of all reinforcement learning environments.\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.AbstractEnvironmentModel","page":"RLBase","title":"ReinforcementLearningBase.AbstractEnvironmentModel","text":"TODO:\n\nDescribe how to model a reinforcement learning environment. TODO: need more investigation Ref: https://bair.berkeley.edu/blog/2019/12/12/mbpo/\n\nAnalytic gradient computation\nSampling-based planning\nModel-based data generation\nValue-equivalence prediction Model-based Reinforcement Learning: A Survey. Tutorial on Model-Based Methods in Reinforcement Learning\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.AbstractPolicy","page":"RLBase","title":"ReinforcementLearningBase.AbstractPolicy","text":"(π::AbstractPolicy)(env) -> action\n\nPolicy is the most basic concept in reinforcement learning. Unlike the definition in some other packages, here a policy is defined as a functional object which takes in an environment and returns an action.\n\nnote: Note\nSee discussions here if you are wondering why we define the input as AbstractEnv instead of state.\n\nwarning: Warning\nThe policy π may change its internal state but it shouldn't change env. When it's really necessary, remember to make a copy of env to keep the original env untouched.\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.GoalState","page":"RLBase","title":"ReinforcementLearningBase.GoalState","text":"Use it to represent the goal state\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.InformationSet","page":"RLBase","title":"ReinforcementLearningBase.InformationSet","text":"See the definition of information set\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.InternalState","page":"RLBase","title":"ReinforcementLearningBase.InternalState","text":"Use it to represent the internal state.\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.MultiAgent-Tuple{Integer}","page":"RLBase","title":"ReinforcementLearningBase.MultiAgent","text":"MultiAgent(n::Integer) -> MultiAgent{n}()\n\nn must be ≥ 2.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.Observation","page":"RLBase","title":"ReinforcementLearningBase.Observation","text":"Sometimes people from different field talk about the same thing with a different name. Here we set the Observation{Any}() as the default state style in this package.\n\nSee discussions here\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.Space","page":"RLBase","title":"ReinforcementLearningBase.Space","text":"A wrapper to treat each element as a sub-space which supports:\n\nBase.in\nRandom.rand\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.WorldSpace","page":"RLBase","title":"ReinforcementLearningBase.WorldSpace","text":"In some cases, we may not be interested in the action/state space. One can return WorldSpace() to keep the interface consistent.\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#Base.copy-Tuple{AbstractEnv}","page":"RLBase","title":"Base.copy","text":"Make an independent copy of env, \n\nnote: Note\nrng (if env has) is also copied!\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#Random.seed!-Tuple{AbstractEnv, Any}","page":"RLBase","title":"Random.seed!","text":"Set the seed of internal rng\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.ActionStyle-Tuple{T} where T<:AbstractEnv","page":"RLBase","title":"ReinforcementLearningBase.ActionStyle","text":"ActionStyle(env::AbstractEnv)\n\nFor environments of discrete actions, specify whether the current state of env contains a full action set or a minimal action set. By default the MINIMAL_ACTION_SET is returned.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.ChanceStyle-Tuple{T} where T<:AbstractEnv","page":"RLBase","title":"ReinforcementLearningBase.ChanceStyle","text":"ChanceStyle(env) = DETERMINISTIC\n\nSpecify which role the chance plays in the env. Possible returns are:\n\nSTOCHASTIC. This is the default return.\nDETERMINISTIC\nEXPLICIT_STOCHASTIC\nSAMPLED_STOCHASTIC\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.DefaultStateStyle-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.DefaultStateStyle","text":"Specify the defalt state style when calling state(env).\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.DynamicStyle-Tuple{T} where T<:AbstractEnv","page":"RLBase","title":"ReinforcementLearningBase.DynamicStyle","text":"DynamicStyle(env::AbstractEnv) = SEQUENTIAL\n\nOnly valid in environments with a NumAgentStyle of MultiAgent. Determine whether the players can play simultaneously or not. Possible returns are:\n\nSEQUENTIAL. This is the default return.\nSIMULTANEOUS.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.InformationStyle-Tuple{T} where T<:AbstractEnv","page":"RLBase","title":"ReinforcementLearningBase.InformationStyle","text":"InformationStyle(env) = IMPERFECT_INFORMATION\n\nDistinguish environments between PERFECT_INFORMATION and IMPERFECT_INFORMATION. IMPERFECT_INFORMATION is returned by default.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.NumAgentStyle-Tuple{T} where T<:AbstractEnv","page":"RLBase","title":"ReinforcementLearningBase.NumAgentStyle","text":"NumAgentStyle(env)\n\nNumber of agents involved in the env. Possible returns are:\n\nSINGLE_AGENT. This is the default return.\n[MultiAgent][@ref].\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.RewardStyle-Tuple{T} where T<:AbstractEnv","page":"RLBase","title":"ReinforcementLearningBase.RewardStyle","text":"Specify whether we can get reward after each step or only at the end of an game. Possible values are STEP_REWARD (the default one) or TERMINAL_REWARD.\n\nnote: Note\nEnvironments of TERMINAL_REWARD style can be viewed as a subset of environments of STEP_REWARD style. For some algorithms, like MCTS, we may have some a more efficient implementation for environments of TERMINAL_REWARD style.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.StateStyle-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.StateStyle","text":"StateStyle(env::AbstractEnv)\n\nDefine the possible styles of state(env). Possible values are:\n\nObservation{T}. This is the default return.\nInternalState{T}\nInformation{T}\nYou can also define your customized state style when necessary.\n\nOr a tuple contains several of the above ones.\n\nThis is useful for environments which provide more than one kind of state.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.UtilityStyle-Tuple{T} where T<:AbstractEnv","page":"RLBase","title":"ReinforcementLearningBase.UtilityStyle","text":"UtilityStyle(env::AbstractEnv)\n\nSpecify the utility style in multi-agent environments. Possible values are:\n\nGENERAL_SUM. The default return.\nZERO_SUM\nCONSTANT_SUM\nIDENTICAL_UTILITY\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.action_space","page":"RLBase","title":"ReinforcementLearningBase.action_space","text":"action_space(env, player=current_player(env))\n\nGet all available actions from environment. See also: legal_action_space\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.chance_player-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.chance_player","text":"chance_player(env)\n\nOnly valid for environments with a chance player.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.child-Tuple{AbstractEnv, Any}","page":"RLBase","title":"ReinforcementLearningBase.child","text":"child(env::AbstractEnv, action)\n\nTreat the env as a game tree. Create an independent child after applying action.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.current_player-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.current_player","text":"current_player(env)\n\nReturn the next player to take action. For Extensive Form Games, a chance player may be returned. (See also chance_player) For SIMULTANEOUS environments, a simultaneous player is always returned. (See also simultaneous_player).\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.is_terminated-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.is_terminated","text":"is_terminated(env, player=current_player(env))\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.legal_action_space","page":"RLBase","title":"ReinforcementLearningBase.legal_action_space","text":"legal_action_space(env, player=current_player(env))\n\nFor environments of MINIMAL_ACTION_SET, the result is the same with action_space.\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.legal_action_space_mask","page":"RLBase","title":"ReinforcementLearningBase.legal_action_space_mask","text":"legal_action_space_mask(env, player=current_player(env)) -> AbstractArray{Bool}\n\nRequired for environments of FULL_ACTION_SET.\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.priority-Tuple{AbstractPolicy, Any}","page":"RLBase","title":"ReinforcementLearningBase.priority","text":"priority(π::AbstractPolicy, experience)\n\nUsually used in offline policies.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.prob","page":"RLBase","title":"ReinforcementLearningBase.prob","text":"Get the action distribution of chance player.\n\nnote: Note\nOnly valid for environments of EXPLICIT_STOCHASTIC style. The current player of env must be the chance player.\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.prob-Tuple{AbstractPolicy, Any, Any}","page":"RLBase","title":"ReinforcementLearningBase.prob","text":"prob(π::AbstractPolicy, env, action)\n\nOnly valid for environments with discrete actions.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.prob-Tuple{AbstractPolicy, Any}","page":"RLBase","title":"ReinforcementLearningBase.prob","text":"prob(π::AbstractPolicy, env) -> Distribution\n\nGet the probability distribution of actions based on policy π given an env.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.reset!-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.reset!","text":"Reset the internal state of an environment\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.reward","page":"RLBase","title":"ReinforcementLearningBase.reward","text":"reward(env, player=current_player(env))\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.simultaneous_player-Tuple{Any}","page":"RLBase","title":"ReinforcementLearningBase.simultaneous_player","text":"simultaneous_player(env)\n\nOnly valid for environments of SIMULTANEOUS style.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.spectator_player-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.spectator_player","text":"spectator_player(env)\n\nUsed in imperfect multi-agent environments.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.state-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.state","text":"state(env, style=[DefaultStateStyle(env)], player=[current_player(env)])\n\nThe state can be of any type. However, most neural network based algorithms assume an AbstractArray is returned. For environments with many different states provided (inner state, information state, etc), users need to provide style to declare which kind of state they want.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.state_space-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.state_space","text":"state_space(env, style=[DefaultStateStyle(env)], player=[current_player(env)])\n\nDescribe all possible states.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.test_interfaces!-Tuple{Any}","page":"RLBase","title":"ReinforcementLearningBase.test_interfaces!","text":"Call this function after writing your customized environment to make sure that all the necessary interfaces are implemented correctly and consistently.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.update!-Tuple{AbstractPolicy, Any}","page":"RLBase","title":"ReinforcementLearningBase.update!","text":"update!(π::AbstractPolicy, experience)\n\nUpdate the policy π with online/offline experience or parameters.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.walk-Tuple{Any, AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.walk","text":"walk(f, env::AbstractEnv)\n\nCall f with env and its descendants. Only use it with small games.\n\n\n\n\n\n","category":"method"},{"location":"","page":"Home","title":"Home","text":"<div align=\"center\">\n  <p>\n  <img src=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/raw/master/docs/src/assets/logo.svg?sanitize=true\" width=\"320px\">\n  </p>\n\n  <p>\n  <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/actions?query=workflow%3ACI\"><img src=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/workflows/CI/badge.svg\"></a>\n  <a href=\"https://juliahub.com/ui/Packages/ReinforcementLearning/6l2TO\"><img src=\"https://juliahub.com/docs/ReinforcementLearning/pkgeval.svg\"></a>\n  <a href=\"https://juliahub.com/ui/Packages/ReinforcementLearning/6l2TO\"><img src=\"https://juliahub.com/docs/ReinforcementLearning/version.svg\"></a>\n  <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/master/LICENSE.md\"><img src=\"http://img.shields.io/badge/license-MIT-brightgreen.svg?style=flat\"></a>\n  <a href=\"https://julialang.org/slack/\"><img src=https://img.shields.io/badge/Chat%20on%20Slack-%23reinforcement--learnin-ff69b4\"></a>\n  <a href=\"https://github.com/SciML/ColPrac\"><img src=\"https://img.shields.io/badge/ColPrac-Contributor's%20Guide-blueviolet\"></a>\n  </p>\n</div>","category":"page"},{"location":"","page":"Home","title":"Home","text":"ReinforcementLearning.jl, as the name says, is a package for reinforcement learning research in Julia.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Our design principles are:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Reusability and extensibility: Provide elaborately designed components and interfaces to help users implement new algorithms.\nEasy experimentation: Make it easy for new users to run benchmark experiments, compare different algorithms, evaluate and diagnose agents.\nReproducibility: Facilitate reproducibility from traditional tabular methods to modern deep reinforcement learning algorithms.","category":"page"},{"location":"#Get-Started","page":"Home","title":"Get Started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"julia> ] add ReinforcementLearning\n\njulia> using ReinforcementLearning\n\njulia> run(E`JuliaRL_BasicDQN_CartPole`)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Check out the Get Started page for more detailed explanation!","category":"page"},{"location":"#Project-Structure","page":"Home","title":"Project Structure","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ReinforcementLearning.jl itself is just a wrapper around several other packages inside the JuliaReinforcementLearning org. The relationship between different packages is described below:","category":"page"},{"location":"","page":"Home","title":"Home","text":"+-----------------------------------------------------------------------------------+\n|                                                                                   |\n|  ReinforcementLearning.jl                                                         |\n|                                                                                   |\n|      +------------------------------+                                             |\n|      | ReinforcementLearningBase.jl |                                             |\n|      +----|-------------------------+                                             |\n|           |                                                                       |\n|           |     +--------------------------------------+                          |\n|           +---->+ ReinforcementLearningEnvironments.jl |                          |\n|           |     +--------------------------------------+                          |\n|           |                                                                       |\n|           |     +------------------------------+                                  |\n|           +---->+ ReinforcementLearningCore.jl |                                  |\n|                 +----|-------------------------+                                  |\n|                      |                                                            |\n|                      |     +-----------------------------+                        |\n|                      +---->+ ReinforcementLearningZoo.jl |                        |\n|                            +----|------------------------+                        |\n|                                 |                                                 |\n|                                 |     +-------------------------------------+     |\n|                                 +---->+ DistributedReinforcementLearning.jl |     |\n|                                       +-------------------------------------+     |\n|                                                                                   |\n+-----------------------------------------------------------------------------------+","category":"page"},{"location":"#Scope-of-Each-Package","page":"Home","title":"Scope of Each Package","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ReinforcementLearningBase.jl Two main concepts in reinforcement learning are precisely defined here: Policy and Environment.\nReinforcementLearningEnvironments.jl Typical environment examples in pure Julia and wrappers for 3-rd party environments are provided in this package.\nReinforcementLearningCore.jl Common utility functions and different layers of abstractions are contained in this package.\nReinforcementLearningZoo.jl Common reinforcement learning algorithms and their typical applications (aka Experiments) are collected in this package.\nDistributedReinforcementLearning.jl This package is still experimental and is not included in ReinforcementLearning.jl yet. Its goal is to extend some algorithms in ReinforcementLearningZoo.jl to apply them in distributed computing systems.","category":"page"},{"location":"#Supporting","page":"Home","title":"Supporting 🖖","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ReinforcementLearning.jl is a MIT licensed open source project with its ongoing development made possible by many contributors in their spare time. However, modern reinforcement learning research requires huge computing resource, which is unaffordable for individual contributors. So if you or your organization could provide the computing resource in some degree and would like to cooperate in some way, please contact us!","category":"page"},{"location":"#Citing","page":"Home","title":"Citing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you use ReinforcementLearning.jl in a scientific publication, we would appreciate references to the following BibTex entry:","category":"page"},{"location":"","page":"Home","title":"Home","text":"@misc{Tian2020Reinforcement,\n  author       = {Jun Tian and other contributors},\n  title        = {ReinforcementLearning.jl: A Reinforcement Learning Package for the Julia Language},\n  year         = 2020,\n  url          = {https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl}\n}","category":"page"},{"location":"rl_core/#ReinforcementLearningCore.jl","page":"RLCore","title":"ReinforcementLearningCore.jl","text":"","category":"section"},{"location":"rl_core/","page":"RLCore","title":"RLCore","text":"Modules = [ReinforcementLearningCore]","category":"page"},{"location":"rl_core/#ReinforcementLearningCore.RLCore","page":"RLCore","title":"ReinforcementLearningCore.RLCore","text":"ReinforcementLearningCore.jl (RLCore) provides some standard and reusable components defined by RLBase, hoping that they are useful for people to implement and experiment with different kinds of algorithms.\n\n\n\n\n\n","category":"module"},{"location":"rl_core/#ReinforcementLearningCore.AbstractApproximator","page":"RLCore","title":"ReinforcementLearningCore.AbstractApproximator","text":"(app::AbstractApproximator)(env)\n\nAn approximator is a functional object for value estimation. It serves as a black box to provides an abstraction over different  kinds of approximate methods (for example DNN provided by Flux or Knet).\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.AbstractExplorer","page":"RLCore","title":"ReinforcementLearningCore.AbstractExplorer","text":"(p::AbstractExplorer)(x)\n(p::AbstractExplorer)(x, mask)\n\nDefine how to select an action based on action values.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.AbstractHook","page":"RLCore","title":"ReinforcementLearningCore.AbstractHook","text":"A hook is called at different stage duiring a run to allow users to inject customized runtime logic. By default, a AbstractHook will do nothing. One can override the behavior by implementing the following methods:\n\n(hook::YourHook)(::PreActStage, agent, env, action), note that there's an extra argument of action.\n(hook::YourHook)(::PostActStage, agent, env)\n(hook::YourHook)(::PreEpisodeStage, agent, env)\n(hook::YourHook)(::PostEpisodeStage, agent, env)\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.AbstractLearner","page":"RLCore","title":"ReinforcementLearningCore.AbstractLearner","text":"(learner::AbstractLearner)(env)\n\nA learner is usually used to estimate state values, state-action values or distributional values based on experiences.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.AbstractTrajectory","page":"RLCore","title":"ReinforcementLearningCore.AbstractTrajectory","text":"AbstractTrajectory\n\nA trajectory is used to record some useful information during the interactions between agents and environments. It behaves similar to a NamedTuple except that we extend it with some optional methods.\n\nRequired Methods:\n\nBase.getindex\nBase.keys\n\nOptional Methods:\n\nBase.length\nBase.isempty\nBase.empty!\nBase.haskey\nBase.push!\nBase.pop!\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.ActorCritic","page":"RLCore","title":"ReinforcementLearningCore.ActorCritic","text":"ActorCritic(;actor, critic, optimizer=ADAM())\n\nThe actor part must return logits (Do not use softmax in the last layer!), and the critic part must return a state value.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.Agent","page":"RLCore","title":"ReinforcementLearningCore.Agent","text":"Agent(;kwargs...)\n\nA wrapper of an AbstractPolicy. Generally speaking, it does nothing but to update the trajectory and policy appropriately in different stages.\n\nKeywords & Fields\n\npolicy::AbstractPolicy: the policy to use\ntrajectory::AbstractTrajectory: used to store transitions between an agent and an environment\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.Agent-Tuple{AbstractStage, AbstractEnv}","page":"RLCore","title":"ReinforcementLearningCore.Agent","text":"Here we extend the definition of (p::AbstractPolicy)(::AbstractEnv) in RLBase to accept an AbstractStage as the first argument. Algorithm designers may customize these behaviors respectively by implementing:\n\n(p::YourPolicy)(::AbstractStage, ::AbstractEnv)\n(p::YourPolicy)(::PreActStage, ::AbstractEnv, action)\n\nThe default behaviors for Agent are:\n\nUpdate the inner trajectory given the context of policy, env, and stage.\nBy default we do nothing.\nIn PreActStage, we push! the current state and the action into   the trajectory.\nIn PostActStage, we query the reward and is_terminated info from   env and push them into trajectory.\nIn the PosEpisodeStage, we push the state at the end of an episode and   a dummy action into the trajectory.\nIn the PreEpisodeStage, we pop out the lastest state and action pair   (which are dummy ones) from trajectory.\nUpdate the inner policy given the context of trajectory, env, and stage.\nBy default, we only update! the policy in the PreActStage. And it's   despatched to update!(policy, trajectory).\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.BatchExplorer","page":"RLCore","title":"ReinforcementLearningCore.BatchExplorer","text":"BatchExplorer(explorer::AbstractExplorer)\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.BatchExplorer-Tuple{AbstractMatrix{T} where T}","page":"RLCore","title":"ReinforcementLearningCore.BatchExplorer","text":"(x::BatchExplorer)(values::AbstractMatrix)\n\nApply inner explorer to each column of values.\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.BatchStepsPerEpisode-Tuple{Int64}","page":"RLCore","title":"ReinforcementLearningCore.BatchStepsPerEpisode","text":"BatchStepsPerEpisode(batch_size::Int; tag = \"TRAINING\")\n\nSimilar to StepsPerEpisode, but is specific to environments which return a Vector of rewards (a typical case with MultiThreadEnv).\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.ComposedHook","page":"RLCore","title":"ReinforcementLearningCore.ComposedHook","text":"ComposedHook(hooks::AbstractHook...)\n\nCompose different hooks into a single hook.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.ComposedStopCondition","page":"RLCore","title":"ReinforcementLearningCore.ComposedStopCondition","text":"ComposedStopCondition(stop_conditions...; reducer = any)\n\nThe result of stop_conditions is reduced by reducer.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.DoEveryNEpisode","page":"RLCore","title":"ReinforcementLearningCore.DoEveryNEpisode","text":"DoEveryNEpisode(f; n=1, t=0)\n\nExecute f(agent, env) every n episode. t is a counter of steps.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.DoEveryNStep","page":"RLCore","title":"ReinforcementLearningCore.DoEveryNStep","text":"DoEveryNStep(f; n=1, t=0)\n\nExecute f(agent, env) every n step. t is a counter of steps.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.EmptyHook","page":"RLCore","title":"ReinforcementLearningCore.EmptyHook","text":"Do nothing\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.EpsilonGreedyExplorer","page":"RLCore","title":"ReinforcementLearningCore.EpsilonGreedyExplorer","text":"EpsilonGreedyExplorer{T}(;kwargs...)\nEpsilonGreedyExplorer(ϵ) -> EpsilonGreedyExplorer{:linear}(; ϵ_stable = ϵ)\n\nEpsilon-greedy strategy: The best lever is selected for a proportion 1 - epsilon of the trials, and a lever is selected at random (with uniform probability) for a proportion epsilon . Multi-armed_bandit\n\nTwo kinds of epsilon-decreasing strategy are implmented here (linear and exp).\n\nEpsilon-decreasing strategy: Similar to the epsilon-greedy strategy, except that the value of epsilon decreases as the experiment progresses, resulting in highly explorative behaviour at the start and highly exploitative behaviour at the finish.  - Multi-armed_bandit\n\nKeywords\n\nT::Symbol: defines how to calculate the epsilon in the warmup steps. Supported values are linear and exp.\nstep::Int = 1: record the current step.\nϵ_init::Float64 = 1.0: initial epsilon.\nwarmup_steps::Int=0: the number of steps to use ϵ_init.\ndecay_steps::Int=0: the number of steps for epsilon to decay from ϵ_init to ϵ_stable.\nϵ_stable::Float64: the epsilon after warmup_steps + decay_steps.\nis_break_tie=false: randomly select an action of the same maximum values if set to true.\nrng=Random.GLOBAL_RNG: set the internal RNG.\nis_training=true, in training mode, step will not be updated. And the ϵ will be set to 0.\n\nExample\n\ns = EpsilonGreedyExplorer{:linear}(ϵ_init=0.9, ϵ_stable=0.1, warmup_steps=100, decay_steps=100)\nplot([RL.get_ϵ(s, i) for i in 1:500], label=\"linear epsilon\")\n\n(Image: )\n\ns = EpsilonGreedyExplorer{:exp}(ϵ_init=0.9, ϵ_stable=0.1, warmup_steps=100, decay_steps=100)\nplot([RL.get_ϵ(s, i) for i in 1:500], label=\"exp epsilon\")\n\n(Image: )\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.EpsilonGreedyExplorer-Tuple{Any}","page":"RLCore","title":"ReinforcementLearningCore.EpsilonGreedyExplorer","text":"(s::EpsilonGreedyExplorer)(values; step) where T\n\nnote: Note\nIf multiple values with the same maximum value are found. Then a random one will be returned!NaN will be filtered unless all the values are NaN. In that case, a random one will be returned.\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.MultiAgentHook","page":"RLCore","title":"ReinforcementLearningCore.MultiAgentHook","text":"MultiAgentHook(player=>hook...)\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.MultiAgentManager-Tuple","page":"RLCore","title":"ReinforcementLearningCore.MultiAgentManager","text":"MultiAgentManager(player => policy...)\n\nThis is the simplest form of multiagent system. At each step they observe the environment from their own perspective and get updated independently. For environments of SEQUENTIAL style, agents which are not the current player will observe a dummy action of NO_OP in the PreActStage.\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.NamedPolicy","page":"RLCore","title":"ReinforcementLearningCore.NamedPolicy","text":"NamedPolicy(name=>policy)\n\nA policy wrapper to provide a name. Mostly used in multi-agent environments.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.NeuralNetworkApproximator","page":"RLCore","title":"ReinforcementLearningCore.NeuralNetworkApproximator","text":"NeuralNetworkApproximator(;kwargs)\n\nUse a DNN model for value estimation.\n\nKeyword arguments\n\nmodel, a Flux based DNN model.\noptimizer=nothing\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.NoOp","page":"RLCore","title":"ReinforcementLearningCore.NoOp","text":"Represent no-operation if it's not the agent's turn.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.QBasedPolicy","page":"RLCore","title":"ReinforcementLearningCore.QBasedPolicy","text":"QBasedPolicy(;learner::Q, explorer::S)\n\nUse a Q-learner to generate estimations of action values. Then an explorer is applied on the estimations to select an action.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.RandomPolicy","page":"RLCore","title":"ReinforcementLearningCore.RandomPolicy","text":"RandomPolicy(action_space=nothing; rng=Random.GLOBAL_RNG)\n\nIf action_space is nothing, then it will use the legal_action_space at runtime to randomly select an action. Otherwise, a random element within action_space is selected. \n\nnote: Note\nYou should always set action_space=nothing when dealing with environments of FULL_ACTION_SET.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.ResizeImage","page":"RLCore","title":"ReinforcementLearningCore.ResizeImage","text":"ResizeImage(img::Array{T, N})\nResizeImage(dims::Int...) -> ResizeImage(Float32, dims...)\nResizeImage(T::Type{<:Number}, dims::Int...)\n\nUsing BSpline method to resize the state field of an observation to size of img (or dims).\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.RewardsPerEpisode","page":"RLCore","title":"ReinforcementLearningCore.RewardsPerEpisode","text":"RewardsPerEpisode(; rewards = Vector{Vector{Float64}}())\n\nStore each reward of each step in every episode in the field of rewards.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.StackFrames","page":"RLCore","title":"ReinforcementLearningCore.StackFrames","text":"StackFrames(::Type{T}=Float32, d::Int...)\n\nUse a pre-initialized CircularArrayBuffer to store the latest several states specified by d. Before processing any observation, the buffer is filled with `zero{T} by default.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.StepsPerEpisode","page":"RLCore","title":"ReinforcementLearningCore.StepsPerEpisode","text":"StepsPerEpisode(; steps = Int[], count = 0)\n\nStore steps of each episode in the field of steps.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.StopAfterEpisode","page":"RLCore","title":"ReinforcementLearningCore.StopAfterEpisode","text":"StopAfterEpisode(episode; cur = 0, is_show_progress = true)\n\nReturn true after being called episode. If is_show_progress is true, the ProgressMeter will be used to show progress.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.StopAfterNSeconds","page":"RLCore","title":"ReinforcementLearningCore.StopAfterNSeconds","text":"StopAfterNSeconds\n\nparameter:\n\ntime badget\n\nstop training after N seconds\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.StopAfterNoImprovement","page":"RLCore","title":"ReinforcementLearningCore.StopAfterNoImprovement","text":"StopAfterNoImprovement()\n\nStop training when a monitored metric has stopped improving.\n\nParameters:\n\nfn: a closure, return a scalar value, which indicates the performance of the policy (the higher the better) e.g.\n\n() -> reward(env)\n() -> totalrewardper_episode.reward\n\npatience: Number of epochs with no improvement after which training will be stopped.\n\nδ: Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.\n\nReturn true after the monitored metric has stopped improving.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.StopAfterStep","page":"RLCore","title":"ReinforcementLearningCore.StopAfterStep","text":"StopAfterStep(step; cur = 1, is_show_progress = true)\n\nReturn true after being called step times.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.StopSignal","page":"RLCore","title":"ReinforcementLearningCore.StopSignal","text":"StopSignal()\n\nCreate a stop signal initialized with a value of false. You can manually set it to true by s[] = true to stop the running loop at any time.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.StopWhenDone","page":"RLCore","title":"ReinforcementLearningCore.StopWhenDone","text":"StopWhenDone()\n\nReturn true if the environment is terminated.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.SumTree","page":"RLCore","title":"ReinforcementLearningCore.SumTree","text":"SumTree(capacity::Int)\n\nEfficiently sample and update weights. For more detals, see the post at here. Here we use a vector to represent the binary tree. Suppose we will have capacity leaves at most. Every time we push! new node into the tree, only the recent capacity node and their sum will be updated! [––––––Parent nodes––––––][––––leaves––––] [size: 2^ceil(Int, log2(capacity))-1 ][     size: capacity   ]\n\nExample\n\njulia> t = SumTree(8)\n0-element SumTree\njulia> for i in 1:16\n       push!(t, i)\n       end\njulia> t\n8-element SumTree:\n  9.0\n 10.0\n 11.0\n 12.0\n 13.0\n 14.0\n 15.0\n 16.0\njulia> sample(t)\n(2, 10.0)\njulia> sample(t)\n(1, 9.0)\njulia> inds, ps = sample(t,100000)\n([8, 4, 8, 1, 5, 2, 2, 7, 6, 6  …  1, 1, 7, 1, 6, 1, 5, 7, 2, 7], [16.0, 12.0, 16.0, 9.0, 13.0, 10.0, 10.0, 15.0, 14.0, 14.0  …  9.0, 9.0, 15.0, 9.0, 14.0, 9.0, 13.0, 15.0, 10.0, 15.0])\njulia> countmap(inds)\nDict{Int64,Int64} with 8 entries:\n  7 => 14991\n  4 => 12019\n  2 => 10003\n  3 => 11027\n  5 => 12971\n  8 => 16052\n  6 => 13952\n  1 => 8985\njulia> countmap(ps)\nDict{Float64,Int64} with 8 entries:\n  9.0  => 8985\n  13.0 => 12971\n  10.0 => 10003\n  14.0 => 13952\n  16.0 => 16052\n  11.0 => 11027\n  15.0 => 14991\n  12.0 => 12019\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.TabularApproximator","page":"RLCore","title":"ReinforcementLearningCore.TabularApproximator","text":"TabularApproximator(table<:AbstractArray, opt)\n\nFor table of 1-d, it will serve as a state value approximator. For table of 2-d, it will serve as a state-action value approximator.\n\nwarning: Warning\nFor table of 2-d, the first dimension is action and the second dimension is state.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.TabularRandomPolicy","page":"RLCore","title":"ReinforcementLearningCore.TabularRandomPolicy","text":"TabularRandomPolicy(;table=Dict{Int, Float32}(), rng=Random.GLOBAL_RNG)\n\nUse a Dict to store action distribution.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.TimePerStep","page":"RLCore","title":"ReinforcementLearningCore.TimePerStep","text":"TimePerStep(;max_steps=100)\nTimePerStep(times::CircularArrayBuffer{Float64}, t::UInt64)\n\nStore time cost of the latest max_steps in the times field.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.TotalBatchRewardPerEpisode-Tuple{Int64}","page":"RLCore","title":"ReinforcementLearningCore.TotalBatchRewardPerEpisode","text":"TotalBatchRewardPerEpisode(batch_size::Int)\n\nSimilar to TotalRewardPerEpisode, but is specific to environments which return a Vector of rewards (a typical case with MultiThreadEnv).\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.TotalRewardPerEpisode","page":"RLCore","title":"ReinforcementLearningCore.TotalRewardPerEpisode","text":"TotalRewardPerEpisode(; rewards = Float64[], reward = 0.0)\n\nStore the total rewards of each episode in the field of rewards.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.Trajectory","page":"RLCore","title":"ReinforcementLearningCore.Trajectory","text":"Trajectory(;[trace_name=trace_container]...)\n\nA simple wrapper of NamedTuple. Define our own type here to avoid type piracy with NamedTuple\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.UCBExplorer-Tuple{Any}","page":"RLCore","title":"ReinforcementLearningCore.UCBExplorer","text":"UCBExplorer(na; c=2.0, ϵ=1e-10, step=1, seed=nothing)\n\nArguments\n\nna is the number of actions used to create a internal counter.\nt is used to store current time step.\nc is used to control the degree of exploration.\nseed, set the seed of inner RNG.\nis_training=true, in training mode, time step and counter will not be updated.\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.UploadTrajectoryEveryNStep","page":"RLCore","title":"ReinforcementLearningCore.UploadTrajectoryEveryNStep","text":"UploadTrajectoryEveryNStep(;mailbox, n, sealer=deepcopy)\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.VBasedPolicy","page":"RLCore","title":"ReinforcementLearningCore.VBasedPolicy","text":"VBasedPolicy(;learner, mapping=default_value_action_mapping)\n\nThe learner must be a value learner. The mapping is a function which returns an action given env and the learner. By default we iterate through all the valid actions and select the best one which lead to the maximum state value.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.WeightedExplorer","page":"RLCore","title":"ReinforcementLearningCore.WeightedExplorer","text":"WeightedExplorer(;is_normalized::Bool, rng=Random.GLOBAL_RNG)\n\nis_normalized is used to indicate if the feeded action values are alrady normalized to have a sum of 1.0.\n\nwarning: Warning\nElements are assumed to be >=0.\n\nSee also: WeightedSoftmaxExplorer\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#Base.push!-Union{Tuple{N}, Tuple{T}, Tuple{CircularArrayBuffers.CircularArrayBuffer{T, N}, StackFrames{T, N}}} where {T, N}","page":"RLCore","title":"Base.push!","text":"When pushing a StackFrames into a CircularArrayBuffer of the same dimension, only the latest frame is pushed. If the StackFrames is one dimension lower, then it is treated as a general AbstractArray and is pushed in as a frame.\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#CUDA.device-Tuple{Any}","page":"RLCore","title":"CUDA.device","text":"device(model)\n\nDetect the suitable running device for the model. Return Val(:cpu) by default.\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningBase.priority-Tuple{AbstractLearner, Any}","page":"RLCore","title":"ReinforcementLearningBase.priority","text":"get_priority(p::AbstractLearner, experience)\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningBase.prob-Tuple{AbstractExplorer, Any, Any}","page":"RLCore","title":"ReinforcementLearningBase.prob","text":"prob(p::AbstractExplorer, x, mask)\n\nSimilart to prob(p::AbstractExplorer, x), but here only the masked elements are considered.\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningBase.prob-Tuple{AbstractExplorer, Any}","page":"RLCore","title":"ReinforcementLearningBase.prob","text":"prob(p::AbstractExplorer, x) -> AbstractDistribution\n\nGet the action distribution given action values.\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningBase.prob-Tuple{EpsilonGreedyExplorer{var\"#s104\", true, R} where {var\"#s104\", R}, Any}","page":"RLCore","title":"ReinforcementLearningBase.prob","text":"prob(s::EpsilonGreedyExplorer, values) ->Categorical\nprob(s::EpsilonGreedyExplorer, values, mask) ->Categorical\n\nReturn the probability of selecting each action given the estimated values of each action.\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningBase.update!-Tuple{AbstractApproximator, Any}","page":"RLCore","title":"ReinforcementLearningBase.update!","text":"update!(a::AbstractApproximator, correction)\n\nUsually the correction is the gradient of inner parameters.\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningBase.update!-Tuple{TabularRandomPolicy, Pair}","page":"RLCore","title":"ReinforcementLearningBase.update!","text":"update!(p::TabularRandomPolicy, state => value)\n\nYou should manually check value sum to 1.0.\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.ApproximatorStyle-Tuple{AbstractApproximator}","page":"RLCore","title":"ReinforcementLearningCore.ApproximatorStyle","text":"Used to detect what an AbstractApproximator is approximating.\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore._discount_rewards!-Tuple{Any, Any, Any, Any, Nothing}","page":"RLCore","title":"ReinforcementLearningCore._discount_rewards!","text":"assuming rewards and new_rewards are Vector\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore._generalized_advantage_estimation!-NTuple{6, Any}","page":"RLCore","title":"ReinforcementLearningCore._generalized_advantage_estimation!","text":"assuming rewards and advantages are Vector\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.check-Tuple{Any, Any}","page":"RLCore","title":"ReinforcementLearningCore.check","text":"Inject some customized checkings here by overwriting this function\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.consecutive_view-Tuple{AbstractArray, Vector{Int64}}","page":"RLCore","title":"ReinforcementLearningCore.consecutive_view","text":"consecutive_view(x::AbstractArray, inds; n_stack = nothing, n_horizon = nothing)\n\nBy default, it behaves the same with select_last_dim(x, inds). If n_stack is set to an int, then for each frame specified by inds, the previous n_stack frames (including the current one) are concatenated as a new dimension. If n_horizon is set to an int, then for each frame specified by inds, the next n_horizon frames (including the current one) are concatenated as a new dimension.\n\nExample\n\njulia> x = collect(1:5)\n5-element Array{Int64,1}:\n 1\n 2\n 3\n 4\n 5\n\njulia> consecutive_view(x, [2,4])  # just the same with `select_last_dim(x, [2,4])`\n2-element view(::Array{Int64,1}, [2, 4]) with eltype Int64:\n 2\n 4\n\njulia> consecutive_view(x, [2,4];n_stack = 2)\n2×2 view(::Array{Int64,1}, [1 3; 2 4]) with eltype Int64:\n 1  3\n 2  4\n\njulia> consecutive_view(x, [2,4];n_horizon = 2)\n2×2 view(::Array{Int64,1}, [2 4; 3 5]) with eltype Int64:\n 2  4\n 3  5\n\njulia> consecutive_view(x, [2,4];n_horizon = 2, n_stack=2)  # note the order here, first we stack, then we apply the horizon\n2×2×2 view(::Array{Int64,1}, [1 2; 2 3]\n\n[3 4; 4 5]) with eltype Int64:\n[:, :, 1] =\n 1  2\n 2  3\n\n[:, :, 2] =\n 3  4\n 4  5\n\nSee also Frame Skipping and Preprocessing for Deep Q networks to gain a better understanding of state stacking and n-step learning.\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.discount_rewards-Union{Tuple{T}, Tuple{Union{AbstractMatrix{T} where T, AbstractVector{T} where T}, T}} where T<:Number","page":"RLCore","title":"ReinforcementLearningCore.discount_rewards","text":"discount_rewards(rewards::VectorOrMatrix, γ::Number;kwargs...)\n\nCalculate the gain started from the current step with discount rate of γ. rewards can be a matrix.\n\nKeyword argments\n\ndims=:, if rewards is a Matrix, then dims can only be 1 or 2.\nterminal=nothing, specify if each reward follows by a terminal. nothing means the game is not terminated yet. If terminal is provided, then the size must be the same with rewards.\ninit=nothing, init can be used to provide the the reward estimation of the last state.\n\nExample\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.flatten_batch-Tuple{AbstractArray}","page":"RLCore","title":"ReinforcementLearningCore.flatten_batch","text":"flatten_batch(x::AbstractArray)\n\nMerge the last two dimension.\n\nExample\n\njulia> x = reshape(1:12, 2, 2, 3)\n2×2×3 reshape(::UnitRange{Int64}, 2, 2, 3) with eltype Int64:\n[:, :, 1] =\n 1  3\n 2  4\n\n[:, :, 2] =\n 5  7\n 6  8\n\n[:, :, 3] =\n  9  11\n 10  12\n\njulia> flatten_batch(x)\n2×6 reshape(::UnitRange{Int64}, 2, 6) with eltype Int64:\n 1  3  5  7   9  11\n 2  4  6  8  10  12\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.generalized_advantage_estimation-Union{Tuple{T}, Tuple{Union{AbstractMatrix{T} where T, AbstractVector{T} where T}, Union{AbstractMatrix{T} where T, AbstractVector{T} where T}, T, T}} where T<:Number","page":"RLCore","title":"ReinforcementLearningCore.generalized_advantage_estimation","text":"generalized_advantage_estimation(rewards::VectorOrMatrix, values::VectorOrMatrix, γ::Number, λ::Number;kwargs...)\n\nCalculate the generalized advantage estimate started from the current step with discount rate of γ and a lambda for GAE-Lambda of 'λ'. rewards and 'values' can be a matrix.\n\nKeyword argments\n\ndims=:, if rewards is a Matrix, then dims can only be 1 or 2.\nterminal=nothing, specify if each reward follows by a terminal. nothing means the game is not terminated yet. If terminal is provided, then the size must be the same with rewards.\n\nExample\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.normlogpdf-Tuple{Any, Any, Any}","page":"RLCore","title":"ReinforcementLearningCore.normlogpdf","text":"GPU automatic differentiable version for the logpdf function of normal distributions. Adding an epsilon value to guarantee numeric stability if sigma is exactly zero (e.g. if relu is used in output layer).\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#StatsBase.sample-Tuple{AbstractTrajectory, ReinforcementLearningCore.AbstractSampler}","page":"RLCore","title":"StatsBase.sample","text":"sample([rng=Random.GLOBAL_RNG], trajectory, sampler, [traces=Val(keys(trajectory))])\n\nnote: Note\nHere we return a copy instead of a view:Each sample is independent of the original trajectory so that trajectory can be updated async.\nCopy is not always so bad.\n\n\n\n\n\n","category":"method"}]
}
