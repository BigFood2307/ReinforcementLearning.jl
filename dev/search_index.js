var documenterSearchIndex = {"docs":
[{"location":"rl_zoo/#ReinforcementLearningZoo.jl","page":"RLZoo","title":"ReinforcementLearningZoo.jl","text":"","category":"section"},{"location":"rl_zoo/","page":"RLZoo","title":"RLZoo","text":"Modules = [ReinforcementLearningZoo]","category":"page"},{"location":"rl_zoo/#ReinforcementLearningZoo.A2CGAELearner","page":"RLZoo","title":"ReinforcementLearningZoo.A2CGAELearner","text":"A2CGAELearner(;kwargs...)\n\nKeyword arguments\n\napproximator, an ActorCritic based NeuralNetworkApproximator\nγ::Float32, reward discount rate.\n'λ::Float32', lambda for GAE-lambda\nactor_loss_weight::Float32\ncritic_loss_weight::Float32\nentropy_loss_weight::Float32\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.A2CLearner","page":"RLZoo","title":"ReinforcementLearningZoo.A2CLearner","text":"A2CLearner(;kwargs...)\n\nKeyword arguments\n\napproximator::ActorCritic\nγ::Float32, reward discount rate.\nactor_loss_weight::Float32\ncritic_loss_weight::Float32\nentropy_loss_weight::Float32\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.BasicDQNLearner","page":"RLZoo","title":"ReinforcementLearningZoo.BasicDQNLearner","text":"BasicDQNLearner(;kwargs...)\n\nSee paper: Playing Atari with Deep Reinforcement Learning\n\nThis is the very basic implementation of DQN. Compared to the traditional Q learning, the only difference is that, in the updating step it uses a batch of transitions sampled from an experience buffer instead of current transition. And the approximator is usually a NeuralNetworkApproximator. You can start from this implementation to understand how everything is organized and how to write your own customized algorithm.\n\nKeywords\n\napproximator::AbstractApproximator: used to get Q-values of a state.\nloss_func: the loss function to use. TODO: provide a default huber_loss?\nγ::Float32=0.99f0: discount rate.\nbatch_size::Int=32\nmin_replay_history::Int=32: number of transitions that should be experienced before updating the approximator.\nrng=Random.GLOBAL_RNG\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.DDPGPolicy-Tuple{}","page":"RLZoo","title":"ReinforcementLearningZoo.DDPGPolicy","text":"DDPGPolicy(;kwargs...)\n\nKeyword arguments\n\nbehavior_actor,\nbehavior_critic,\ntarget_actor,\ntarget_critic,\nstart_policy,\nγ = 0.99f0,\nρ = 0.995f0,\nbatch_size = 32,\nstart_steps = 10000,\nupdate_after = 1000,\nupdate_every = 50,\nact_limit = 1.0,\nact_noise = 0.1,\nstep = 0,\nrng = Random.GLOBAL_RNG,\n\n\n\n\n\n","category":"method"},{"location":"rl_zoo/#ReinforcementLearningZoo.DQNLearner","page":"RLZoo","title":"ReinforcementLearningZoo.DQNLearner","text":"DQNLearner(;kwargs...)\n\nSee paper: Human-level control through deep reinforcement learning\n\nKeywords\n\napproximator::AbstractApproximator: used to get Q-values of a state.\ntarget_approximator::AbstractApproximator: similar to approximator, but used to estimate the target (the next state).\nloss_func: the loss function.\nγ::Float32=0.99f0: discount rate.\nbatch_size::Int=32\nupdate_horizon::Int=1: length of update ('n' in n-step update).\nmin_replay_history::Int=32: number of transitions that should be experienced before updating the approximator.\nupdate_freq::Int=4: the frequency of updating the approximator.\ntarget_update_freq::Int=100: the frequency of syncing target_approximator.\nstack_size::Union{Int, Nothing}=4: use the recent stack_size frames to form a stacked state.\nrng = Random.GLOBAL_RNG\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.DQNLearner-Tuple{Any}","page":"RLZoo","title":"ReinforcementLearningZoo.DQNLearner","text":"note: Note\nThe state of the observation is assumed to have been stacked, if !isnothing(stack_size).\n\n\n\n\n\n","category":"method"},{"location":"rl_zoo/#ReinforcementLearningZoo.IQNLearner","page":"RLZoo","title":"ReinforcementLearningZoo.IQNLearner","text":"IQNLearner(;kwargs)\n\nSee paper\n\nKeyworkd arugments\n\napproximator, a ImplicitQuantileNet\ntarget_approximator, a ImplicitQuantileNet, must have the same structure as approximator\nκ = 1.0f0,\nN = 32,\nN′ = 32,\nNₑₘ = 64,\nK = 32,\nγ = 0.99f0,\nstack_size = 4,\nbatch_size = 32,\nupdate_horizon = 1,\nmin_replay_history = 20000,\nupdate_freq = 4,\ntarget_update_freq = 8000,\nupdate_step = 0,\ndefault_priority = 1.0f2,\nβ_priority = 0.5f0,\nrng = Random.GLOBAL_RNG,\ndevice_seed = nothing,\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.ImplicitQuantileNet","page":"RLZoo","title":"ReinforcementLearningZoo.ImplicitQuantileNet","text":"ImplicitQuantileNet(;ψ, ϕ, header)\n\n        quantiles (n_action, n_quantiles, batch_size)\n           ↑\n         header\n           ↑\nfeature ↱  ⨀   ↰ transformed embedding\n       ψ       ϕ\n       ↑       ↑\n       s        τ\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.PPOLearner","page":"RLZoo","title":"ReinforcementLearningZoo.PPOLearner","text":"PPOLearner(;kwargs)\n\nKeyword arguments\n\napproximator,\nγ = 0.99f0,\nλ = 0.95f0,\nclip_range = 0.2f0,\nmax_grad_norm = 0.5f0,\nn_microbatches = 4,\nn_epochs = 4,\nactor_loss_weight = 1.0f0,\ncritic_loss_weight = 0.5f0,\nentropy_loss_weight = 0.01f0,\nrng = Random.GLOBAL_RNG,\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.PrioritizedDQNLearner","page":"RLZoo","title":"ReinforcementLearningZoo.PrioritizedDQNLearner","text":"PrioritizedDQNLearner(;kwargs...)\n\nSee paper: Prioritized Experience Replay And also https://danieltakeshi.github.io/2019/07/14/per/\n\nKeywords\n\napproximator::AbstractApproximator: used to get Q-values of a state.\ntarget_approximator::AbstractApproximator: similar to approximator, but used to estimate the target (the next state).\nloss_func: the loss function.\nγ::Float32=0.99f0: discount rate.\nbatch_size::Int=32\nupdate_horizon::Int=1: length of update ('n' in n-step update).\nmin_replay_history::Int=32: number of transitions that should be experienced before updating the approximator.\nupdate_freq::Int=4: the frequency of updating the approximator.\ntarget_update_freq::Int=100: the frequency of syncing target_approximator.\nstack_size::Union{Int, Nothing}=4: use the recent stack_size frames to form a stacked state.\ndefault_priority::Float64=100.: the default priority for newly added transitions.\nrng = Random.GLOBAL_RNG\n\n\n\n\n\n","category":"type"},{"location":"rl_zoo/#ReinforcementLearningZoo.PrioritizedDQNLearner-Tuple{Any}","page":"RLZoo","title":"ReinforcementLearningZoo.PrioritizedDQNLearner","text":"note: Note\nThe state of the observation is assumed to have been stacked, if !isnothing(stack_size).\n\n\n\n\n\n","category":"method"},{"location":"rl_zoo/#ReinforcementLearningZoo.RainbowLearner","page":"RLZoo","title":"ReinforcementLearningZoo.RainbowLearner","text":"RainbowLearner(;kwargs...)\n\nSee paper: Rainbow: Combining Improvements in Deep Reinforcement Learning\n\nKeywords\n\napproximator::AbstractApproximator: used to get Q-values of a state.\ntarget_approximator::AbstractApproximator: similar to approximator, but used to estimate the target (the next state).\nloss_func: the loss function.\nVₘₐₓ::Float32: the maximum value of distribution.\nVₘᵢₙ::Float32: the minimum value of distribution.\nn_actions::Int: number of possible actions.\nγ::Float32=0.99f0: discount rate.\nbatch_size::Int=32\nupdate_horizon::Int=1: length of update ('n' in n-step update).\nmin_replay_history::Int=32: number of transitions that should be experienced before updating the approximator.\nupdate_freq::Int=4: the frequency of updating the approximator.\ntarget_update_freq::Int=500: the frequency of syncing target_approximator.\nstack_size::Union{Int, Nothing}=4: use the recent stack_size frames to form a stacked state.\ndefault_priority::Float32=1.0f2.: the default priority for newly added transitions. It must be >= 1.\nn_atoms::Int=51: the number of buckets of the value function distribution.\nstack_size::Union{Int, Nothing}=4: use the recent stack_size frames to form a stacked state.\nrng = Random.GLOBAL_RNG\n\n\n\n\n\n","category":"type"},{"location":"rl_envs/#ReinforcementLearningEnvironments.jl","page":"RLEnvs","title":"ReinforcementLearningEnvironments.jl","text":"","category":"section"},{"location":"rl_envs/","page":"RLEnvs","title":"RLEnvs","text":"Modules = [ReinforcementLearningEnvironments]","category":"page"},{"location":"rl_envs/#ReinforcementLearningEnvironments.AcrobotEnv-Tuple{}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.AcrobotEnv","text":"AcrobotEnv(;kwargs...)\n\nKeyword arguments\n\nT = Float64\nlink_length_a = T(1.)\nlink_length_b = T(1.)\nlink_mass_a = T(1.)\nlink_mass_b = T(1.)\nlink_com_pos_a = T(0.5)\nlink_com_pos_b = T(0.5)\nlink_moi = T(1.)\nmax_vel_a = T(4 * π)\nmax_vel_b = T(9 * π)\ng = T(9.8)\ndt = T(0.2)\nmax_steps = 200\nbook_or_nips = 'book'\navail_torque = [T(-1.), T(0.), T(1.)]\n\n\n\n\n\n","category":"method"},{"location":"rl_envs/#ReinforcementLearningEnvironments.CartPoleEnv-Tuple{}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.CartPoleEnv","text":"CartPoleEnv(;kwargs...)\n\nKeyword arguments\n\nT = Float64\ngravity = T(9.8)\nmasscart = T(1.0)\nmasspole = T(0.1)\nhalflength = T(0.5)\nforcemag = T(10.0)\nmax_steps = 200\n'dt = 0.02'\nrng = Random.GLOBAL_RNG\n\n\n\n\n\n","category":"method"},{"location":"rl_envs/#ReinforcementLearningEnvironments.MountainCarEnv-Tuple{}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.MountainCarEnv","text":"MountainCarEnv(;kwargs...)\n\nKeyword arguments\n\nT = Float64\ncontinuous = false\nrng = Random.GLOBAL_RNG\nmin_pos = -1.2\nmax_pos = 0.6\nmax_speed = 0.07\ngoal_pos = 0.5\nmax_steps = 200\ngoal_velocity = 0.0\npower = 0.001\ngravity = 0.0025\n\n\n\n\n\n","category":"method"},{"location":"rl_envs/#ReinforcementLearningEnvironments.PendulumEnv-Tuple{}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.PendulumEnv","text":"PendulumEnv(;kwargs...)\n\nKeyword arguments\n\nT = Float64\nmax_speed = T(8)\nmax_torque = T(2)\ng = T(10)\nm = T(1)\nl = T(1)\ndt = T(0.05)\nmax_steps = 200\ncontinuous::Bool = true\nn_actions::Int = 3\nrng = Random.GLOBAL_RNG\n\n\n\n\n\n","category":"method"},{"location":"rl_envs/#ReinforcementLearningEnvironments.PendulumNonInteractiveEnv","page":"RLEnvs","title":"ReinforcementLearningEnvironments.PendulumNonInteractiveEnv","text":"A non-interactive pendulum environment.\n\nAccepts only nothing actions, which result in the system being simulated for one time step. Sets env.done to true once maximum_time is reached. Resets to a random position and momentum. Always returns zero rewards.\n\nUseful for debugging and development purposes, particularly in model-based reinforcement learning.\n\n\n\n\n\n","category":"type"},{"location":"rl_envs/#ReinforcementLearningEnvironments.PendulumNonInteractiveEnv-Tuple{}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.PendulumNonInteractiveEnv","text":"PendulumNonInteractiveEnv(;kwargs...)\n\nKeyword arguments\n\nfloat_type = Float64\ngravity = 9.8\nlength = 2.0\nmass = 1.0\nstep_size = 0.01\nmaximum_time = 10.0\nrng = Random.GLOBAL_RNG\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.jl","page":"RLBase","title":"ReinforcementLearningBase.jl","text":"","category":"section"},{"location":"rl_base/","page":"RLBase","title":"RLBase","text":"Modules = [ReinforcementLearningBase]","category":"page"},{"location":"rl_base/#ReinforcementLearningBase.RLBase","page":"RLBase","title":"ReinforcementLearningBase.RLBase","text":"ReinforcementLearningBase.jl (RLBase) provides some common constants, traits, abstractions and interfaces in developing reinforcement learning algorithms in  Julia. From the concept level, they can be organized in the following parts:\n\nPolicy\nEnvironmentModel\nEnvironment\nTraits for Environment\n\n\n\n\n\n","category":"module"},{"location":"rl_base/#ReinforcementLearningBase.CONSTANT_SUM","page":"RLBase","title":"ReinforcementLearningBase.CONSTANT_SUM","text":"Rewards of all players sum to a constant\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.DETERMINISTIC","page":"RLBase","title":"ReinforcementLearningBase.DETERMINISTIC","text":"No chance player in the environment. And the game is deterministic.\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.EXPLICIT_STOCHASTIC","page":"RLBase","title":"ReinforcementLearningBase.EXPLICIT_STOCHASTIC","text":"Environment contains chance player and the probability is known.\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.FULL_ACTION_SET","page":"RLBase","title":"ReinforcementLearningBase.FULL_ACTION_SET","text":"The action space of the environment may contains illegal actions\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.GENERAL_SUM","page":"RLBase","title":"ReinforcementLearningBase.GENERAL_SUM","text":"Total rewards of all players may be different in each step\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.IDENTICAL_REWARD","page":"RLBase","title":"ReinforcementLearningBase.IDENTICAL_REWARD","text":"Every player gets the same reward\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.IMPERFECT_INFORMATION","page":"RLBase","title":"ReinforcementLearningBase.IMPERFECT_INFORMATION","text":"The inner state of some players' observations may be different\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.MINIMAL_ACTION_SET","page":"RLBase","title":"ReinforcementLearningBase.MINIMAL_ACTION_SET","text":"All actions in the action space of the environment are legal\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.PERFECT_INFORMATION","page":"RLBase","title":"ReinforcementLearningBase.PERFECT_INFORMATION","text":"All players observe the same state\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.SAMPLED_STOCHASTIC","page":"RLBase","title":"ReinforcementLearningBase.SAMPLED_STOCHASTIC","text":"Environment contains chance player and the probability is unknown. Usually only a dummy action is allowed in this case.\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.SEQUENTIAL","page":"RLBase","title":"ReinforcementLearningBase.SEQUENTIAL","text":"Environment with the DynamicStyle of SEQUENTIAL must takes actions from different players one-by-one.\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.SIMULTANEOUS","page":"RLBase","title":"ReinforcementLearningBase.SIMULTANEOUS","text":"Environment with the DynamicStyle of SIMULTANEOUS must take in actions from some (or all) players at one time\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.STEP_REWARD","page":"RLBase","title":"ReinforcementLearningBase.STEP_REWARD","text":"We can get reward after each step\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.STOCHASTIC","page":"RLBase","title":"ReinforcementLearningBase.STOCHASTIC","text":"No chance player in the environment. And the game is stochastic.\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.TERMINAL_REWARD","page":"RLBase","title":"ReinforcementLearningBase.TERMINAL_REWARD","text":"Only get reward at the end of environment\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.ZERO_SUM","page":"RLBase","title":"ReinforcementLearningBase.ZERO_SUM","text":"Rewards of all players sum to 0\n\n\n\n\n\n","category":"constant"},{"location":"rl_base/#ReinforcementLearningBase.AbstractEnv","page":"RLBase","title":"ReinforcementLearningBase.AbstractEnv","text":"(env::AbstractEnv)(action) = env(action, get_current_player(env)) -> env\n(env::AbstractEnv)(action, player) -> env\n\nSuper type of all reinforcement learning environments.\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.AbstractEnvironmentModel","page":"RLBase","title":"ReinforcementLearningBase.AbstractEnvironmentModel","text":"Describe how to model a reinforcement learning environment. TODO: need more investigation Ref: https://bair.berkeley.edu/blog/2019/12/12/mbpo/\n\nAnalytic gradient computation\nSampling-based planning\nModel-based data generation\nValue-equivalence prediction\n\nModel-based Reinforcement Learning: A Survey. Tutorial on Model-Based Methods in Reinforcement Learning\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.AbstractPolicy","page":"RLBase","title":"ReinforcementLearningBase.AbstractPolicy","text":"(π::AbstractPolicy)(env) -> action\n\nPolicy is the most basic concept in reinforcement learning. A policy is a functional object which takes in an environemnt and generate an action.\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.AbstractSpace","page":"RLBase","title":"ReinforcementLearningBase.AbstractSpace","text":"Describe the span of states and actions. Usually the following methods are implemented:\n\nBase.length\nBase.in\nRandom.rand\nBase.eltype\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.ContinuousSpace-Tuple{Any,Any}","page":"RLBase","title":"ReinforcementLearningBase.ContinuousSpace","text":"ContinuousSpace(low, high)\n\nSimilar to DiscreteSpace, but the span is continuous.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.DictSpace-Tuple{Vararg{Pair{#s13,#s12} where #s12<:AbstractSpace where #s13<:Union{AbstractString, Symbol},N} where N}","page":"RLBase","title":"ReinforcementLearningBase.DictSpace","text":"DictSpace(ps::Pair{<:Union{Symbol,AbstractString},<:AbstractSpace}...)\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.DiscreteSpace","page":"RLBase","title":"ReinforcementLearningBase.DiscreteSpace","text":"DiscreteSpace(span)\n\nThe span can be of any iterators.\n\nExample\n\njulia> s = DiscreteSpace([1,2,3])\nDiscreteSpace{Array{Int64,1}}([1, 2, 3])\n\njulia> 0 ∉ s\ntrue\n\njulia> 2 ∈ s\ntrue\n\njulia> s = DiscreteSpace(Set([:a, :c, :a, :b]))\nDiscreteSpace{Set{Symbol}}(Set(Symbol[:a, :b, :c]))\n\njulia> s = DiscreteSpace(3)\nDiscreteSpace{UnitRange{Int64}}(1:3)\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.DiscreteSpace-Tuple{Any,Any}","page":"RLBase","title":"ReinforcementLearningBase.DiscreteSpace","text":"DiscreteSpace(low, high)\n\nCreate a DiscreteSpace with span of low:high\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.DiscreteSpace-Union{Tuple{T}, Tuple{T}} where T<:Integer","page":"RLBase","title":"ReinforcementLearningBase.DiscreteSpace","text":"DiscreteSpace(high::T)\n\nCreate a DiscreteSpace with span of 1:high\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.EmptySpace","page":"RLBase","title":"ReinforcementLearningBase.EmptySpace","text":"EmptySpace()\n\nThere's nothing in the EmptySpace!\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.MultiContinuousSpace-Tuple{Any,Any}","page":"RLBase","title":"ReinforcementLearningBase.MultiContinuousSpace","text":"MultiContinuousSpace(low, high)\n\nSimilar to ContinuousSpace, but scaled to multi-dimension.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.MultiDiscreteSpace","page":"RLBase","title":"ReinforcementLearningBase.MultiDiscreteSpace","text":"MultiDiscreteSpace(low::T, high::T) where {T<:AbstractArray}\n\nSimilar to DiscreteSpace, but scaled to multi-dimension.\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.MultiDiscreteSpace-Union{Tuple{T}, Tuple{T}} where T<:AbstractArray","page":"RLBase","title":"ReinforcementLearningBase.MultiDiscreteSpace","text":"MultiDiscreteSpace(high::T) where {T<:AbstractArray}\n\nThe low will fall back to ones(eltype(T), size(high)).\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.MultiThreadEnv","page":"RLBase","title":"ReinforcementLearningBase.MultiThreadEnv","text":"MultiThreadEnv(envs::Vector{<:AbstractEnv})\n\nWrap multiple environments into one environment. Each environment will run in parallel by leveraging Threads.@spawn. So remember to set the environment variable JULIA_NUM_THREADS!\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.RandomPolicy","page":"RLBase","title":"ReinforcementLearningBase.RandomPolicy","text":"RandomPolicy(action_space, rng=Random.GLOBAL_RNG)\n\nConstruct a random policy with actions in action_space. If action_space is nothing then the legal_actions at runtime will be used to randomly sample a valid action.\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#ReinforcementLearningBase.RandomPolicy-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.RandomPolicy","text":"RandomPolicy(env::AbstractEnv; rng=Random.GLOBAL_RNG)\n\nIf env is of FULL_ACTION_SET, then action is randomly chosen at runtime in get_actions(env). Otherwise, the env is supposed to be of MINIMAL_ACTION_SET. The get_actions(env) is supposed to be static and will only be used to initialize the random policy for once.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.StateCachedEnv","page":"RLBase","title":"ReinforcementLearningBase.StateCachedEnv","text":"Cache the state so that get_state(env) will always return the same result before the next interaction with env.\n\n\n\n\n\n","category":"type"},{"location":"rl_base/#Base.copy-Tuple{AbstractEnv}","page":"RLBase","title":"Base.copy","text":"Make an independent copy of env\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#Base.run-Tuple{Any,AbstractEnv}","page":"RLBase","title":"Base.run","text":"run(π, env::AbstractEnv)\n\nRun the policy π in env until the end.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#Random.seed!-Tuple{AbstractEnv,Any}","page":"RLBase","title":"Random.seed!","text":"Set the seed of internal rng\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.ActionStyle-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.ActionStyle","text":"ActionStyle(env::AbstractEnv)\n\nSpecify whether the current state of env contains a full action set or a minimal action set. By default the MINIMAL_ACTION_SET is returned.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.ChanceStyle-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.ChanceStyle","text":"ChanceStyle(env) = DETERMINISTIC\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.DynamicStyle-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.DynamicStyle","text":"DynamicStyle(env::AbstractEnv) = SEQUENTIAL\n\nDetermine whether the players can play simultaneously or not. Default value is SEQUENTIAL\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.InformationStyle-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.InformationStyle","text":"InformationStyle(env) = PERFECT_INFORMATION\n\nSpecify whether the env is PERFECT_INFORMATION or IMPERFECT_INFORMATION. Return PERFECT_INFORMATION by default.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.NumAgentStyle-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.NumAgentStyle","text":"NumAgentStyle(env)\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.RewardStyle-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.RewardStyle","text":"Specify whether we can get reward after each step or only at the end of an game. Possible values are STEP_REWARD or TERMINAL_REWARD\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.UtilityStyle-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.UtilityStyle","text":"UtilityStyle(env::AbstractEnv)\n\nSpecify the utility style in multi-agent environments. Possible values are:\n\nZERO_SUM\nCONSTANT_SUM\nGENERAL_SUM\nIDENTICAL_REWARD\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.child-Tuple{AbstractEnv,Any}","page":"RLBase","title":"ReinforcementLearningBase.child","text":"child(env::AbstractEnv, action)\n\nTreat the env as a game tree. Create an independent child after applying action.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.get_actions","page":"RLBase","title":"ReinforcementLearningBase.get_actions","text":"get_actions(env, player=get_current_player(env))\n\nGet all available actions from environment. See also: get_legal_actions\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.get_chance_player-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.get_chance_player","text":"get_chance_player(env)\n\nOnly valid for environments with a chance player.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.get_current_player-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.get_current_player","text":"get_current_player(env)\n\nReturn the next player to take action. For Extensive Form Games, A chance player may be returned. (See also get_chance_player) For SIMULTANEOUS environments, a simultaneous player may be returned. (See also get_simultaneouse_player).\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.get_history","page":"RLBase","title":"ReinforcementLearningBase.get_history","text":"Get all actions in each ply\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.get_legal_actions","page":"RLBase","title":"ReinforcementLearningBase.get_legal_actions","text":"get_legal_actions(env, player=get_current_player(env))\n\nOnly valid for environments of FULL_ACTION_SET.\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.get_legal_actions_mask","page":"RLBase","title":"ReinforcementLearningBase.get_legal_actions_mask","text":"get_legal_actions_mask(env, player=get_current_player(env)) -> AbstractArray{Bool}\n\nRequired for environments of FULL_ACTION_SET.\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.get_priority-Tuple{AbstractPolicy,Any}","page":"RLBase","title":"ReinforcementLearningBase.get_priority","text":"get_priority(π::AbstractPolicy, experience)\n\nUsually used in offline policies.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.get_prob","page":"RLBase","title":"ReinforcementLearningBase.get_prob","text":"get_prob(env, player=get_chance_player(env))\n\nOnly valid for environments of EXPLICIT_STOCHASTIC style. Here player must be a chance player.\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.get_prob-Tuple{AbstractPolicy,Any,Any}","page":"RLBase","title":"ReinforcementLearningBase.get_prob","text":"get_prob(π::AbstractPolicy, env, action)\n\nOnly valid for environments with discrete action space.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.get_prob-Tuple{AbstractPolicy,Any}","page":"RLBase","title":"ReinforcementLearningBase.get_prob","text":"get_prob(π::AbstractPolicy, env)\n\nGet the probability distribution of actions based on policy π given an env. \n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.get_reward","page":"RLBase","title":"ReinforcementLearningBase.get_reward","text":"get_reward(env, player=get_current_player(env))\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.get_simultaneouse_player-Tuple{Any}","page":"RLBase","title":"ReinforcementLearningBase.get_simultaneouse_player","text":"get_simultaneouse_player(env)\n\nOnly valid for environments of SIMULTANEOUS style.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.get_spectator_player-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.get_spectator_player","text":"get_spectator_player(env)\n\nUsed in imperfect multi-agent environments.\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.get_state","page":"RLBase","title":"ReinforcementLearningBase.get_state","text":"get_state([t::Type], env, player=get_current_player(env)) -> state\n\nThe state can be of any type. However, most neural network based algorithms assume it's an AbstractArray. For environments with many different states provided (inner state, information state, etc), users need to provide t::Type to declare which kind of state they want.\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.get_terminal","page":"RLBase","title":"ReinforcementLearningBase.get_terminal","text":"get_terminal(env, player=get_current_player(env))\n\n\n\n\n\n","category":"function"},{"location":"rl_base/#ReinforcementLearningBase.reset!-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.reset!","text":"Reset the internal state of an environment\n\n\n\n\n\n","category":"method"},{"location":"rl_base/#ReinforcementLearningBase.update!-Tuple{AbstractPolicy,Any}","page":"RLBase","title":"ReinforcementLearningBase.update!","text":"update!(π::AbstractPolicy, experience)\n\nUpdate the policy π with online/offline experience.\n\n\n\n\n\n","category":"method"},{"location":"","page":"Home","title":"Home","text":"<div align=\"center\">\n  <p>\n  <img src=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/raw/master/docs/src/assets/logo.svg?sanitize=true\" width=\"320px\">\n  </p>\n</div>","category":"page"},{"location":"","page":"Home","title":"Home","text":"ReinforcementLearning.jl, as the name says, is a package for reinforcement learning research in Julia.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Our design principles are:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Reusability and extensibility: Provide elaborately designed components and interfaces to help users implement new algorithms.\nEasy experimentation: Make it easy for new users to run benchmark experiments, compare different algorithms, evaluate and diagnose agents.\nReproducibility: Facilitate reproducibility from traditional tabular methods to modern deep reinforcement learning algorithms.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package can be installed from the package manager in Julia's REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"] add ReinforcementLearning","category":"page"},{"location":"#Resources","page":"Home","title":"Resources","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Get Started in 3 lines!\nGuide\nBlog\nAn Introduction to ReinforcementLearning.jl: Design, Implementation & Thoughts\nManual\nReinforcementLearningBase.jl\nReinforcementLearningCore.jl\nReinforcementLearningEnvironments.jl\nReinforcementLearningZoo.jl","category":"page"},{"location":"#Project-Structure","page":"Home","title":"Project Structure","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ReinforcementLearning.jl itself is just a wrapper around several other packages inside the JuliaReinforcementLearning org. The relationship between different packages is described below:","category":"page"},{"location":"","page":"Home","title":"Home","text":"<pre>+-------------------------------------------------------------------------------------------+\n|                                                                                           |\n|  <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl\">ReinforcementLearning.jl</a>                                                                 |\n|                                                                                           |\n|      +------------------------------+                                                     |\n|      | <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearningBase.jl\">ReinforcementLearningBase.jl</a> |                                                     |\n|      +--------|---------------------+                                                     |\n|               |                                                                           |\n|               |         +--------------------------------------+                          |\n|               |         | <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironments.jl\">ReinforcementLearningEnvironments.jl</a> |                          |\n|               |         |                                      |                          |\n|               |         |     (Conditionally depends on)       |                          |\n|               |         |                                      |                          |\n|               |         |     <a href=\"https://github.com/JuliaReinforcementLearning/ArcadeLearningEnvironment.jl\">ArcadeLearningEnvironment.jl</a>     |                          |\n|               +--------&gt;+     <a href=\"https://github.com/JuliaReinforcementLearning/OpenSpiel.jl\">OpenSpiel.jl</a>                     |                          |\n|               |         |     <a href=\"https://github.com/JuliaPOMDP/POMDPs.jl\">POMDPs.jl</a>                        |                          |\n|               |         |     <a href=\"https://github.com/JuliaPy/PyCall.jl\">PyCall.jl</a>                        |                          |\n|               |         |     <a href=\"https://github.com/JuliaReinforcementLearning/ViZDoom.jl\">ViZDoom.jl</a>                       |                          |\n|               |         |     Maze.jl(WIP)                     |                          |\n|               |         +--------------------------------------+                          |\n|               |                                                                           |\n|               |         +------------------------------+                                  |\n|               +--------&gt;+ <a href=\"\">ReinforcementLearningCore.jl</a> |                                  |\n|                         +--------|---------------------+                                  |\n|                                  |                                                        |\n|                                  |          +-----------------------------+               |\n|                                  |---------&gt;+ <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearningZoo.jl\">ReinforcementLearningZoo.jl</a> |               |\n|                                  |          +-----------------------------+               |\n|                                  |                                                        |\n|                                  |          +----------------------------------------+    |\n|                                  +---------&gt;+ <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearningAnIntroduction.jl\">ReinforcementLearningAnIntroduction.jl</a> |    |\n|                                             +----------------------------------------+    |\n+-------------------------------------------------------------------------------------------+\n</pre>","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nReinforcementLearningAnIntroduction.jl contains some traditional reinforcement algorithms and it is not registered yet. So it is not included in ReinforcementLearning.jl. The reason to do so is to ease the burden of maintenance.","category":"page"},{"location":"rl_core/#ReinforcementLearningCore.jl","page":"RLCore","title":"ReinforcementLearningCore.jl","text":"","category":"section"},{"location":"rl_core/","page":"RLCore","title":"RLCore","text":"Modules = [ReinforcementLearningCore]","category":"page"},{"location":"rl_core/#ReinforcementLearningCore.RLCore","page":"RLCore","title":"ReinforcementLearningCore.RLCore","text":"ReinforcementLearningCore.jl (RLCore) provides some standard and reusable components defined by RLBase, hoping that they are useful for people to implement and experiment with different kinds of algorithms.\n\n\n\n\n\n","category":"module"},{"location":"rl_core/#ReinforcementLearningCore.RTSA","page":"RLCore","title":"ReinforcementLearningCore.RTSA","text":"An alias of (:reward, :terminal, :state, :action)\n\n\n\n\n\n","category":"constant"},{"location":"rl_core/#ReinforcementLearningCore.SARTSA","page":"RLCore","title":"ReinforcementLearningCore.SARTSA","text":"An alias of (:state, :action, :reward, :terminal, :next_state, :next_action)\n\n\n\n\n\n","category":"constant"},{"location":"rl_core/#ReinforcementLearningCore.AbstractAgent","page":"RLCore","title":"ReinforcementLearningCore.AbstractAgent","text":"(agent::AbstractAgent)(env) = agent(PRE_ACT_STAGE, env) -> action\n(agent::AbstractAgent)(stage::AbstractStage, env)\n\nSimilar to AbstractPolicy, an agent is also a functional object which takes in an observation and returns an action. The main difference is that, we divide an experiment into the following stages:\n\nPRE_EXPERIMENT_STAGE\nPRE_EPISODE_STAGE\nPRE_ACT_STAGE\nPOST_ACT_STAGE\nPOST_EPISODE_STAGE\nPOST_EXPERIMENT_STAGE\n\nIn each stage, different types of agents may have different behaviors, like updating experience buffer, environment model or policy.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.AbstractApproximator","page":"RLCore","title":"ReinforcementLearningCore.AbstractApproximator","text":"(app::AbstractApproximator)(env)\n\nAn approximator is a functional object for value estimation. It serves as a black box to provides an abstraction over different  kinds of approximate methods (for example DNN provided by Flux or Knet).\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.AbstractExplorer","page":"RLCore","title":"ReinforcementLearningCore.AbstractExplorer","text":"(p::AbstractExplorer)(x)\n(p::AbstractExplorer)(x, mask)\n\nDefine how to select an action based on action values.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.AbstractHook","page":"RLCore","title":"ReinforcementLearningCore.AbstractHook","text":"A hook is called at different stage duiring a run to allow users to inject customized runtime logic. By default, a AbstractHook will do nothing. One can override the behavior by implementing the following methods:\n\n(hook::YourHook)(::PreActStage, agent, env, action), note that there's an extra argument of action.\n(hook::YourHook)(::PostActStage, agent, env)\n(hook::YourHook)(::PreEpisodeStage, agent, env)\n(hook::YourHook)(::PostEpisodeStage, agent, env)\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.AbstractLearner","page":"RLCore","title":"ReinforcementLearningCore.AbstractLearner","text":"(learner::AbstractLearner)(env)\n\nA learner is usually used to estimate state values, state-action values or distributional values based on experiences.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.AbstractStage","page":"RLCore","title":"ReinforcementLearningCore.AbstractStage","text":"                  +-----------------------------------------------------------+                      \n                  |Episode                                                    |                      \n                  |                                                           |\n\nPREEXPERIMENTSTAGE  |            PREACTSTAGE    POSTACTSTAGE                | POSTEXPERIMENTSTAGE          |            |                  |                |                       |          |                     v            |        +––-+   v   +–––-+    v   +––-+             |          v                     ––––––––––->+ env +–––>+ agent +–––->+ env +–-> ... –––->......                                    |  ^     +––-+       +–––-+ action +––-+          ^  |                                             |  |                                                     |  |                                             |  +–PREEPISODESTAGE            POSTEPISODESTAGE––+  |                                             |                                                           |                                             |                                                           |                                             +–––––––––––––––––––––––––––––-+     \n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.AbstractTrajectory","page":"RLCore","title":"ReinforcementLearningCore.AbstractTrajectory","text":"AbstractTrajectory{names,types} <: AbstractArray{NamedTuple{names,types},1}\n\nA trajectory is used to record some useful information during the interactions between agents and environments.\n\nParameters\n\nnames::NTuple{Symbol}, indicate what fields to be recorded.\ntypes::Tuple{DataType...}, the datatypes of names.\n\nThe length of names and types must match.\n\nRequired Methods:\n\nget_trace\nBase.push!(t::AbstractTrajectory, kv::Pair{Symbol})\nBase.pop!(t::AbstractTrajectory, s::Symbol)\n\nOptional Methods:\n\nBase.length\nBase.size\nBase.lastindex\nBase.isempty\nBase.empty!\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.ActorCritic","page":"RLCore","title":"ReinforcementLearningCore.ActorCritic","text":"ActorCritic(;actor, critic, optimizer=ADAM())\n\nThe actor part must return logits (Do not use softmax in the last layer!), and the critic part must return a state value.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.Agent","page":"RLCore","title":"ReinforcementLearningCore.Agent","text":"Agent(;kwargs...)\n\nOne of the most commonly used AbstractAgent.\n\nGenerally speaking, it does nothing but update the trajectory and policy appropriately in different stages.\n\nKeywords & Fields\n\npolicy::AbstractPolicy: the policy to use\ntrajectory::AbstractTrajectory: used to store transitions between an agent and an environment\nrole=RLBase.DEFAULT_PLAYER: used to distinguish different agents\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.BatchExplorer","page":"RLCore","title":"ReinforcementLearningCore.BatchExplorer","text":"BatchExplorer(explorer::AbstractExplorer)\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.BatchExplorer-Tuple{AbstractArray{T,2} where T}","page":"RLCore","title":"ReinforcementLearningCore.BatchExplorer","text":"(x::BatchExplorer)(values::AbstractMatrix)\n\nApply inner explorer to each column of values.\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.BatchStepsPerEpisode-Tuple{Int64}","page":"RLCore","title":"ReinforcementLearningCore.BatchStepsPerEpisode","text":"BatchStepsPerEpisode(batch_size::Int; tag = \"TRAINING\")\n\nSimilar to StepsPerEpisode, but only work for MultiThreadEnv\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.CircularArrayBuffer","page":"RLCore","title":"ReinforcementLearningCore.CircularArrayBuffer","text":"CircularArrayBuffer{T}(d::Integer...) -> CircularArrayBuffer{T, N}\n\nCircularArrayBuffer uses a N-dimention Array of size d to serve as a buffer for N-1-dimention Arrays with the same size.\n\nExamples\n\njulia> b = CircularArrayBuffer{Float64}(2, 2, 3)\n2×2×0 CircularArrayBuffer{Float64,3}\n\njulia> capacity(b)\n3\n\njulia> length(b)\n0\n\njulia> push!(b, [1. 1.; 2. 2.])\n2×2×1 CircularArrayBuffer{Float64,3}:\n[:, :, 1] =\n 1.0  1.0\n 2.0  2.0\n\njulia> b\n2×2×1 CircularArrayBuffer{Float64,3}:\n[:, :, 1] =\n 1.0  1.0\n 2.0  2.0\n\njulia> length(b)\n4\n\njulia> nframes(cb::CircularArrayBuffer) = cb.length\nnframes (generic function with 1 method)\n\njulia> nframes(b)\n1\n\njulia> ones(2,2)\n2×2 Array{Float64,2}:\n 1.0  1.0\n 1.0  1.0\n\njulia> 3 .* ones(2,2)\n2×2 Array{Float64,2}:\n 3.0  3.0\n 3.0  3.0\n\njulia> 3 * ones(2,2)\n2×2 Array{Float64,2}:\n 3.0  3.0\n 3.0  3.0\n\njulia> b = CircularArrayBuffer{Float64}(2, 2, 3)\n2×2×0 CircularArrayBuffer{Float64,3}\n\njulia> capacity(b)\n3\n\njulia> nframes(b)\n0\n\njulia> push!(b, 1 .* ones(2,2))\n2×2×1 CircularArrayBuffer{Float64,3}:\n[:, :, 1] =\n 1.0  1.0\n 1.0  1.0\n\njulia> b\n2×2×1 CircularArrayBuffer{Float64,3}:\n[:, :, 1] =\n 1.0  1.0\n 1.0  1.0\n\njulia> nframes(b)\n1\n\njulia> for i in 2:4\n           push!(b, i .* ones(2,2))\n       end\n\njulia> b\n2×2×3 CircularArrayBuffer{Float64,3}:\n[:, :, 1] =\n 2.0  2.0\n 2.0  2.0\n\n[:, :, 2] =\n 3.0  3.0\n 3.0  3.0\n\n[:, :, 3] =\n 4.0  4.0\n 4.0  4.0\n\njulia> isfull(b)\ntrue\n\njulia> nframes(b)\n3\n\njulia> size(b)\n(2, 2, 3)\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.CircularCompactPSARTSATrajectory-Tuple{}","page":"RLCore","title":"ReinforcementLearningCore.CircularCompactPSARTSATrajectory","text":"CircularCompactPSARTSATrajectory(;kwargs)\n\nSimilar to CircularCompactSARTSATrajectory, except that another trace named priority is added.\n\nKey word arguments\n\ncapacity::Int, the maximum length of each trace.\nstate_type = Int\nstate_size = ()\naction_type = Int\naction_size = ()\nreward_type = Float32\nreward_size = ()\nterminal_type = Bool\nterminal_size = ()\npriority_type = Float32\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.CircularCompactSARTSATrajectory-Tuple{}","page":"RLCore","title":"ReinforcementLearningCore.CircularCompactSARTSATrajectory","text":"CircularCompactSARTSATrajectory(;kwargs...)\n\nSimilar to VectorialCompactSARTSATrajectory, instead of using Vectors as containers, CircularArrayBuffers are used here.\n\nKey word arguments\n\ncapacity::Int, the maximum length of each trace.\nstate_type = Int\nstate_size = ()\naction_type = Int\naction_size = ()\nreward_type = Float32\nreward_size = ()\nterminal_type = Bool\nterminal_size = ()\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.CircularTrajectory-Tuple{}","page":"RLCore","title":"ReinforcementLearningCore.CircularTrajectory","text":"CircularTrajectory(; capacity, trace_name=eltype=>size...)\n\nSimilar to VectorialTrajectory, but we use the CircularArrayBuffer to store the traces. The capacity here is used to specify the maximum length of the trajectory.\n\nExample\n\njulia> t = CircularTrajectory(capacity=10, state=Float64=>(3,3), reward=Int=>tuple())\n0-element Trajectory{(:state, :reward),Tuple{Float64,Int64},NamedTuple{(:state, :reward),Tuple{CircularArrayBuffer{Float64,3},CircularArrayBuffer{Int64,1}}}}\n\njulia> push!(t,state=rand(3,3), reward=1)\n\njulia> push!(t,state=rand(3,3), reward=2)\n\njulia> get_trace(t, :reward)\n2-element CircularArrayBuffer{Int64,1}:\n 1\n 2\n\njulia> get_trace(t, :state)\n3×3×2 CircularArrayBuffer{Float64,3}:\n[:, :, 1] =\n 0.699906  0.382396   0.927411\n 0.269807  0.0581324  0.239609\n 0.222304  0.514408   0.318905\n\n[:, :, 2] =\n 0.956228  0.992505  0.109743\n 0.763497  0.381387  0.540566\n 0.223081  0.834308  0.634759\n\njulia> pop!(t)\n\njulia> get_trace(t, :state)\n3×3×1 CircularArrayBuffer{Float64,3}:\n[:, :, 1] =\n 0.699906  0.382396   0.927411\n 0.269807  0.0581324  0.239609\n 0.222304  0.514408   0.318905\n\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.ComposedHook","page":"RLCore","title":"ReinforcementLearningCore.ComposedHook","text":"ComposedHook(hooks::AbstractHook...)\n\nCompose different hooks into a single hook.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.ComposedStopCondition","page":"RLCore","title":"ReinforcementLearningCore.ComposedStopCondition","text":"ComposedStopCondition(stop_conditions; reducer = any)\n\nThe result of stop_conditions is reduced by reducer.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.CumulativeReward","page":"RLCore","title":"ReinforcementLearningCore.CumulativeReward","text":"CumulativeReward(rewards::Vector{Float64} = [0.0])\n\nStore cumulative rewards since the beginning to the field of rewards.\n\nnote: Note\nIf the environment is a RewardOverriddenEnv, then the original reward is recorded instead.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.DoEveryNEpisode","page":"RLCore","title":"ReinforcementLearningCore.DoEveryNEpisode","text":"DoEveryNEpisode(f; n=1, t=0)\n\nExecute f(agent, env) every n episode. t is a counter of steps.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.DoEveryNStep","page":"RLCore","title":"ReinforcementLearningCore.DoEveryNStep","text":"DoEveryNStep(f; n=1, t=0)\n\nExecute f(agent, env) every n step. t is a counter of steps.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.DynaAgent","page":"RLCore","title":"ReinforcementLearningCore.DynaAgent","text":"DynaAgent(;kwargs...)\n\nDynaAgent is first introduced in: Sutton, Richard S. \"Dyna, an integrated architecture for learning, planning, and reacting.\" ACM Sigart Bulletin 2.4 (1991): 160-163.\n\nKeywords & Fields\n\npolicy::AbstractPolicy: the policy to use\nmodel::AbstractEnvironmentModel: describe the environment to interact with\ntrajectory::AbstractTrajectory: used to store transitions between agent and environment\nrole=:DEFAULT: used to distinguish different agents\nplan_step::Int=10: the count of planning steps\n\nThe main difference between DynaAgent and Agent is that an environment model is involved. It is best described in the book: Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\n(Image: ) (Image: )\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.EmptyHook","page":"RLCore","title":"ReinforcementLearningCore.EmptyHook","text":"Do nothing\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.EpisodicCompactSARTSATrajectory","page":"RLCore","title":"ReinforcementLearningCore.EpisodicCompactSARTSATrajectory","text":"EpisodicCompactSARTSATrajectory(; state_type = Int, action_type = Int, reward_type = Float32, terminal_type = Bool)\n\nExactly the same with VectorialCompactSARTSATrajectory. It only exists for multiple dispatch purpose.\n\nwarning: Warning\nThe EpisodicCompactSARTSATrajectory will not be automatically emptified when reaching the end of an episode.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.EpsilonGreedyExplorer","page":"RLCore","title":"ReinforcementLearningCore.EpsilonGreedyExplorer","text":"EpsilonGreedyExplorer{T}(;kwargs...)\nEpsilonGreedyExplorer(ϵ) -> EpsilonGreedyExplorer{:linear}(; ϵ_stable = ϵ)\n\nEpsilon-greedy strategy: The best lever is selected for a proportion 1 - epsilon of the trials, and a lever is selected at random (with uniform probability) for a proportion epsilon . Multi-armed_bandit\n\nTwo kinds of epsilon-decreasing strategy are implmented here (linear and exp).\n\nEpsilon-decreasing strategy: Similar to the epsilon-greedy strategy, except that the value of epsilon decreases as the experiment progresses, resulting in highly explorative behaviour at the start and highly exploitative behaviour at the finish.  - Multi-armed_bandit\n\nKeywords\n\nT::Symbol: defines how to calculate the epsilon in the warmup steps. Supported values are linear and exp.\nstep::Int = 1: record the current step.\nϵ_init::Float64 = 1.0: initial epsilon.\nwarmup_steps::Int=0: the number of steps to use ϵ_init.\ndecay_steps::Int=0: the number of steps for epsilon to decay from ϵ_init to ϵ_stable.\nϵ_stable::Float64: the epsilon after warmup_steps + decay_steps.\nis_break_tie=false: randomly select an action of the same maximum values if set to true.\nrng=Random.GLOBAL_RNG: set the internal RNG.\nis_training=true, in training mode, step will not be updated. And the ϵ will be set to 0.\n\nExample\n\ns = EpsilonGreedyExplorer{:linear}(ϵ_init=0.9, ϵ_stable=0.1, warmup_steps=100, decay_steps=100)\nplot([RL.get_ϵ(s, i) for i in 1:500], label=\"linear epsilon\")\n\n(Image: )\n\ns = EpsilonGreedyExplorer{:exp}(ϵ_init=0.9, ϵ_stable=0.1, warmup_steps=100, decay_steps=100)\nplot([RL.get_ϵ(s, i) for i in 1:500], label=\"exp epsilon\")\n\n(Image: )\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.EpsilonGreedyExplorer-Tuple{Any}","page":"RLCore","title":"ReinforcementLearningCore.EpsilonGreedyExplorer","text":"(s::EpsilonGreedyExplorer)(values; step) where T\n\nnote: Note\nIf multiple values with the same maximum value are found. Then a random one will be returned!NaN will be filtered unless all the values are NaN. In that case, a random one will be returned.\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.NeuralNetworkApproximator","page":"RLCore","title":"ReinforcementLearningCore.NeuralNetworkApproximator","text":"NeuralNetworkApproximator(;kwargs)\n\nUse a DNN model for value estimation.\n\nKeyword arguments\n\nmodel, a Flux based DNN model.\noptimizer=nothing\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.OffPolicy","page":"RLCore","title":"ReinforcementLearningCore.OffPolicy","text":"OffPolicy(π_target::P, π_behavior::B) -> OffPolicy{P,B}\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.QBasedPolicy","page":"RLCore","title":"ReinforcementLearningCore.QBasedPolicy","text":"QBasedPolicy(;learner::Q, explorer::S)\n\nUse a Q-learner to generate estimations of action values. Then an explorer is applied on the estimations to select an action.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.ResizeImage","page":"RLCore","title":"ReinforcementLearningCore.ResizeImage","text":"ResizeImage(img::Array{T, N})\nResizeImage(dims::Int...) -> ResizeImage(Float32, dims...)\nResizeImage(T::Type{<:Number}, dims::Int...)\n\nUsing BSpline method to resize the state field of an observation to size of img (or dims).\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.RewardsPerEpisode","page":"RLCore","title":"ReinforcementLearningCore.RewardsPerEpisode","text":"RewardsPerEpisode(; rewards = Vector{Vector{Float64}}())\n\nStore each reward of each step in every episode in the field of rewards.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.StackFrames","page":"RLCore","title":"ReinforcementLearningCore.StackFrames","text":"StackFrames(::Type{T}=Float32, d::Int...)\n\nUse a pre-initialized CircularArrayBuffer to store the latest several states specified by d. Before processing any observation, the buffer is filled with zero{T}.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.StepsPerEpisode","page":"RLCore","title":"ReinforcementLearningCore.StepsPerEpisode","text":"StepsPerEpisode(; steps = Int[], count = 0)\n\nStore steps of each episode in the field of steps.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.StopAfterEpisode","page":"RLCore","title":"ReinforcementLearningCore.StopAfterEpisode","text":"StopAfterEpisode(episode; cur = 0, is_show_progress = true)\n\nReturn true after being called episode. If is_show_progress is true, the ProgressMeter will be used to show progress.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.StopAfterStep","page":"RLCore","title":"ReinforcementLearningCore.StopAfterStep","text":"StopAfterStep(step; cur = 1, is_show_progress = true)\n\nReturn true after being called step times.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.StopWhenDone","page":"RLCore","title":"ReinforcementLearningCore.StopWhenDone","text":"StopWhenDone()\n\nReturn true if the environment is terminated.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.SumTree","page":"RLCore","title":"ReinforcementLearningCore.SumTree","text":"SumTree(capacity::Int)\n\nEfficiently sample and update weights. For more detals, see the post at here. Here we use a vector to represent the binary tree. Suppose we will have capacity leaves at most. Every time we push! new node into the tree, only the recent capacity node and their sum will be updated! [––––––Parent nodes––––––][––––leaves––––] [size: 2^ceil(Int, log2(capacity))-1 ][     size: capacity   ]\n\nExample\n\njulia> t = SumTree(8)\n0-element SumTree\njulia> for i in 1:16\n       push!(t, i)\n       end\njulia> t\n8-element SumTree:\n  9.0\n 10.0\n 11.0\n 12.0\n 13.0\n 14.0\n 15.0\n 16.0\njulia> sample(t)\n(2, 10.0)\njulia> sample(t)\n(1, 9.0)\njulia> inds, ps = sample(t,100000)\n([8, 4, 8, 1, 5, 2, 2, 7, 6, 6  …  1, 1, 7, 1, 6, 1, 5, 7, 2, 7], [16.0, 12.0, 16.0, 9.0, 13.0, 10.0, 10.0, 15.0, 14.0, 14.0  …  9.0, 9.0, 15.0, 9.0, 14.0, 9.0, 13.0, 15.0, 10.0, 15.0])\njulia> countmap(inds)\nDict{Int64,Int64} with 8 entries:\n  7 => 14991\n  4 => 12019\n  2 => 10003\n  3 => 11027\n  5 => 12971\n  8 => 16052\n  6 => 13952\n  1 => 8985\njulia> countmap(ps)\nDict{Float64,Int64} with 8 entries:\n  9.0  => 8985\n  13.0 => 12971\n  10.0 => 10003\n  14.0 => 13952\n  16.0 => 16052\n  11.0 => 11027\n  15.0 => 14991\n  12.0 => 12019\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.TabularApproximator","page":"RLCore","title":"ReinforcementLearningCore.TabularApproximator","text":"TabularApproximator(table<:AbstractArray)\n\nFor table of 1-d, it will serve as a state value approximator. For table of 2-d, it will serve as a state-action value approximator.\n\nwarning: Warning\nFor table of 2-d, the first dimension is action and the second dimension is state.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.TabularApproximator-Tuple{}","page":"RLCore","title":"ReinforcementLearningCore.TabularApproximator","text":"TabularApproximator(; n_state, n_action = nothing, init = 0.0)\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.TimePerStep","page":"RLCore","title":"ReinforcementLearningCore.TimePerStep","text":"TimePerStep(;max_steps=100)\nTimePerStep(times::CircularArrayBuffer{Float64}, t::UInt64)\n\nStore time cost of the latest max_steps in the times field.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.TotalBatchRewardPerEpisode-Tuple{Int64}","page":"RLCore","title":"ReinforcementLearningCore.TotalBatchRewardPerEpisode","text":"TotalBatchRewardPerEpisode(batch_size::Int)\n\nSimilar to TotalRewardPerEpisode, but will record total rewards per episode in MultiThreadEnv.\n\nnote: Note\nIf the environment is a RewardOverriddenEnv, then the original reward is recorded.\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.TotalRewardPerEpisode","page":"RLCore","title":"ReinforcementLearningCore.TotalRewardPerEpisode","text":"TotalRewardPerEpisode(; rewards = Float64[], reward = 0.0)\n\nStore the total rewards of each episode in the field of rewards.\n\nnote: Note\nIf the environment is a RewardOverriddenenv, then the original reward is recorded.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.Trajectory","page":"RLCore","title":"ReinforcementLearningCore.Trajectory","text":"Trajectory{names,types,Tbs}(trajectories::Tbs)\n\nA container of different trajectories. Usually you won't use it directly.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.UCBExplorer-Tuple{AbstractArray}","page":"RLCore","title":"ReinforcementLearningCore.UCBExplorer","text":"(ucb::UCBExplorer)(values::AbstractArray)\n\nUnlike EpsilonGreedyExplorer, uncertaintyies are considered in UCB.\n\nnote: Note\nIf multiple values with the same maximum value are found. Then a random one will be returned!\n\nA_t = undersetaarg max left Q_t(a) + c sqrtfracln tN_t(a) right\n\nSee more details at Section (2.7) on Page 35 of the book Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.UCBExplorer-Tuple{Any}","page":"RLCore","title":"ReinforcementLearningCore.UCBExplorer","text":"UCBExplorer(na; c=2.0, ϵ=1e-10, step=1, seed=nothing)\n\nArguments\n\nna is the number of actions used to create a internal counter.\nt is used to store current time step.\nc is used to control the degree of exploration.\nseed, set the seed of inner RNG.\nis_training=true, in training mode, time step and counter will not be updated.\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.VBasedPolicy","page":"RLCore","title":"ReinforcementLearningCore.VBasedPolicy","text":"VBasedPolicy(;learner, mapping, explorer=GreedyExplorer())\n\nKey words & Fields\n\nlearner::AbstractLearner, learn how to estimate state values.\nmapping, a customized function (env, learner) -> action_values\nexplorer::AbstractExplorer, decide which action to take based on action values.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#ReinforcementLearningCore.VectorialCompactSARTSATrajectory-Tuple{}","page":"RLCore","title":"ReinforcementLearningCore.VectorialCompactSARTSATrajectory","text":"VectorialCompactSARTSATrajectory(; state_type = Int, action_type = Int, reward_type = Float32, terminal_type = Bool)\n\nThis function creates a VectorialTrajectory of RTSA fields. Here the Compact in the function name means that, state and next_state, action and next_action reuse a same vector underlying.\n\nExample\n\njulia> t = VectorialCompactSARTSATrajectory()\n0-element Trajectory{(:state, :action, :reward, :terminal, :next_state, :next_action),Tuple{Int64,Int64,Float32,Bool,Int64,Int64},NamedTuple{(:reward, :terminal, :state, :action),Tuple{Array{Float32,1},Array{Bool,1},Array{Int64,1},Array{Int64,1}}}}\n\njulia> push!(t, state=0, action=0)\n\njulia> push!(t, reward=0.f0, terminal=false, state=1, action=1)\n\njulia> t\n1-element Trajectory{(:state, :action, :reward, :terminal, :next_state, :next_action),Tuple{Int64,Int64,Float32,Bool,Int64,Int64},NamedTuple{(:reward, :terminal, :state, :action),Tuple{Array{Float32,1},Array{Bool,1},Array{Int64,1},Array{Int64,1}}}}:\n (state = 0, action = 0, reward = 0.0, terminal = 0, next_state = 1, next_action = 1)\n\njulia> push!(t, reward=1.f0, terminal=true, state=2, action=2)\n\njulia> t\n2-element Trajectory{(:state, :action, :reward, :terminal, :next_state, :next_action),Tuple{Int64,Int64,Float32,Bool,Int64,Int64},NamedTuple{(:reward, :terminal, :state, :action),Tuple{Array{Float32,1},Array{Bool,1},Array{Int64,1},Array{Int64,1}}}}:\n (state = 0, action = 0, reward = 0.0, terminal = 0, next_state = 1, next_action = 1)\n (state = 1, action = 1, reward = 1.0, terminal = 1, next_state = 2, next_action = 2)\n\njulia> get_trace(t, :state, :action)\n(state = [0, 1], action = [0, 1])\n\njulia> get_trace(t, :next_state, :next_action)\n(next_state = [1, 2], next_action = [1, 2])\n\njulia> pop!(t)\n1-element Trajectory{(:state, :action, :reward, :terminal, :next_state, :next_action),Tuple{Int64,Int64,Float32,Bool,Int64,Int64},NamedTuple{(:reward, :terminal, :state, :action),Tuple{Array{Float32,1},Array{Bool,1},Array{Int64,1},Array{Int64,1}}}}:\n (state = 0, action = 0, reward = 0.0, terminal = 0, next_state = 1, next_action = 1)\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.VectorialTrajectory-Tuple{}","page":"RLCore","title":"ReinforcementLearningCore.VectorialTrajectory","text":"VectorialTrajectory(;trace_name=trace_type ...)\n\nUse Vector to store the traces.\n\nExample\n\njulia> t = VectorialTrajectory(;a=Int, b=Symbol)\n0-element Trajectory{(:a, :b),Tuple{Int64,Symbol},NamedTuple{(:a, :b),Tuple{Array{Int64,1},Array{Symbol,1}}}}\n\njulia> push!(t, a=0, b=:x)\n\njulia> push!(t, a=1, b=:y)\n\njulia> t\n2-element Trajectory{(:a, :b),Tuple{Int64,Symbol},NamedTuple{(:a, :b),Tuple{Array{Int64,1},Array{Symbol,1}}}}:\n (a = 0, b = :x)\n (a = 1, b = :y)\n\njulia> get_trace(t, :b)\n2-element Array{Symbol,1}:\n :x\n :y\n\njulia> pop!(t)\n\njulia> t\n1-element Trajectory{(:a, :b),Tuple{Int64,Symbol},NamedTuple{(:a, :b),Tuple{Array{Int64,1},Array{Symbol,1}}}}:\n (a = 0, b = :x)\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.WeightedExplorer","page":"RLCore","title":"ReinforcementLearningCore.WeightedExplorer","text":"WeightedExplorer(;is_normalized::Bool)\n\nis_normalized is used to indicate if the feeded action values are alrady normalized to have a sum of 1.0.\n\n\n\n\n\n","category":"type"},{"location":"rl_core/#Base.getindex-Tuple{Trajectory,Symbol}","page":"RLCore","title":"Base.getindex","text":"A helper function to access inner fields\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#Base.pop!-Tuple{AbstractTrajectory,Vararg{Symbol,N} where N}","page":"RLCore","title":"Base.pop!","text":"Base.pop!(t::AbstractTrajectory, s::Symbol...)\n\npop! out one element of the traces specified in s\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#Base.pop!-Union{Tuple{AbstractTrajectory{names,types} where types}, Tuple{names}} where names","page":"RLCore","title":"Base.pop!","text":"Base.pop!(t::AbstractTrajectory{names}) where {names}\n\npop! out one element of each trace in t\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#Base.push!-Tuple{AbstractTrajectory}","page":"RLCore","title":"Base.push!","text":"Base.push!(t::AbstractTrajectory; kwargs...)\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#CUDA.device-Tuple{Any}","page":"RLCore","title":"CUDA.device","text":"device(model)\n\nDetect the suitable running device for the model. Return Val(:cpu) by default.\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningBase.get_priority-Tuple{AbstractLearner,Any}","page":"RLCore","title":"ReinforcementLearningBase.get_priority","text":"get_priority(p::AbstractLearner, experience)\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningBase.get_prob-Tuple{AbstractExplorer,Any,Any}","page":"RLCore","title":"ReinforcementLearningBase.get_prob","text":"get_prob(p::AbstractExplorer, x, mask)\n\nSimilart to get_prob(p::AbstractExplorer, x), but here only the masked elements are considered.\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningBase.get_prob-Tuple{AbstractExplorer,Any}","page":"RLCore","title":"ReinforcementLearningBase.get_prob","text":"get_prob(p::AbstractExplorer, x) -> AbstractDistribution\n\nGet the action distribution given action values.\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningBase.get_prob-Tuple{EpsilonGreedyExplorer{#s62,true,R} where R where #s62,Any}","page":"RLCore","title":"ReinforcementLearningBase.get_prob","text":"get_prob(s::EpsilonGreedyExplorer, values) ->Categorical\nget_prob(s::EpsilonGreedyExplorer, values, mask) ->Categorical\n\nReturn the probability of selecting each action given the estimated values of each action.\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningBase.update!-Tuple{AbstractApproximator,Any}","page":"RLCore","title":"ReinforcementLearningBase.update!","text":"update!(a::AbstractApproximator, correction)\n\nUsually the correction is the gradient of inner parameters.\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningBase.update!-Tuple{AbstractEnvironmentModel,AbstractTrajectory,AbstractPolicy}","page":"RLCore","title":"ReinforcementLearningBase.update!","text":"By default, only use trajectory to update model\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningBase.update!-Union{Tuple{N}, Tuple{T}, Tuple{CircularArrayBuffer{T,N},AbstractArray}} where N where T","page":"RLCore","title":"ReinforcementLearningBase.update!","text":"update!(cb::CircularArrayBuffer{T,N}, data::AbstractArray)\n\nupdate! the last frame of cb with data.\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.ApproximatorStyle-Tuple{AbstractApproximator}","page":"RLCore","title":"ReinforcementLearningCore.ApproximatorStyle","text":"Used to detect what an AbstractApproximator is approximating.\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore._discount_rewards!-Tuple{Any,Any,Any,Any,Nothing}","page":"RLCore","title":"ReinforcementLearningCore._discount_rewards!","text":"assuming rewards and new_rewards are Vector\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore._generalized_advantage_estimation!-NTuple{6,Any}","page":"RLCore","title":"ReinforcementLearningCore._generalized_advantage_estimation!","text":"assuming rewards and advantages are Vector\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.discount_rewards-Union{Tuple{T}, Tuple{Union{AbstractArray{T,2} where T, AbstractArray{T,1} where T},T}} where T<:Number","page":"RLCore","title":"ReinforcementLearningCore.discount_rewards","text":"discount_rewards(rewards::VectorOrMatrix, γ::Number;kwargs...)\n\nCalculate the gain started from the current step with discount rate of γ. rewards can be a matrix.\n\nKeyword argments\n\ndims=:, if rewards is a Matrix, then dims can only be 1 or 2.\nterminal=nothing, specify if each reward follows by a terminal. nothing means the game is not terminated yet. If terminal is provided, then the size must be the same with rewards.\ninit=nothing, init can be used to provide the the reward estimation of the last state.\n\nExample\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.flatten_batch-Tuple{AbstractArray}","page":"RLCore","title":"ReinforcementLearningCore.flatten_batch","text":"flatten_batch(x::AbstractArray)\n\nMerge the last two dimension.\n\nExample\n\njulia> x = reshape(1:12, 2, 2, 3)\n2×2×3 reshape(::UnitRange{Int64}, 2, 2, 3) with eltype Int64:\n[:, :, 1] =\n 1  3\n 2  4\n\n[:, :, 2] =\n 5  7\n 6  8\n\n[:, :, 3] =\n  9  11\n 10  12\n\njulia> flatten_batch(x)\n2×6 reshape(::UnitRange{Int64}, 2, 6) with eltype Int64:\n 1  3  5  7   9  11\n 2  4  6  8  10  12\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.generalized_advantage_estimation-Union{Tuple{T}, Tuple{Union{AbstractArray{T,2} where T, AbstractArray{T,1} where T},Union{AbstractArray{T,2} where T, AbstractArray{T,1} where T},T,T}} where T<:Number","page":"RLCore","title":"ReinforcementLearningCore.generalized_advantage_estimation","text":"generalized_advantage_estimation(rewards::VectorOrMatrix, values::VectorOrMatrix, γ::Number, λ::Number;kwargs...)\n\nCalculate the generalized advantage estimate started from the current step with discount rate of γ and a lambda for GAE-Lambda of 'λ'. rewards and 'values' can be a matrix.\n\nKeyword argments\n\ndims=:, if rewards is a Matrix, then dims can only be 1 or 2.\nterminal=nothing, specify if each reward follows by a terminal. nothing means the game is not terminated yet. If terminal is provided, then the size must be the same with rewards.\n\nExample\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.get_trace-Tuple{AbstractTrajectory,Vararg{Symbol,N} where N}","page":"RLCore","title":"ReinforcementLearningCore.get_trace","text":"get_trace(t::AbstractTrajectory, s::Symbol...)\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.get_trace-Union{Tuple{AbstractTrajectory{names,types} where types}, Tuple{names}} where names","page":"RLCore","title":"ReinforcementLearningCore.get_trace","text":"get_trace(t::AbstractTrajectory{names}) where {names}\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.get_trace-Union{Tuple{N}, Tuple{AbstractTrajectory,Tuple{Vararg{Symbol,N}}}} where N","page":"RLCore","title":"ReinforcementLearningCore.get_trace","text":"get_trace(t::AbstractTrajectory, s::NTuple{N,Symbol}) where {N}\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.huber_loss-Tuple{Any,Any}","page":"RLCore","title":"ReinforcementLearningCore.huber_loss","text":"huber_loss(labels, predictions; δ = 1.0f0)\n\nSee Huber loss\n\n\n\n\n\n","category":"method"},{"location":"rl_core/#ReinforcementLearningCore.huber_loss_unreduced-Tuple{Any,Any}","page":"RLCore","title":"ReinforcementLearningCore.huber_loss_unreduced","text":"huber_loss_unreduced(labels, predictions; δ = 1.0f0)\n\nSimilar to huber_loss, but it doesn't do the mean operation in the last step.\n\n\n\n\n\n","category":"method"}]
}
