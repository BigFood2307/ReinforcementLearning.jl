var documenterSearchIndex = {"docs":
[{"location":"components/agents/#Agents-1","page":"Agents","title":"Agents","text":"","category":"section"},{"location":"components/agents/#","page":"Agents","title":"Agents","text":"AbstractAgent\nAgent\nDynaAgent","category":"page"},{"location":"components/agents/#ReinforcementLearning.AbstractAgent","page":"Agents","title":"ReinforcementLearning.AbstractAgent","text":"An agent is a functional object, which takes in an observation and returns an action. Agents must also implement the update!(agent::AbstractAgent, obs_action::Pair) method to indicate how to update the internal state of the agent.\n\n\n\n\n\n","category":"type"},{"location":"components/agents/#ReinforcementLearning.Agent","page":"Agents","title":"ReinforcementLearning.Agent","text":"Agent(;kwargs...)\n\nOne of the most commonly used AbstractAgent.\n\nGenerally speaking, it does nothing but\n\nPass observation to the policy to generate an action\nUpdate the buffer using the observation => action pair\nUpdate the policy with the newly updated buffer\n\nKeywords & Fields\n\nπ::AbstractPolicy: the policy to use\nbuffer::AbstractTurnBuffer: used to store transitions between agent and environment\nrole=:DEFAULT: used to distinguish different agents\n\n\n\n\n\n","category":"type"},{"location":"components/agents/#ReinforcementLearning.DynaAgent","page":"Agents","title":"ReinforcementLearning.DynaAgent","text":"DynaAgent(;kwargs...)\n\nDynaAgent is first introduced in: Sutton, Richard S. \"Dyna, an integrated architecture for learning, planning, and reacting.\" ACM Sigart Bulletin 2.4 (1991): 160-163.\n\nKeywords & Fields\n\nπ::AbstractPolicy: the policy to use\nmodel::AbstractEnvironmentModel: describe the environment to interact with\nbuffer::AbstractTurnBuffer: used to store transitions between agent and environment\nrole=:DEFAULT: used to distinguish different agents\nplan_step::Int=10: the count of planning steps\n\nThe main difference between DynaAgent and Agent is that an environment model is involved. It is best described in the book: Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\n(Image: )\n\n(Image: )\n\n\n\n\n\n","category":"type"},{"location":"components/buffers/#Buffers-1","page":"Buffers","title":"Buffers","text":"","category":"section"},{"location":"components/buffers/#","page":"Buffers","title":"Buffers","text":"AbstractTurnBuffer\nCircularArrayBuffer\nCircularTurnBuffer\ncircular_RTSA_buffer\ncircular_PRTSA_buffer\nEpisodeTurnBuffer\nepisode_RTSA_buffer","category":"page"},{"location":"components/buffers/#ReinforcementLearning.AbstractTurnBuffer","page":"Buffers","title":"ReinforcementLearning.AbstractTurnBuffer","text":"AbstractTurnBuffer{names, types} <: AbstractArray{NamedTuple{names, types}, 1}\n\nAbstractTurnBuffer is supertype of a collection of buffers to store the interactions between agents and environments. It is a subtype of AbstractArray{NamedTuple{names, types}, 1} where names specifies which fields are to store and types is the coresponding types of the names.\n\nRequired Methods Brief Description\nBase.push!(b::AbstractTurnBuffer{names, types}, s[, a, r, d, s′, a′]) Push a turn info into the buffer. According to different names and types of the buffer b, it may accept different number of arguments\nisfull(b) Check whether the buffer is full or not\nBase.length(b) Return the length of buffer\nBase.getindex(b::AbstractTurnBuffer{names, types}) Return a turn of type NamedTuple{names, types}\nBase.empty!(b) Reset the buffer\nOptional Methods \nBase.size(b) Return (length(b),) by default\nBase.isempty(b) Check whether the buffer is empty or not. Return length(b) == 0 by default\nBase.lastindex(b) Return length(b) by default\n\n\n\n\n\n","category":"type"},{"location":"components/buffers/#ReinforcementLearning.CircularTurnBuffer","page":"Buffers","title":"ReinforcementLearning.CircularTurnBuffer","text":"CircularTurnBuffer{names,types,Tbs}\n\nContain a collection of buffers (mainly are CircularArrayBuffer, but not restriced to it) to represent the interractions between agents and environments. This struct itself is very simple, some commonly used buffers are provided by:\n\ncircular_PRTSA_buffer\ncircular_PRTSA_buffer\n\nFields\n\nbuffers: a tuple of inner buffers\n\n\n\n\n\n","category":"type"},{"location":"components/buffers/#ReinforcementLearning.circular_RTSA_buffer","page":"Buffers","title":"ReinforcementLearning.circular_RTSA_buffer","text":"circular_RTSA_buffer(;kwargs...) -> CircularTurnBuffer\n\nA helper function to help generate a CircularTurnBuffer with RTSA (Reward, Terminal, State, Action) fields as buffers.\n\nKeywords\n\nNecessary\n\ncapacity::Int: the maximum length of the buffer\n\nOptional\n\nstate_eltype::Type=Int: the type of the state field in an Observation, Int by default\nstate_size::NTuple{N, Int}=(): the size of the state field in an Observation, the N must match ndims(state_eltype). Since the default state_eltype is Int, it is an empty tuple here by default.\naction_eltype::Type=Int: similar to state_eltype\naction_size::NTuple{N, Int}=(): similar to state_size\nreward_eltype::Type=Float32: similar to state_eltype\nreward_size::NTuple{N, Int}=(): similar to state_size\nterminal_eltype::Type=Bool: similar to state_eltype\nterminal_size::NTuple{N, Int}=(): similar to state_size\n\nThe following picture will help you understand how the data are organized.\n\n(Image: )\n\nExamples\n\njulia> using ReinforcementLearning\n\njulia> b = circular_RTSA_buffer(;capacity=2, state_eltype=Array{Float32, 2}, state_size=(2, 2))\n0-element CircularTurnBuffer{(:reward, :terminal, :state, :action),Tuple{Float32,Bool,Array{Float32,2},Int64},NamedTuple{(:reward, :terminal, :state, :action),Tuple{CircularArrayBuffer{Float32,Float32,1},CircularArrayBuffer{Bool,Bool,1},CircularArrayBuffer{Array{Float32,2},Float32,3},CircularArrayBuffer{Int64,Int64,1}}}}\n\njulia> push!(b; reward = 0.0, terminal = true, state = Float32[0 0; 0 0], action = 0)\n\njulia> length(b)\n0\n\njulia> b.buffers.reward\n1-element CircularArrayBuffer{Float32,Float32,1}:\n 0.0\n\njulia> b.buffers.terminal\n1-element CircularArrayBuffer{Bool,Bool,1}:\n 1\n\njulia> b.buffers.state\n2×2×1 CircularArrayBuffer{Array{Float32,2},Float32,3}:\n[:, :, 1] =\n 0.0  0.0\n 0.0  0.0\n\njulia> b.buffers.action\n1-element CircularArrayBuffer{Int64,Int64,1}:\n 0\n\njulia> push!(b; reward = 1.0, terminal = false, state = Float32[1 1; 1 1], action = 1)\n\njulia> b\n1-element CircularTurnBuffer{(:reward, :terminal, :state, :action),Tuple{Float32,Bool,Array{Float32,2},Int64},NamedTuple{(:reward, :terminal, :state, :action),Tuple{CircularArrayBuffer{Float32,Float32,1},CircularArrayBuffer{Bool,Bool,1},CircularArrayBuffer{Array{Float32,2},Float32,3},CircularArrayBuffer{Int64,Int64,1}}}}:\n (state = Float32[0.0 0.0; 0.0 0.0], action = 0, reward = 1.0f0, terminal = false, next_state = Float32[1.0 1.0; 1.0 1.0], next_action = 1)\n\njulia> push!(b; reward = 2.0, terminal = true, state = Float32[2 2; 2 2], action = 2)\n\njulia> b\n2-element CircularTurnBuffer{(:reward, :terminal, :state, :action),Tuple{Float32,Bool,Array{Float32,2},Int64},NamedTuple{(:reward, :terminal, :state, :action),Tuple{CircularArrayBuffer{Float32,Float32,1},CircularArrayBuffer{Bool,Bool,1},CircularArrayBuffer{Array{Float32,2},Float32,3},CircularArrayBuffer{Int64,Int64,1}}}}:\n (state = Float32[0.0 0.0; 0.0 0.0], action = 0, reward = 1.0f0, terminal = false, next_state = Float32[1.0 1.0; 1.0 1.0], next_action = 1)\n (state = Float32[1.0 1.0; 1.0 1.0], action = 1, reward = 2.0f0, terminal = true, next_state = Float32[2.0 2.0; 2.0 2.0], next_action = 2) \n\njulia> push!(b; reward = 3.0, terminal = false, state = Float32[3 3; 3 3], action = 3)\n\njulia> b\n2-element CircularTurnBuffer{(:reward, :terminal, :state, :action),Tuple{Float32,Bool,Array{Float32,2},Int64},NamedTuple{(:reward, :terminal, :state, :action),Tuple{CircularArrayBuffer{Float32,Float32,1},CircularArrayBuffer{Bool,Bool,1},CircularArrayBuffer{Array{Float32,2},Float32,3},CircularArrayBuffer{Int64,Int64,1}}}}:\n (state = Float32[1.0 1.0; 1.0 1.0], action = 1, reward = 2.0f0, terminal = true, next_state = Float32[2.0 2.0; 2.0 2.0], next_action = 2) \n (state = Float32[2.0 2.0; 2.0 2.0], action = 2, reward = 3.0f0, terminal = false, next_state = Float32[3.0 3.0; 3.0 3.0], next_action = 3)\n\njulia> b.buffers.state\n2×2×3 CircularArrayBuffer{Array{Float32,2},Float32,3}:\n[:, :, 1] =\n 1.0  1.0\n 1.0  1.0\n\n[:, :, 2] =\n 2.0  2.0\n 2.0  2.0\n\n[:, :, 3] =\n 3.0  3.0\n 3.0  3.0\n\njulia> b.buffers.reward\n3-element CircularArrayBuffer{Float32,Float32,1}:\n 1.0\n 2.0\n 3.0\n\njulia> length(b)\n2\n\n\n\n\n\n","category":"function"},{"location":"components/buffers/#ReinforcementLearning.circular_PRTSA_buffer","page":"Buffers","title":"ReinforcementLearning.circular_PRTSA_buffer","text":"circular_PRTSA_buffer(;kwargs...) -> CircularTurnBuffer\n\nThe only difference compared to circular_RTSA_buffer is that a new field named priority is added. Notice that the struct of priority is not a CircularArrayBuffer but a SumTree.\n\nKeywords\n\nNecessary\n\ncapacity::Int: the maximum length of the buffer\n\nOptional\n\nstate_eltype::Type=Int: the type of the state field in an Observation, Int by default\nstate_size::NTuple{N, Int}=(): the size of the state field in an Observation, the N must match ndims(state_eltype). Since the default state_eltype is Int, it is an empty tuple here by default.\naction_eltype::Type=Int: similar to state_eltype\naction_size::NTuple{N, Int}=(): similar to state_size\nreward_eltype::Type=Float32: similar to state_eltype\nreward_size::NTuple{N, Int}=(): similar to state_size\nterminal_eltype::Type=Bool: similar to state_eltype\nterminal_size::NTuple{N, Int}=(): similar to state_size\npriority_eltype::Type=Float64: the type of the priority field\n\n\n\n\n\n","category":"function"},{"location":"components/buffers/#ReinforcementLearning.EpisodeTurnBuffer","page":"Buffers","title":"ReinforcementLearning.EpisodeTurnBuffer","text":"EpisodeTurnBuffer{names, types, Tbs} <: AbstractTurnBuffer{names, types}\nEpisodeTurnBuffer{names, types}() where {names, types}\n\nSimilar to CircularTurnBuffer, but instead of using CircularArrayBuffer, it uses a vector to store each element specified by names and types. And when it reaches the end of an episode, the buffer is emptied first when a new observation is pushed.\n\nnote: Note\nNotice that, before emptifying the EpisodeTurnBuffer, the last element of each field is exracted and then pushed at the head of the buffer. Without this step, the first transition of the new episode will be lost!\n\nSee also: episode_RTSA_buffer\n\n\n\n\n\n","category":"type"},{"location":"components/buffers/#ReinforcementLearning.episode_RTSA_buffer","page":"Buffers","title":"ReinforcementLearning.episode_RTSA_buffer","text":"episode_RTSA_buffer(;kwargs...) -> EpisodeTurnBuffer\n\nInitialize an EpisodeTurnBuffer with fields of Reward, Terminal, State, Action.\n\nKeywords\n\nstate_eltype::Type=Int: the type of state.\naction_eltype::Type=Int: the type of action.\nreward_eltype::Type=Float32: the type of reward.\nterminal_eltype::Type=Bool: the type of terminal.\n\n\n\n\n\n","category":"function"},{"location":"components/environments/#Environments-1","page":"Environments","title":"Environments","text":"","category":"section"},{"location":"components/environments/#","page":"Environments","title":"Environments","text":"This package relies on some interfaces provided by the ReinforcementLearningEnvironments.jl (RLEnvs). For completeness, we will also give a short introduction to it here.","category":"page"},{"location":"components/environments/#","page":"Environments","title":"Environments","text":"RLEnvs provides many interfaces similar to OpenAI Gym. But also extends it a little bit to make things easier to interact with sync/async, single/multi agent environments.","category":"page"},{"location":"components/environments/#","page":"Environments","title":"Environments","text":"Basically, an environment is a functional object which takes in an action and changes its internal state correspondly. For sync environments, both env(action) and reset!(env) should return nothing, and observe(env) should return an Observation. For async environments, they should all return a task.","category":"page"},{"location":"components/environments/#","page":"Environments","title":"Environments","text":"A specially kind of environment is WrappedEnv.","category":"page"},{"location":"components/environments/#","page":"Environments","title":"Environments","text":"Observation\nWrappedEnv","category":"page"},{"location":"components/environments/#ReinforcementLearning.WrappedEnv","page":"Environments","title":"ReinforcementLearning.WrappedEnv","text":"WrappedEnv(;env, preprocessor)\n\nThe observation of env is first processed by the preprocessor.\n\n\n\n\n\n","category":"type"},{"location":"components/environments/#Preprocessors-1","page":"Environments","title":"Preprocessors","text":"","category":"section"},{"location":"components/environments/#","page":"Environments","title":"Environments","text":"Following are some built-in preprocessors. Notice that preprocessors can be chained (like Chain(p1, p2, ps...)) to get a composed preprocessor.","category":"page"},{"location":"components/environments/#","page":"Environments","title":"Environments","text":"AbstractPreprocessor\nFourierPreprocessor\nPolynomialPreprocessor\nTilingPreprocessor\nImageResize\nImageCrop\nImageResizeNearestNeighbour","category":"page"},{"location":"components/environments/#ReinforcementLearning.AbstractPreprocessor","page":"Environments","title":"ReinforcementLearning.AbstractPreprocessor","text":"Preprocess an Observation and return a new observation. By default, the preprocessor is only applied to the state field of the Observation and other fields remain unchanged.\n\nFor customized preprocessors that inherit AbstractPreprocessor, you can change this behavior by rewriting (p::AbstractPreprocessor)(obs::Observation) method.\n\n\n\n\n\n","category":"type"},{"location":"components/environments/#ReinforcementLearning.FourierPreprocessor","page":"Environments","title":"ReinforcementLearning.FourierPreprocessor","text":"FourierPreprocessor(order::Int)\n\nTransform a scalar to a vector of order+1 Fourier bases.\n\n\n\n\n\n","category":"type"},{"location":"components/environments/#ReinforcementLearning.PolynomialPreprocessor","page":"Environments","title":"ReinforcementLearning.PolynomialPreprocessor","text":"PolynomialPreprocessor(order::Int)\n\nTransform a scalar to vector of maximum order polynomial.\n\n\n\n\n\n","category":"type"},{"location":"components/environments/#ReinforcementLearning.TilingPreprocessor","page":"Environments","title":"ReinforcementLearning.TilingPreprocessor","text":"TilingPreprocessor(tilings::Vector{<:Tiling})\n\nUse each tilings to encode the state and return a vector.\n\n\n\n\n\n","category":"type"},{"location":"components/environments/#ReinforcementLearning.ImageResize","page":"Environments","title":"ReinforcementLearning.ImageResize","text":"ImageResize(img::Array{T, N})\nImageResize(dims::Int...) -> ImageResize(Float32, dims...)\nImageResize(T::Type{<:Number}, dims::Int...)\n\nUsing BSpline method to resize the state field of an Observation to size of img (or dims).\n\n\n\n\n\n","category":"type"},{"location":"components/environments/#ReinforcementLearning.ImageCrop","page":"Environments","title":"ReinforcementLearning.ImageCrop","text":"struct ImageCrop\n    xidx::UnitRange{Int64}\n    yidx::UnitRange{Int64}\nend\n\nSelect indices xidx and yidx from a 2 or 3 dimensional array. TODO: Inefficient!\n\nExample:\n\nc = ImageCrop(2:5, 3:2:9)\nc([10i + j for i in 1:10, j in 1:10])\n\n\n\n\n\n","category":"type"},{"location":"components/environments/#ReinforcementLearning.ImageResizeNearestNeighbour","page":"Environments","title":"ReinforcementLearning.ImageResizeNearestNeighbour","text":"struct ImageResizeNearestNeighbour\n    outdim::Tuple{Int64, Int64}\nend\n\nResize any image to outdim = (width, height) by nearest-neighbour interpolation (i.e. subsampling).\n\nExample:\n\nr = ImageResizeNearestNeighbour((50, 50))\nr(rand(200, 200))\nr(rand(UInt8, 3, 100, 100))\n\n\n\n\n\n","category":"type"},{"location":"components/learners/#Learners-1","page":"Learners","title":"Learners","text":"","category":"section"},{"location":"components/learners/#","page":"Learners","title":"Learners","text":"AbstractLearner","category":"page"},{"location":"components/learners/#ReinforcementLearning.AbstractLearner","page":"Learners","title":"ReinforcementLearning.AbstractLearner","text":"A learner is used to define:\n\nHow to generate necessary training data?\nHow to update the inner Approximators?\n\n\n\n\n\n","category":"type"},{"location":"components/learners/#Traditional-Learners-1","page":"Learners","title":"Traditional Learners","text":"","category":"section"},{"location":"components/learners/#","page":"Learners","title":"Learners","text":"TDLearner\nDoubleLearner\nDifferentialTDLearner\nTDλReturnLearner\nMonteCarloLearner\nGradientBanditLearner","category":"page"},{"location":"components/learners/#ReinforcementLearning.TDLearner","page":"Learners","title":"ReinforcementLearning.TDLearner","text":"TDLearner(approximator::Tapp, γ::Float64, optimizer::Float64; n::Int=0) where {Tapp<:AbstractVApproximator}\nTDLearner(approximator::Tapp, γ::Float64, optimizer::Float64; n::Int=0, method::Symbol=:SARSA) where {Tapp<:AbstractQApproximator}\n\nThe TDLearner(Temporal Difference Learner) use the latest n step experiences to update the approximator. Note that n starts with 0, which means looking forward for the next n steps. γ is the discount rate of experience. optimizer is the learning rate.\n\nFor AbstractVApproximator, the only supported update method is :SRS, which means only States, Rewards and next_Sates are used to update the approximator.\n\nFor AbstractQApproximator, the following methods are supported:\n\n:SARS (aka Q-Learning)\n:SARSA\n:ExpectedSARSA\n\n\n\n\n\n","category":"type"},{"location":"components/learners/#ReinforcementLearning.DoubleLearner","page":"Learners","title":"ReinforcementLearning.DoubleLearner","text":"DoubleLearner(;L1, L2)\n\nFor now, this only supports TDLearner for L1 and L2.\n\n\n\n\n\n","category":"type"},{"location":"components/learners/#ReinforcementLearning.DifferentialTDLearner","page":"Learners","title":"ReinforcementLearning.DifferentialTDLearner","text":"DifferentialTDLearner(;approximator::A, α::Float64, β::Float64, R̄::Float64 = 0.0, n::Int = 0)\n\n\n\n\n\n","category":"type"},{"location":"components/learners/#ReinforcementLearning.TDλReturnLearner","page":"Learners","title":"ReinforcementLearning.TDλReturnLearner","text":"TDλReturnLearner(;approximator::Tapp, γ::Float64 = 1.0, α::Float64, λ::Float64)\n\n\n\n\n\n","category":"type"},{"location":"components/learners/#ReinforcementLearning.MonteCarloLearner","page":"Learners","title":"ReinforcementLearning.MonteCarloLearner","text":"MonteCarloLearner(; approximator::A, γ = 1.0, α = 1.0, kind = FIRST_VISIT, sampling = NO_SAMPLING, returns = CachedSampleAvg())\n\n\n\n\n\n","category":"type"},{"location":"components/learners/#ReinforcementLearning.GradientBanditLearner","page":"Learners","title":"ReinforcementLearning.GradientBanditLearner","text":"GradientBanditLearner(;approximator::A, optimizer::O, baseline::B)\n\n\n\n\n\n","category":"type"},{"location":"components/learners/#Q-Learners-1","page":"Learners","title":"Q Learners","text":"","category":"section"},{"location":"components/learners/#","page":"Learners","title":"Learners","text":"BasicDQNLearner\nDQNLearner\nPrioritizedDQNLearner\nRainbowLearner","category":"page"},{"location":"components/learners/#ReinforcementLearning.BasicDQNLearner","page":"Learners","title":"ReinforcementLearning.BasicDQNLearner","text":"BasicDQNLearner(;kwargs...)\n\nSee paper: Playing Atari with Deep Reinforcement Learning\n\nThis is the very basic implementation of DQN. Compared to the traditional Q learning, the only difference is that, in the updating step it uses a batch of transitions sampled from an experience buffer instead of current transition. And the approximator is usually a NeuralNetworkQ.\n\nYou can start from this implementation to understand how everything is organized and how to write your own customized algorithm.\n\nKeywords\n\napproximator::AbstractQApproximator: used to get Q-values of a state.\nloss_fun: the loss function to use. TODO: provide a default huber_loss?\nγ::Float32=0.99f0: discount rate.\nbatch_size::Int=32\nupdate_horizon::Int=1: length of update ('n' in n-step update).\nmin_replay_history::Int=32: number of transitions that should be experienced before updating the approximator.\n\n\n\n\n\n","category":"type"},{"location":"components/learners/#ReinforcementLearning.DQNLearner","page":"Learners","title":"ReinforcementLearning.DQNLearner","text":"DQNLearner(;kwargs...)\n\nSee paper: Human-level control through deep reinforcement learning\n\nKeywords\n\napproximator::AbstractQApproximator: used to get Q-values of a state.\ntarget_approximator::AbstractQApproximator: similar to approximator, but used to estimate the target (the next state).\nloss_fun: the loss function.\nγ::Float32=0.99f0: discount rate.\nbatch_size::Int=32\nupdate_horizon::Int=1: length of update ('n' in n-step update).\nmin_replay_history::Int=32: number of transitions that should be experienced before updating the approximator.\nupdate_freq::Int=4: the frequency of updating the approximator.\ntarget_update_freq::Int=100: the frequency of syncing target_approximator.\nstack_size::Union{Int, Nothing}=4: use the recent stack_size frames to form a stacked state.\n\n\n\n\n\n","category":"type"},{"location":"components/learners/#ReinforcementLearning.PrioritizedDQNLearner","page":"Learners","title":"ReinforcementLearning.PrioritizedDQNLearner","text":"PrioritizedDQNLearner(;kwargs...)\n\nSee paper: Prioritized Experience Replay\n\nKeywords\n\napproximator::AbstractQApproximator: used to get Q-values of a state.\ntarget_approximator::AbstractQApproximator: similar to approximator, but used to estimate the target (the next state).\nloss_fun: the loss function.\nγ::Float32=0.99f0: discount rate.\nbatch_size::Int=32\nupdate_horizon::Int=1: length of update ('n' in n-step update).\nmin_replay_history::Int=32: number of transitions that should be experienced before updating the approximator.\nupdate_freq::Int=4: the frequency of updating the approximator.\ntarget_update_freq::Int=100: the frequency of syncing target_approximator.\nstack_size::Union{Int, Nothing}=4: use the recent stack_size frames to form a stacked state.\ndefault_priority::Float64=100.: the default priority for newly added transitions.\n\n\n\n\n\n","category":"type"},{"location":"components/learners/#ReinforcementLearning.RainbowLearner","page":"Learners","title":"ReinforcementLearning.RainbowLearner","text":"RainbowLearner(;kwargs...)\n\nSee paper: Rainbow: Combining Improvements in Deep Reinforcement Learning\n\nKeywords\n\napproximator::AbstractQApproximator: used to get Q-values of a state.\ntarget_approximator::AbstractQApproximator: similar to approximator, but used to estimate the target (the next state).\nloss_fun: the loss function.\nVₘₐₓ::Float32: the maximum value of distribution.\nVₘᵢₙ::Float32: the minimum value of distribution.\nn_actions::Int: number of possible actions.\nγ::Float32=0.99f0: discount rate.\nbatch_size::Int=32\nupdate_horizon::Int=1: length of update ('n' in n-step update).\nmin_replay_history::Int=32: number of transitions that should be experienced before updating the approximator.\nupdate_freq::Int=4: the frequency of updating the approximator.\ntarget_update_freq::Int=500: the frequency of syncing target_approximator.\nstack_size::Union{Int, Nothing}=4: use the recent stack_size frames to form a stacked state.\ndefault_priority::Float64=100.: the default priority for newly added transitions.\nn_atoms::Int=51: the number of buckets of the value function distribution.\nstack_size::Union{Int, Nothing}=4: use the recent stack_size frames to form a stacked state.\ndefault_priority::Float64=100.: the default priority for newly added transitions.\n\nwarning: Warning\nThe Zygote_gpu version is slow due to that the argmax(A, dims=1) falls back to the CPU version in CuArrays.\n\n\n\n\n\n","category":"type"},{"location":"components/policies/#Policies-1","page":"Policies","title":"Policies","text":"","category":"section"},{"location":"components/policies/#","page":"Policies","title":"Policies","text":"AbstractPolicy\nTabularRandomPolicy\nTabularDeterministicPolicy\nExploringStartPolicy\nOffPolicy\nQBasedPolicy\nVBasedPolicy\nReinforcePolicy","category":"page"},{"location":"components/policies/#ReinforcementLearning.AbstractPolicy","page":"Policies","title":"ReinforcementLearning.AbstractPolicy","text":"A policy is a functional object to generate an action given a state.\n\n\n\n\n\n","category":"type"},{"location":"components/policies/#ReinforcementLearning.TabularRandomPolicy","page":"Policies","title":"ReinforcementLearning.TabularRandomPolicy","text":"TabularRandomPolicy(prob::Array{Float64, 2})\n\nprob describes the distribution of actions for each state.\n\n\n\n\n\n","category":"type"},{"location":"components/policies/#ReinforcementLearning.TabularDeterministicPolicy","page":"Policies","title":"ReinforcementLearning.TabularDeterministicPolicy","text":"TabularDeterministicPolicy(table::Vector{Int}, nactions::Int) -> TabularDeterministicPolicy\n\nAlways select the specific action according to the state defined in table. nactions is used to define the number of possible actions.\n\n\n\n\n\n","category":"type"},{"location":"components/policies/#ReinforcementLearning.ExploringStartPolicy","page":"Policies","title":"ReinforcementLearning.ExploringStartPolicy","text":"ExploringStartPolicy(π::P, actions::A) -> ExploringStartPolicy{P,A}\n\nGenerate a random action at the beginning of an episode. For other steps, use π to generate the action.\n\n\n\n\n\n","category":"type"},{"location":"components/policies/#ReinforcementLearning.OffPolicy","page":"Policies","title":"ReinforcementLearning.OffPolicy","text":"OffPolicy(π_target::P, π_behavior::B) -> OffPolicy{P,B}\n\n\n\n\n\n","category":"type"},{"location":"components/policies/#ReinforcementLearning.QBasedPolicy","page":"Policies","title":"ReinforcementLearning.QBasedPolicy","text":"QBasedPolicy(learner::Q, selector::S) -> QBasedPolicy{Q, S}\n\nUse a Q-learner to generate the estimations of actions and use selector to get the action.\n\n\n\n\n\n","category":"type"},{"location":"components/policies/#ReinforcementLearning.VBasedPolicy","page":"Policies","title":"ReinforcementLearning.VBasedPolicy","text":"VBasedPolicy(learner::V, f::F) -> VBasedPolicy{V,F}\n\nUse a function f to generate action and update the V-learner in the meantime.\n\n\n\n\n\n","category":"type"},{"location":"components/policies/#ReinforcementLearning.ReinforcePolicy","page":"Policies","title":"ReinforcementLearning.ReinforcePolicy","text":"ReinforcePolicy(approximator, α, γ)\n\nThis is the very basic reinforce algorithm.\n\nTODO: implement some other variants.\n\n\n\n\n\n","category":"type"},{"location":"components/action_selectors/#Action-Selectors-1","page":"Action Selectors","title":"Action Selectors","text":"","category":"section"},{"location":"components/action_selectors/#","page":"Action Selectors","title":"Action Selectors","text":"AbstractDiscreteActionSelector\nAlternateSelector\nEpsilonGreedySelector\nUCBSelector\nWeightedSelector","category":"page"},{"location":"components/action_selectors/#ReinforcementLearning.AbstractDiscreteActionSelector","page":"Action Selectors","title":"ReinforcementLearning.AbstractDiscreteActionSelector","text":"AbstractDiscreteActionSelector\n\nGenerate an action given the estimated value of different actions.\n\nRequired Methods Brief Description\nselector(values; kwargs...) selector, an instance of AbstractDiscreteActionSelector, must be a callable object which takes in an estimation of all discrete actions and returns an action.\n\n\n\n\n\n","category":"type"},{"location":"components/action_selectors/#ReinforcementLearning.AlternateSelector","page":"Action Selectors","title":"ReinforcementLearning.AlternateSelector","text":"AlternateSelector(n::Int)\n\nUsed to ensure that all actions are selected alternatively.\n\nFields\n\nn::Int: means the optional actions are 1:n.\nstep::Int=0: record the number of times that the selector is applied.\n\n\n\n\n\n","category":"type"},{"location":"components/action_selectors/#ReinforcementLearning.EpsilonGreedySelector","page":"Action Selectors","title":"ReinforcementLearning.EpsilonGreedySelector","text":"EpsilonGreedySelector{T}(;kwargs...)\n\nEpsilon-greedy strategy: The best lever is selected for a proportion 1 - epsilon of the trials, and a lever is selected at random (with uniform probability) for a proportion epsilon . Multi-armed_bandit\n\nTwo kinds of epsilon-decreasing strategy are implmented here (linear and exp).\n\nEpsilon-decreasing strategy: Similar to the epsilon-greedy strategy, except that the value of epsilon decreases as the experiment progresses, resulting in highly explorative behaviour at the start and highly exploitative behaviour at the finish.  - Multi-armed_bandit\n\nKeywords\n\nT::Symbol: defines how to calculate the epsilon in the warmup steps. Supported values are linear and exp.\nstep::Int = 1: record the current step.\nϵ_init::Float64 = 1.0: initial epsilon.\nwarmup_steps::Int=0: the number of steps to use ϵ_init.\ndecay_steps::Int=0: the number of steps for epsilon to decay from ϵ_init to ϵ_stable.\nϵ_stable::Float64: the epsilon after warmup_steps + decay_steps.\n\nExample\n\ns = EpsilonGreedySelector{:linear}(ϵ_init=0.9, ϵ_stable=0.1, warmup_steps=100, decay_steps=100)\nplot([RL.get_ϵ(s, i) for i in 1:500], label=\"linear epsilon\")\n\n(Image: )\n\ns = EpsilonGreedySelector{:exp}(ϵ_init=0.9, ϵ_stable=0.1, warmup_steps=100, decay_steps=100)\nplot([RL.get_ϵ(s, i) for i in 1:500], label=\"exp epsilon\")\n\n(Image: )\n\n\n\n\n\n","category":"type"},{"location":"components/action_selectors/#ReinforcementLearning.UCBSelector","page":"Action Selectors","title":"ReinforcementLearning.UCBSelector","text":"UCBSelector(na; c=2.0, ϵ=1e-10)\n\nArguments\n\nna is the number of actions used to create a internal counter.\nt is used to store current time step.\nc is used to control the degree of exploration.\n\n\n\n\n\n","category":"type"},{"location":"components/action_selectors/#ReinforcementLearning.WeightedSelector","page":"Action Selectors","title":"ReinforcementLearning.WeightedSelector","text":"WeightedSelector(is_normalized::Bool)\n\nis_normalized is used to indicating if the feeded action values are alrady normalized to have a sum of 1.0.\n\n\n\n\n\n","category":"type"},{"location":"components/approximators/#Approximators-1","page":"Approximators","title":"Approximators","text":"","category":"section"},{"location":"components/approximators/#","page":"Approximators","title":"Approximators","text":"AbstractApproximator \nAbstractVApproximator \nAbstractQApproximator \nLinearVApproximator\nTabularVApproximator\nLinearQApproximator\nNeuralNetworkQ","category":"page"},{"location":"components/approximators/#ReinforcementLearning.AbstractApproximator","page":"Approximators","title":"ReinforcementLearning.AbstractApproximator","text":"An approximator is a functional object to estimate a state. Two typical kinds of approximators are AbstractVApproximator and AbstractQApproximator.\n\n\n\n\n\n","category":"type"},{"location":"components/approximators/#ReinforcementLearning.AbstractVApproximator","page":"Approximators","title":"ReinforcementLearning.AbstractVApproximator","text":"A collection of approximators to estimate the value of a state.\n\n\n\n\n\n","category":"type"},{"location":"components/approximators/#ReinforcementLearning.AbstractQApproximator","page":"Approximators","title":"ReinforcementLearning.AbstractQApproximator","text":"A collection of approximators to estimate the values of actions given a state.\n\n\n\n\n\n","category":"type"},{"location":"components/approximators/#ReinforcementLearning.LinearVApproximator","page":"Approximators","title":"ReinforcementLearning.LinearVApproximator","text":"LinearVApproximator(weights::Array{Float64, N}) -> LinearVApproximator{N}\n\nUse the weighted sum to represent the estimation of a state. The state is expected to have the same length with weights.\n\nSee also LinearQApproximator\n\n\n\n\n\n","category":"type"},{"location":"components/approximators/#ReinforcementLearning.TabularVApproximator","page":"Approximators","title":"ReinforcementLearning.TabularVApproximator","text":"TabularVApproximator(table) -> TabularVApproximator\nTabularVApproximator(ns::Int, init::Float64=0.0) -> TabularVApproximator\n\nUse a table of type Vector{Float64} of length ns to record the state values.\n\n\n\n\n\n","category":"type"},{"location":"components/approximators/#ReinforcementLearning.LinearQApproximator","page":"Approximators","title":"ReinforcementLearning.LinearQApproximator","text":"LinearQApproximator(weights::Vector{Float64}, feature_func::F, actions::Vector{Int}) -> LinearQApproximator{F}\n\nUse weighted sum to represent the estimation given a state and an action.\n\nFields\n\nweights::Vector{Float64}: the weight of each feature.\nfeature_func::Function: decide how to generate a feature vector of length(weights) given a state and an action as parameters.\nactions::Vector{Int}: all possible actions.\n\nSee also LinearVApproximator.\n\n\n\n\n\n","category":"type"},{"location":"components/approximators/#ReinforcementLearning.NeuralNetworkQ","page":"Approximators","title":"ReinforcementLearning.NeuralNetworkQ","text":"NeuralNetworkQ(;kwargs...) -> NeuralNetworkQ{D, M, O, P}\n\nUse neural networks to generate estimations of state-action values.\n\nKeywords\n\nmodel::M: describes the network structure.\noptimizer::O: defines how to update parameters given grads.\nparameters::P=params(model): the parameters of model.\ndevice::Symbol=:cpu: the param D of NeuralNetworkQ, specify where to run the model. Supported keywords are:\n:cpu\n:gpu, if the specified device is :gpu, then we will automatically change the device to one of the following device types according to the backend(model):\n:Knet_gpu, means the model is Knet that runs on gpu.\n:Zygote_gpu, means the model is Flux with Zygote that runs on gpu.\n\nFields\n\nmodel::M\noptimizer::O\nparams::P\n\n\n\n\n\n","category":"type"},{"location":"tips_for_developers/#Tips-for-Developers-1","page":"Tips for Developers","title":"Tips for Developers","text":"","category":"section"},{"location":"tips_for_developers/#","page":"Tips for Developers","title":"Tips for Developers","text":"This page aims to provide a short introduction to Julia and some related packages for those who are interested in making contributions to this package.","category":"page"},{"location":"tips_for_developers/#Basic-1","page":"Tips for Developers","title":"Basic","text":"","category":"section"},{"location":"tips_for_developers/#","page":"Tips for Developers","title":"Tips for Developers","text":"Go through the latest doc. Especially keep an eye on the following sections if you come from other programming languages:","category":"page"},{"location":"tips_for_developers/#","page":"Tips for Developers","title":"Tips for Developers","text":"Noteworthy Differences from other Languages.\nYou don't need to remember all those details, but giving a glimp of them will make things easier to transfer to Julia.\nFunction like objects\nIt shouldn't surprise you that many important components in this package are functional objects.\nParametric Types\nIt may be a little struggling to create a flexible type if you have little background in parameteric types. See also Design Patterns with Parameteric Methods\nTraits and Multiple Dispatch\nYou may have heard a lot of Julia users talking about Traits and Multiple Dispatch. This blog provides a general introduction to them.\nSubArrays\nTo avoid unnecessary memory allocation, views are widely used in this package.\nPerformance Tips\nAlways keep those suggestions in mind if you want to make your algorithms run fast.\nMulti-Threading\nMulti-Threading will be heavily use in some async algorithms.","category":"page"},{"location":"tips_for_developers/#Code-Structure-1","page":"Tips for Developers","title":"Code Structure","text":"","category":"section"},{"location":"tips_for_developers/#","page":"Tips for Developers","title":"Tips for Developers","text":"src\n├── components\n│   ├── action_selectors\n│   ├── agents\n│   ├── approximators\n│   ├── buffers\n│   ├── environment_models\n│   ├── learners\n│   └── policies\n├── extensions\n├── glue\n└── Utils","category":"page"},{"location":"tips_for_developers/#","page":"Tips for Developers","title":"Tips for Developers","text":"components: many important concepts in reinforcement learning have a corresponding component here.\nextensions: patches for upstream packages.\nglue: functions and types related to running experiments are here (like how to configure experiments, logging...).\nUtils: an inner module which provides many common helper functions.","category":"page"},{"location":"tips_for_developers/#Some-Important-Packages-1","page":"Tips for Developers","title":"Some Important Packages","text":"","category":"section"},{"location":"tips_for_developers/#[ReinforcementLearningEnvironments.jl](https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironments.jl)-1","page":"Tips for Developers","title":"ReinforcementLearningEnvironments.jl","text":"","category":"section"},{"location":"tips_for_developers/#","page":"Tips for Developers","title":"Tips for Developers","text":"ReinforcementLearningEnvironments.jl provides only some very common environments (for example: CartPoleEnv). But it also provides a set of unified interfaces for some other environments with the help of Requires.jl. ","category":"page"},{"location":"tips_for_developers/#[Flux.jl](https://github.com/FluxML/Flux.jl)-1","page":"Tips for Developers","title":"Flux.jl","text":"","category":"section"},{"location":"tips_for_developers/#","page":"Tips for Developers","title":"Tips for Developers","text":"Flux.jl is a very lightweight package for deep learning. Its source code is really worth reading.","category":"page"},{"location":"tips_for_developers/#[Zygote.jl](https://github.com/FluxML/Zygote.jl)-1","page":"Tips for Developers","title":"Zygote.jl","text":"","category":"section"},{"location":"tips_for_developers/#","page":"Tips for Developers","title":"Tips for Developers","text":"The next-gen AD system for Flux. Basically you only need to understand how to implement a customized @adjoin.","category":"page"},{"location":"tips_for_developers/#[CuArrays.jl](https://github.com/JuliaGPU/CuArrays.jl)-1","page":"Tips for Developers","title":"CuArrays.jl","text":"","category":"section"},{"location":"tips_for_developers/#","page":"Tips for Developers","title":"Tips for Developers","text":"A basic understanding of how CuArrays.jl works will help you to write some efficient customized GPU kernels. An Introduction to GPU Programming in Julia  is also a very helpful resource.","category":"page"},{"location":"tips_for_developers/#[Knet.jl](https://github.com/denizyuret/Knet.jl)-1","page":"Tips for Developers","title":"Knet.jl","text":"","category":"section"},{"location":"tips_for_developers/#","page":"Tips for Developers","title":"Tips for Developers","text":"Another excellent julia package for deep learning. You'll find it easy to write algorithms that run on both Flux and Knet in this package.","category":"page"},{"location":"tips_for_developers/#Comparison-with-Other-RL-Packages-1","page":"Tips for Developers","title":"Comparison with Other RL Packages","text":"","category":"section"},{"location":"tips_for_developers/#Comparison-with-[Dopamine](https://github.com/google/dopamine)-1","page":"Tips for Developers","title":"Comparison with Dopamine","text":"","category":"section"},{"location":"tips_for_developers/#Comparison-with-[Ray/RLlib](https://ray.readthedocs.io/en/latest/rllib.html)-1","page":"Tips for Developers","title":"Comparison with Ray/RLlib","text":"","category":"section"},{"location":"tips_for_developers/#Q-and-A-1","page":"Tips for Developers","title":"Q&A","text":"","category":"section"},{"location":"tips_for_developers/#","page":"Tips for Developers","title":"Tips for Developers","text":"Your questions will appear here.","category":"page"},{"location":"utils/#Utils-1","page":"Utils","title":"Utils","text":"","category":"section"},{"location":"utils/#","page":"Utils","title":"Utils","text":"Modules = [ReinforcementLearning.Utils]","category":"page"},{"location":"utils/#ReinforcementLearning.Utils.SumTree","page":"Utils","title":"ReinforcementLearning.Utils.SumTree","text":"SumTree(capacity::Int)\n\nEfficiently sample and update weights. For more detals, see the post at here. Here we use a vector to represent the binary tree. Suppose we will have capacity leaves at most. Every time we push! new node into the tree, only the recent capacity node and their sum will be updated! [––––––Parent nodes––––––][––––leaves––––] [size: 2^ceil(Int, log2(capacity))-1 ][     size: capacity   ]\n\nExample\n\njulia> t = SumTree(8)\n0-element SumTree\njulia> for i in 1:16\n       push!(t, i)\n       end\njulia> t\n8-element SumTree:\n  9.0\n 10.0\n 11.0\n 12.0\n 13.0\n 14.0\n 15.0\n 16.0\njulia> sample(t)\n(2, 10.0)\njulia> sample(t)\n(1, 9.0)\njulia> inds, ps = sample(t,100000)\n([8, 4, 8, 1, 5, 2, 2, 7, 6, 6  …  1, 1, 7, 1, 6, 1, 5, 7, 2, 7], [16.0, 12.0, 16.0, 9.0, 13.0, 10.0, 10.0, 15.0, 14.0, 14.0  …  9.0, 9.0, 15.0, 9.0, 14.0, 9.0, 13.0, 15.0, 10.0, 15.0])\njulia> countmap(inds)\nDict{Int64,Int64} with 8 entries:\n  7 => 14991\n  4 => 12019\n  2 => 10003\n  3 => 11027\n  5 => 12971\n  8 => 16052\n  6 => 13952\n  1 => 8985\njulia> countmap(ps)\nDict{Float64,Int64} with 8 entries:\n  9.0  => 8985\n  13.0 => 12971\n  10.0 => 10003\n  14.0 => 13952\n  16.0 => 16052\n  11.0 => 11027\n  15.0 => 14991\n  12.0 => 12019\n\n\n\n\n\n","category":"type"},{"location":"utils/#ReinforcementLearning.Utils.Tiling","page":"Utils","title":"ReinforcementLearning.Utils.Tiling","text":"Tiling(ranges::NTuple{N, Tr}) where {N, Tr}\n\nUsing a tuple of ranges to simulate a tiling. The length of ranges indicates the dimension of tilling.\n\nExample\n\njulia> t = Tiling((1:2:5, 10:5:20))\nTiling{2,StepRange{Int64,Int64}}((1:2:5, 10:5:20), [1 3; 2 4])\n\njulia> encode(t, (2, 12))  # encode into an Int\n1\n\njulia> encode(t, (2, 18))\n3\n\njulia> t2 = t - (1, 3)  # shift a little to get a new Tiling\nTiling{2,StepRange{Int64,Int64}}((0:2:4, 7:5:17), [1 3; 2 4])\n\n\n\n\n\n","category":"type"},{"location":"utils/#ReinforcementLearning.Utils.findallmax-Tuple{Any}","page":"Utils","title":"ReinforcementLearning.Utils.findallmax","text":"findallmax(A::AbstractArray)\n\nLike findmax, but all the indices of the maximum value are returned.\n\nwarning: Warning\nAll elements of value NaN in A will be ignored, unless all elements are NaN. In that case, the returned maximum value will be NaN and the returned indices will be collect(1:length(A))\n\n#Examples\n\njulia> findallmax([-Inf, -Inf, -Inf])\n(-Inf, [1, 2, 3])\n\njulia> findallmax([Inf, Inf, Inf])\n(Inf, [1, 2, 3])\n\njulia> findallmax([Inf, 0, Inf])\n(Inf, [1, 3])\n\njulia> findallmax([0,1,2,1,2,1,0])\n(2, [3, 5])\n\n\n\n\n\n","category":"method"},{"location":"utils/#ReinforcementLearning.Utils.huber_loss-Tuple{Any,Any}","page":"Utils","title":"ReinforcementLearning.Utils.huber_loss","text":"huber_loss(labels, predictions;δ = 1.0)\n\nSee huber_loss and the implementation in TensorFlow.\n\nwarning: Warning\nThe return is not reduced!\n\n\n\n\n\n","category":"method"},{"location":"utils/#Base.:--Tuple{Tiling,Any}","page":"Utils","title":"Base.:-","text":"(-)(t::Tiling, xs)\n\nShift t along each dimension by each element in xs.\n\n\n\n\n\n","category":"method"},{"location":"utils/#ReinforcementLearning.Utils.get-Tuple{SumTree,Any}","page":"Utils","title":"ReinforcementLearning.Utils.get","text":"!!! this is unsafe, always check the real_ind, or you may get bound error in some extreme cases.\n\n\n\n\n\n","category":"method"},{"location":"core/#Core-1","page":"Core","title":"Core","text":"","category":"section"},{"location":"core/#","page":"Core","title":"Core","text":"run\npolicy_evaluation!\npolicy_improvement!\npolicy_iteration!\nvalue_iteration!","category":"page"},{"location":"core/#Base.run","page":"Core","title":"Base.run","text":"run(agent::AbstractAgent, env::AbstractEnv, stop_condition; hook = EmptyHook())\n\nSee also Stop Conditions, Hooks\n\n\n\n\n\n","category":"function"},{"location":"core/#ReinforcementLearning.policy_evaluation!","page":"Core","title":"ReinforcementLearning.policy_evaluation!","text":"policy_evaluation!(V::AbstractVApproximator, π, model::AbstractDistributionBasedModel; γ::Float64=0.9, θ::Float64=1e-4)\n\nSee more details at Section (4.1) on Page 75 of the book Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\n\n\n\n\n","category":"function"},{"location":"core/#ReinforcementLearning.policy_improvement!","page":"Core","title":"ReinforcementLearning.policy_improvement!","text":"policy_improvement!(;V::AbstractVApproximator, π::AbstractPolicy, model::AbstractDistributionBasedModel, γ::Float64 = 0.9)\n\nSee more details at Section (4.2) on Page 76 of the book Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\n\n\n\n\n","category":"function"},{"location":"core/#ReinforcementLearning.policy_iteration!","page":"Core","title":"ReinforcementLearning.policy_iteration!","text":"policy_iteration!(V::AbstractVApproximator, π, model::AbstractDistributionBasedModel; γ::Float64=0.9, θ::Float64=1e-4, max_iter=typemax(Int))\n\nSee more details at Section (4.3) on Page 80 of the book Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\n\n\n\n\n","category":"function"},{"location":"core/#ReinforcementLearning.value_iteration!","page":"Core","title":"ReinforcementLearning.value_iteration!","text":"value_iteration!(V::AbstractVApproximator, model::AbstractDistributionBasedModel; γ::Float64=0.9, θ::Float64=1e-4, max_iter=typemax(Int))\n\nSee more details at Section (4.4) on Page 83 of the book Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\n\n\n\n\n","category":"function"},{"location":"core/#Stop-Conditions-1","page":"Core","title":"Stop Conditions","text":"","category":"section"},{"location":"core/#","page":"Core","title":"Core","text":"StopAfterStep\nStopAfterEpisode\nStopWhenDone\nComposedStopCondition","category":"page"},{"location":"core/#ReinforcementLearning.StopAfterStep","page":"Core","title":"ReinforcementLearning.StopAfterStep","text":"StopAfterStep(step; cur = 1, is_show_progress = true, tag = \"TRAINING\")\n\nReturn true after being called for step.\n\n\n\n\n\n","category":"type"},{"location":"core/#ReinforcementLearning.StopAfterEpisode","page":"Core","title":"ReinforcementLearning.StopAfterEpisode","text":"StopAfterEpisode(episode; cur = 0, is_show_progress = true, tag = \"TRAINING\")\n\nReturn true after being called episode. If is_show_progress is true, the ProgressMeter will be used to show progress.\n\n\n\n\n\n","category":"type"},{"location":"core/#ReinforcementLearning.StopWhenDone","page":"Core","title":"ReinforcementLearning.StopWhenDone","text":"StopWhenDone()\n\nReturn true if the terminal field of an observation is true.\n\n\n\n\n\n","category":"type"},{"location":"core/#ReinforcementLearning.ComposedStopCondition","page":"Core","title":"ReinforcementLearning.ComposedStopCondition","text":"ComposedStopCondition(stop_conditions; reducer = any)\n\nThe result of stop_conditions is reduced by reducer.\n\n\n\n\n\n","category":"type"},{"location":"core/#Hooks-1","page":"Core","title":"Hooks","text":"","category":"section"},{"location":"core/#","page":"Core","title":"Core","text":"AbstractHook\nComposedHook\nEmptyHook\nStepsPerEpisode\nRewardsPerEpisode\nTotalRewardPerEpisode\nCumulativeReward\nTimePerStep","category":"page"},{"location":"core/#ReinforcementLearning.AbstractHook","page":"Core","title":"ReinforcementLearning.AbstractHook","text":"A hook is called at different stage duiring a run. One can inject customized runtime logic in it.\n\n\n\n\n\n","category":"type"},{"location":"core/#ReinforcementLearning.ComposedHook","page":"Core","title":"ReinforcementLearning.ComposedHook","text":"ComposedHook(hooks...)\n\nCompose different hooks into a single hook.\n\n\n\n\n\n","category":"type"},{"location":"core/#ReinforcementLearning.EmptyHook","page":"Core","title":"ReinforcementLearning.EmptyHook","text":"Do nothing\n\n\n\n\n\n","category":"type"},{"location":"core/#ReinforcementLearning.StepsPerEpisode","page":"Core","title":"ReinforcementLearning.StepsPerEpisode","text":"StepsPerEpisode(; steps = Int[], count = 0, tag = \"TRAINING\")\n\nStore steps of each episode in the field of steps.\n\n\n\n\n\n","category":"type"},{"location":"core/#ReinforcementLearning.RewardsPerEpisode","page":"Core","title":"ReinforcementLearning.RewardsPerEpisode","text":"RewardsPerEpisode(; rewards = Vector{Vector{Float64}}(), tag = \"TRAINING\")\n\nStore each reward of each step in every episode in the field of rewards.\n\n\n\n\n\n","category":"type"},{"location":"core/#ReinforcementLearning.TotalRewardPerEpisode","page":"Core","title":"ReinforcementLearning.TotalRewardPerEpisode","text":"TotalRewardPerEpisode(; rewards = Float64[], reward = 0.0, tag = \"TRAINING\")\n\nStore the total rewards of each episode in the field of rewards.\n\n\n\n\n\n","category":"type"},{"location":"core/#ReinforcementLearning.CumulativeReward","page":"Core","title":"ReinforcementLearning.CumulativeReward","text":"CumulativeReward(rewards::Vector{Float64} = [0.0], tag::String = \"TRAINING\")\n\nStore cumulative rewards since the beginning to the field of rewards.\n\n\n\n\n\n","category":"type"},{"location":"core/#ReinforcementLearning.TimePerStep","page":"Core","title":"ReinforcementLearning.TimePerStep","text":"TimePerStep(;max_steps=100)\nTimePerStep(times::CircularArrayBuffer{Float64, 1}, t::UInt64)\n\nStore time cost of the latest max_steps in the times field.\n\n\n\n\n\n","category":"type"},{"location":"a_quick_example/#A-Quick-Example-1","page":"A Quick Example","title":"A Quick Example","text":"","category":"section"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"Welcome to the world of reinforcement learning in Julia! Here's a quick example to show you how to train an agent with a BasicDQNLearner to play the CartPoleEnv.","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"note: Note\nNotice that a lot of dependent packages are under rapid development. To make sure that you can reproduce the result in this example, you are suggested to:Make sure that your Julia version is v1.3-rc3 or above\nClone git@github.com:JuliaReinforcementLearning/ReinforcementLearning.jl.git\ncd ReinforcementLearning.jl\njulia --project=docs\n]instantiate","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"First, let's make sure that running the following code will not trigger any error:","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"import Random # hide\nRandom.seed!(1) # hide\n\nusing ReinforcementLearning, ReinforcementLearningEnvironments, Flux\nusing StatsBase:mean","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"Cartpole is considered to be one of the simplest environments for DRL(Deep Reinforcement Learning) algorithms testing. The state of the Cartpole environment can be described with 4 numbers and the actions are two integers(1 and 2). Before game terminates, agent can gain a reward of +1 for each step. And the game will be forced to end after 200 steps, thus the maximum reward of an episode is 200. ","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"env = CartPoleEnv(;T=Float32)\nns, na = length(observation_space(env)), length(action_space(env))  # (4, 2)","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"Then we can create an agent:","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"backend  = :Zygote\ndevice = :cpu\n\nagent = Agent(\n    π = QBasedPolicy(\n        learner = BasicDQNLearner(\n            approximator = NeuralNetworkQ(\n                model = Chain(\n                    Dense(ns, 128, relu; backend=backend),\n                    Dense(128, 128, relu; backend=backend),\n                    Dense(128, na; backend=backend)\n                    ),\n                optimizer = ADAM(),\n                device = device\n            ),\n            batch_size = 32,\n            min_replay_history = 100,\n            loss_fun = huber_loss,\n        ),\n        selector = EpsilonGreedySelector{:exp}(ϵ_stable = 0.01, decay_steps = 500),\n    ),\n    buffer = circular_RTSA_buffer(\n        capacity = 1000,\n        state_eltype = Float32,\n        state_size = (ns,),\n    )\n)","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"Relax! We promise that all the new concepts above will be explained in detail later.","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"For now, you only need to know that an Agent is usually composed by a policy and a buffer. Here we are using a very common QBasedPolicy and a circular_RTSA_buffer. For a QBasedPolicy we need to provide a learner and a selector. The learner here is used to provide the value estimations of all actions in a step, and the selector is use to select an action based on those estimations. For a buffer, it stores some transitions between an agent and an environment and is used to improve the policy. That's all!","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"To record the reward and performance , we need some hooks:","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"hook = ComposedHook(\n    TotalRewardPerEpisode(),\n    TimePerStep()\n)","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"And finally, let's push the button:","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"run(agent, env, StopAfterStep(10000; is_show_progress=false); hook = hook)\n\nprint(\"\"\"\n    backend = $backend, device = $device\n    avg_reward = $(mean(hook[1].rewards))\n    avg_fps = $(1/mean(hook[2].times))\n    \"\"\")","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"We can also plot the rewards stored in our hook:","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"using Plots\nplot(hook[1].rewards, xlabel=\"Episode\", ylabel=\"Reward\", label=\"\")\nsavefig(\"a_quick_example_cartpole_cpu_basic_dqn.png\"); nothing # hide","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"(Image: )","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"That's fantastic!","category":"page"},{"location":"a_quick_example/#","page":"A Quick Example","title":"A Quick Example","text":"\"But I'm new to Julia and RL. Can I learn RL by using this package?\"\nYes! One of this package's main goals is to be educational. Reinforcement Learning: An Introduction is a good introductory book. And we reproduce almost all the examples mentioned in that book by using this package here.\n\"What if I have a solid background in RL but new to Julia?\"\nProgramming isn't hard. Programming well is very hard!  - CS 3110\nFortunately, Julia provides some amazing features together with many awesome packages to make things much easier. We provide a Tips for Developers section to help you grasp Julia in depth.\n\"I'm experienced in both Julia and RL. But I find it hard to use this package...\"\nAlthough we tried our best to make concepts and codes as simple as possible, it is still possible that they are not very intuitive enough. So do not hesitate to JOIN US (create an issue or a PR). We need YOU to improve all this stuff together!","category":"page"},{"location":"overview/#Overview-1","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"overview/#","page":"Overview","title":"Overview","text":"Before diving into details, let's review some basic concepts in RL(Reinforcement Learning) first. Then we'll gradually introduce the relationship between those concepts and our implementations in this package.","category":"page"},{"location":"overview/#Key-Concepts-1","page":"Overview","title":"Key Concepts","text":"","category":"section"},{"location":"overview/#Agent-and-Environment-1","page":"Overview","title":"Agent and Environment","text":"","category":"section"},{"location":"overview/#","page":"Overview","title":"Overview","text":"(Image: agent_environment_relation)","category":"page"},{"location":"overview/#","page":"Overview","title":"Overview","text":"Generally speaking, RL is to learn how to take actions so as to maximize a numerical reward. Two core concepts in RL are Agent and Environment. In each step, the agent is provided with some observation of the environment and is required to take an action. Then the environment takes in that action and transites to another state, providing a numerical reward in the meantime.","category":"page"},{"location":"overview/#","page":"Overview","title":"Overview","text":"In our package, Agent is an abstract type of AbstractAgent. And Environment is an abstract type of AbstractEnvironment provided in another package named ReinforcementLearningEnvironments.jl. We can observe an environment to get an Observation and interact! with an environment using an action. Usually, agents and environments are functional objects. So we can use the piping operator (|>) to simulate the steps implied in the above picture: env |> observe |> agent |> env. See Agents and Environments for more some concrete implementations.","category":"page"},{"location":"overview/#","page":"Overview","title":"Overview","text":"(Image: multi_agent)","category":"page"},{"location":"overview/#","page":"Overview","title":"Overview","text":"For multi-agent environments, an AgentManager is introduced to manage the interactions between agents and environments (for now it's just a Tuple).","category":"page"},{"location":"overview/#","page":"Overview","title":"Overview","text":"Now let's take a closer look at Agent:","category":"page"},{"location":"overview/#","page":"Overview","title":"Overview","text":"(Image: agent_in_detail)","category":"page"},{"location":"overview/#","page":"Overview","title":"Overview","text":"This is a typical hierarchical chart of different components. After getting an Observation from the envrionment, the Agents use it to fill Buffers and update Environment Models and Policies. A policy is used to generate an action given an Observation. Usually a policy contains Learners to decide how to update internal Approximators. A typical approximator is NeuralNetworkQ, which uses neural networks to approximator state-action values.","category":"page"},{"location":"overview/#","page":"Overview","title":"Overview","text":"Notice that different components may have different implementations, so some steps are optional. Anyway, this picture should give you a perceptual knowledge of how those components are organized. You can move on and read the introduction of each components for more details.","category":"page"},{"location":"components/environment_models/#Environment-Models-1","page":"Environment Models","title":"Environment Models","text":"","category":"section"},{"location":"components/environment_models/#","page":"Environment Models","title":"Environment Models","text":"AbstractEnvironmentModel \nAbstractSampleBasedModel \nAbstractDistributionBasedModel \nDynamicDistributionModel\nDeterministicDistributionModel \nExperienceBasedSampleModel \nPrioritizedSweepingSampleModel \nTimeBasedSampleModel","category":"page"},{"location":"components/environment_models/#ReinforcementLearning.AbstractEnvironmentModel","page":"Environment Models","title":"ReinforcementLearning.AbstractEnvironmentModel","text":"Describe how to model a reinforcement learning environment.\n\nSee also AbstractDistributionBasedModel, AbstractSampleBasedModel.\n\n\n\n\n\n","category":"type"},{"location":"components/environment_models/#ReinforcementLearning.AbstractSampleBasedModel","page":"Environment Models","title":"ReinforcementLearning.AbstractSampleBasedModel","text":"A collection of models that can be used to sample transitions.\n\n\n\n\n\n","category":"type"},{"location":"components/environment_models/#ReinforcementLearning.AbstractDistributionBasedModel","page":"Environment Models","title":"ReinforcementLearning.AbstractDistributionBasedModel","text":"A collection of models that can get the distribution given a state and an actioi.\n\n\n\n\n\n","category":"type"},{"location":"components/environment_models/#ReinforcementLearning.DynamicDistributionModel","page":"Environment Models","title":"ReinforcementLearning.DynamicDistributionModel","text":"DynamicDistributionModel(f::Tf, ns::Int, na::Int) -> DynamicDistributionModel{Tf}\n\nUse a general function f to store the transformations. ns and na are the number of states and actions.\n\n\n\n\n\n","category":"type"},{"location":"components/environment_models/#ReinforcementLearning.DeterministicDistributionModel","page":"Environment Models","title":"ReinforcementLearning.DeterministicDistributionModel","text":"DeterministicDistributionModel(table::Array{Vector{NamedTuple{(:nextstate, :reward, :prob),Tuple{Int,Float64,Float64}}}, 2})\n\nStore all the transformations in the table field.\n\n\n\n\n\n","category":"type"},{"location":"components/environment_models/#ReinforcementLearning.ExperienceBasedSampleModel","page":"Environment Models","title":"ReinforcementLearning.ExperienceBasedSampleModel","text":"ExperienceBasedSampleModel() -> ExperienceBasedSampleModel\n\nGenerate a transition based on previous experiences.\n\n\n\n\n\n","category":"type"},{"location":"components/environment_models/#ReinforcementLearning.PrioritizedSweepingSampleModel","page":"Environment Models","title":"ReinforcementLearning.PrioritizedSweepingSampleModel","text":"PrioritizedSweepingSampleModel(θ::Float64=1e-4)\n\nSee more details at Section (8.4) on Page 168 of the book Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\n\n\n\n\n","category":"type"},{"location":"components/environment_models/#ReinforcementLearning.TimeBasedSampleModel","page":"Environment Models","title":"ReinforcementLearning.TimeBasedSampleModel","text":"TimeBasedSampleModel(nactions::Int, κ::Float64 = 1e-4)\n\n\n\n\n\n","category":"type"},{"location":"#","page":"Home","title":"Home","text":"(Image: logo)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"ReinforcementLearning.jl, as the name says, is a package for reinforcement learning research in Julia.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Our design principles are:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Reusability and extensibility: Provide elaborately designed components and interfaces to help users implement new algorithms.\nEasy experimentation: Make it easy for new users to run benchmark experiments, compare different algorithms, evaluate and diagnose agents.\nReproducibility: Facilitate reproducibility from traditional tabular methods to modern deep reinforcement learning algorithms.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Key capabilities/features include:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Well tested traditional methods:\nTDLearner\nDifferentialTDLearner\nTDλReturnLearner\nDoubleLearner\nMonteCarloLearner\nGradientBanditLearner\nReinforcePolicy\nEfficiently implemented deep reinforcement learning algorithms:\nDeep Q-Learning:\nBasicDQNLearner\nDQNLearner\nPrioritizedDQNLearner\nRainbowLearner\nPluggable deep learning framework backend:\nFlux.jl\nKnet.jl\nBuilt-in TensorBoard support.","category":"page"},{"location":"#Installation-1","page":"Home","title":"Installation","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"This package can be installed from the package manager in Julia's REPL:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"] add ReinforcementLearning","category":"page"},{"location":"#","page":"Home","title":"Home","text":"note: Note\nThis package relies on many new features introduced since Julia 1.3, so make sure that you have the proper Julia version installed. Considering that this package is still under rapid development, you're strongly suggested to install the master branch of this package by the following steps:Make sure that your Julia version is v1.3-rc3 or above\ngit clone git@github.com:JuliaReinforcementLearning/ReinforcementLearning.jl.git\ncd ReinforcementLearning.jl\njulia --project=.\n] instantiate","category":"page"},{"location":"#","page":"Home","title":"Home","text":"To play with some common reinforcement learning environments, you may also want to install:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"] add ReinforcementLearningEnvironments","category":"page"},{"location":"experiments/atari_dqn/#Playing-the-Atari-Games-with-DQN-1","page":"Play Atari Games with DQN","title":"Playing the Atari Games with DQN","text":"","category":"section"},{"location":"experiments/atari_dqn/#","page":"Play Atari Games with DQN","title":"Play Atari Games with DQN","text":"Here we provide an example to show you how to play the Atari games with DQN. First, let's install some required packages:","category":"page"},{"location":"experiments/atari_dqn/#","page":"Play Atari Games with DQN","title":"Play Atari Games with DQN","text":"] add ArcadeLearningEnvironment. Please note that this package only works on Linux systems. For now, it doesn't use the BinaryBuilder.jl and you have to make sure that some necessary building tools are properly installed(Good First Issue!!!). If you are using Ubuntu, you are lucky. sudo apt-get install -y --no-install-recommends cmake build-essential libz-dev unzip should be enough.\n] add ReinforcementLearningEnvironments. This package provides some unified interfaces.\n] add ReinforcementLearning#master. This package is still under rapid development, so we need to install the master branch.\n] add CuArrays. To enable the GPU support.\n] add Flux#master. We need the master branch of Flux to use Zygote.jl for backprop.\n] add Plots. To plot the rewards.","category":"page"},{"location":"experiments/atari_dqn/#","page":"Play Atari Games with DQN","title":"Play Atari Games with DQN","text":"Now we can try to train a DQN agent.","category":"page"},{"location":"experiments/atari_dqn/#","page":"Play Atari Games with DQN","title":"Play Atari Games with DQN","text":"using ReinforcementLearning, ReinforcementLearningEnvironments, ArcadeLearningEnvironment, Flux, Plots\n\nusing Random\nRandom.seed!(11)\n\nstate_size = (84, 84)\nn_frames = 4\n\nenv = WrappedEnv(\n    env = AtariEnv(;\n        name=\"pong\",\n        grayscale_obs=true,\n        noop_max=30,\n        frame_skip=4,\n        terminal_on_life_loss=false,\n        repeat_action_probability=0.,\n        max_num_frames_per_episode=4 * 100000,\n        color_averaging=false,\n        full_action_space=false,\n        seed=(22, 33)\n        ),\n    preprocessor = Chain(\n        ImageResize(84, 84),\n        StackFrames(state_size..., n_frames)\n    )\n)\n\nna = length(action_space(env))\n\ndevice = :gpu\n\nagent = Agent(\n    π = QBasedPolicy(\n        learner = DQNLearner(\n            approximator = NeuralNetworkQ(\n                model = Chain(\n                    x -> x ./ 255,\n                    Conv((8,8), n_frames => 32, relu; stride=4),\n                    Conv((4,4), 32 => 64, relu; stride=2),\n                    Conv((3,3), 64 => 64, relu; stride=1),\n                    x -> reshape(x, :, size(x)[end]),\n                    Dense(7*7*64, 512, relu),\n                    Dense(512, na),\n                    ),\n                optimizer = ADAM(0.00001),\n                device = device\n            ),\n            target_approximator = NeuralNetworkQ(\n                model = Chain(\n                    x -> x ./ 255,\n                    Conv((8,8), n_frames => 32, relu; stride=4),\n                    Conv((4,4), 32 => 64, relu; stride=2),\n                    Conv((3,3), 64 => 64, relu; stride=1),\n                    x -> reshape(x, :, size(x)[end]),\n                    Dense(7*7*64, 512, relu),\n                    Dense(512, na),\n                    ),\n                optimizer = ADAM(0.00001),\n                device = device\n            ),\n            update_freq = 4,\n            γ = 0.99f0,\n            update_horizon = 1,\n            batch_size = 32,\n            stack_size = n_frames,\n            min_replay_history = 10000,\n            loss_fun = huber_loss,\n            target_update_freq = 1000,\n        ),\n        selector = EpsilonGreedySelector{:exp}(ϵ_init=1.0, ϵ_stable = 0.01, decay_steps = 30000),\n    ),\n    buffer = circular_RTSA_buffer(\n        capacity = 100000,\n        state_eltype = Float32,\n        state_size = state_size,\n    )\n)\n\nhook = ComposedHook(\n    TotalRewardPerEpisode(),\n    TimePerStep()\n);\n\nrun(agent, env, StopAfterStep(3000000; is_show_progress=true); hook = hook)","category":"page"},{"location":"experiments/atari_dqn/#","page":"Play Atari Games with DQN","title":"Play Atari Games with DQN","text":"Finally we can plot the rewards of each episode:","category":"page"},{"location":"experiments/atari_dqn/#","page":"Play Atari Games with DQN","title":"Play Atari Games with DQN","text":"plot(hook[1].rewards, xlabel=\"Episode\", ylabel=\"Reward\", label=\"\")","category":"page"},{"location":"experiments/atari_dqn/#","page":"Play Atari Games with DQN","title":"Play Atari Games with DQN","text":"(Image: )","category":"page"}]
}
