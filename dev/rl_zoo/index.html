<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>RLZoo · ReinforcementLearning.jl</title><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-149861753-1', 'auto');
ga('send', 'pageview', {'page': location.pathname + location.search + location.hash});
</script><link rel="canonical" href="https://juliareinforcementlearning.github.io/ReinforcementLearning.jl/latest/rl_zoo/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../assets/custom.css" rel="stylesheet" type="text/css"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="ReinforcementLearning.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">ReinforcementLearning.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../overview/">Overview</a></li><li><a class="tocitem" href="../a_quick_example/">A Quick Example</a></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../rl_base/">RLBase</a></li><li><a class="tocitem" href="../rl_core/">RLCore</a></li><li><a class="tocitem" href="../rl_envs/">RLEnvs</a></li><li class="is-active"><a class="tocitem" href>RLZoo</a></li></ul></li><li><a class="tocitem" href="../tips_for_developers/">Tips for Developers</a></li><li><span class="tocitem">Experiments</span><ul><li><a class="tocitem" href="../experiments/atari_dqn/">Play Atari Games with DQN</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Manual</a></li><li class="is-active"><a href>RLZoo</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>RLZoo</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/master/docs/src/rl_zoo.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="ReinforcementLearningZoo.jl-1"><a class="docs-heading-anchor" href="#ReinforcementLearningZoo.jl-1">ReinforcementLearningZoo.jl</a><a class="docs-heading-anchor-permalink" href="#ReinforcementLearningZoo.jl-1" title="Permalink"></a></h1><p><a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningZoo.jl">ReinforcementLearningZoo.jl</a> (<strong>RLZoo</strong>) provides some implementations of the most typical reinforcement learning algorithms.</p><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.BasicDQNLearner" href="#ReinforcementLearningZoo.BasicDQNLearner"><code>ReinforcementLearningZoo.BasicDQNLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">BasicDQNLearner(;kwargs...)</code></pre><p>See paper: <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Playing Atari with Deep Reinforcement Learning</a></p><p>This is the very basic implementation of DQN. Compared to the traditional Q learning, the only difference is that, in the updating step it uses a batch of transitions sampled from an experience buffer instead of current transition. And the <code>approximator</code> is usually a <a href="../rl_core/#ReinforcementLearningCore.NeuralNetworkApproximator"><code>NeuralNetworkApproximator</code></a>. You can start from this implementation to understand how everything is organized and how to write your own customized algorithm.</p><p><strong>Keywords</strong></p><ul><li><code>approximator</code>::<a href="../rl_base/#ReinforcementLearningBase.AbstractApproximator"><code>AbstractApproximator</code></a>: used to get Q-values of a state.</li><li><code>loss_func</code>: the loss function to use. TODO: provide a default <a href="../rl_core/#ReinforcementLearningCore.huber_loss"><code>huber_loss</code></a>?</li><li><code>γ::Float32=0.99f0</code>: discount rate.</li><li><code>batch_size::Int=32</code></li><li><code>min_replay_history::Int=32</code>: number of transitions that should be experienced before updating the <code>approximator</code>.</li><li><code>seed=nothing</code>.</li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.DQNLearner" href="#ReinforcementLearningZoo.DQNLearner"><code>ReinforcementLearningZoo.DQNLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">DQNLearner(;kwargs...)</code></pre><p>See paper: <a href="https://www.nature.com/articles/nature14236">Human-level control through deep reinforcement learning</a></p><p><strong>Keywords</strong></p><ul><li><code>approximator</code>::<a href="../rl_base/#ReinforcementLearningBase.AbstractApproximator"><code>AbstractApproximator</code></a>: used to get Q-values of a state.</li><li><code>target_approximator</code>::<a href="../rl_base/#ReinforcementLearningBase.AbstractApproximator"><code>AbstractApproximator</code></a>: similar to <code>approximator</code>, but used to estimate the target (the next state).</li><li><code>loss_func</code>: the loss function.</li><li><code>γ::Float32=0.99f0</code>: discount rate.</li><li><code>batch_size::Int=32</code></li><li><code>update_horizon::Int=1</code>: length of update (&#39;n&#39; in n-step update).</li><li><code>min_replay_history::Int=32</code>: number of transitions that should be experienced before updating the <code>approximator</code>.</li><li><code>update_freq::Int=4</code>: the frequency of updating the <code>approximator</code>.</li><li><code>target_update_freq::Int=100</code>: the frequency of syncing <code>target_approximator</code>.</li><li><code>stack_size::Union{Int, Nothing}=4</code>: use the recent <code>stack_size</code> frames to form a stacked state.</li><li><code>seed = nothing</code></li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.PrioritizedDQNLearner" href="#ReinforcementLearningZoo.PrioritizedDQNLearner"><code>ReinforcementLearningZoo.PrioritizedDQNLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">PrioritizedDQNLearner(;kwargs...)</code></pre><p>See paper: <a href="https://arxiv.org/abs/1511.05952">Prioritized Experience Replay</a></p><p><strong>Keywords</strong></p><ul><li><code>approximator</code>::<a href="../rl_base/#ReinforcementLearningBase.AbstractApproximator"><code>AbstractApproximator</code></a>: used to get Q-values of a state.</li><li><code>target_approximator</code>::<a href="../rl_base/#ReinforcementLearningBase.AbstractApproximator"><code>AbstractApproximator</code></a>: similar to <code>approximator</code>, but used to estimate the target (the next state).</li><li><code>loss_func</code>: the loss function.</li><li><code>γ::Float32=0.99f0</code>: discount rate.</li><li><code>batch_size::Int=32</code></li><li><code>update_horizon::Int=1</code>: length of update (&#39;n&#39; in n-step update).</li><li><code>min_replay_history::Int=32</code>: number of transitions that should be experienced before updating the <code>approximator</code>.</li><li><code>update_freq::Int=4</code>: the frequency of updating the <code>approximator</code>.</li><li><code>target_update_freq::Int=100</code>: the frequency of syncing <code>target_approximator</code>.</li><li><code>stack_size::Union{Int, Nothing}=4</code>: use the recent <code>stack_size</code> frames to form a stacked state.</li><li><code>default_priority::Float64=100.</code>: the default priority for newly added transitions.</li><li><code>seed = nothing</code></li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.RainbowLearner" href="#ReinforcementLearningZoo.RainbowLearner"><code>ReinforcementLearningZoo.RainbowLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">RainbowLearner(;kwargs...)</code></pre><p>See paper: <a href="https://arxiv.org/abs/1710.02298">Rainbow: Combining Improvements in Deep Reinforcement Learning</a></p><p><strong>Keywords</strong></p><ul><li><code>approximator</code>::<a href="../rl_base/#ReinforcementLearningBase.AbstractApproximator"><code>AbstractApproximator</code></a>: used to get Q-values of a state.</li><li><code>target_approximator</code>::<a href="../rl_base/#ReinforcementLearningBase.AbstractApproximator"><code>AbstractApproximator</code></a>: similar to <code>approximator</code>, but used to estimate the target (the next state).</li><li><code>loss_func</code>: the loss function.</li><li><code>Vₘₐₓ::Float32</code>: the maximum value of distribution.</li><li><code>Vₘᵢₙ::Float32</code>: the minimum value of distribution.</li><li><code>n_actions::Int</code>: number of possible actions.</li><li><code>γ::Float32=0.99f0</code>: discount rate.</li><li><code>batch_size::Int=32</code></li><li><code>update_horizon::Int=1</code>: length of update (&#39;n&#39; in n-step update).</li><li><code>min_replay_history::Int=32</code>: number of transitions that should be experienced before updating the <code>approximator</code>.</li><li><code>update_freq::Int=4</code>: the frequency of updating the <code>approximator</code>.</li><li><code>target_update_freq::Int=500</code>: the frequency of syncing <code>target_approximator</code>.</li><li><code>stack_size::Union{Int, Nothing}=4</code>: use the recent <code>stack_size</code> frames to form a stacked state.</li><li><code>default_priority::Float64=100.</code>: the default priority for newly added transitions.</li><li><code>n_atoms::Int=51</code>: the number of buckets of the value function distribution.</li><li><code>stack_size::Union{Int, Nothing}=4</code>: use the recent <code>stack_size</code> frames to form a stacked state.</li><li><code>default_priority::Float64=100.</code>: the default priority for newly added transitions.</li><li><code>seed = nothing</code></li></ul></div></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../rl_envs/">« RLEnvs</a><a class="docs-footer-nextpage" href="../tips_for_developers/">Tips for Developers »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Thursday 23 April 2020 15:35">Thursday 23 April 2020</span>. Using Julia version 1.3.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
