<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>RLZoo · ReinforcementLearning.jl</title><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-149861753-1', 'auto');
ga('send', 'pageview', {'page': location.pathname + location.search + location.hash});
</script><link rel="canonical" href="https://juliareinforcementlearning.github.io/ReinforcementLearning.jl/latest/rl_zoo/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../assets/custom.css" rel="stylesheet" type="text/css"/><link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" rel="stylesheet" type="text/css"/></head><body><div id="top" class="navbar-wrapper">
<nav class="navbar navbar-expand-lg  navbar-dark fixed-top" style="background-color: #1fd1f9; background-image: linear-gradient(315deg, #1fd1f9 0%, #b621fe 74%); " id="mainNav">
  <div class="container-md">
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarTogglerDemo01" aria-controls="navbarTogglerDemo01" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>
  <div class="collapse navbar-collapse" id="navbarTogglerDemo01">
    <span class="navbar-brand">
        <a class="navbar-brand" href="/">
          <!-- <img src="/assets/site/logo.svg" width="30" height="30" alt="logo" loading="lazy"> -->
          JuliaReinforcementLearning
        </a>
    </span>

    <ul class="navbar-nav ml-auto">
        <li class="nav-item">
        <a class="nav-link" href="/get_started/">Get Started</a>
        </li>
        <li class="nav-item">
        <a class="nav-link" href="/guide/">Guide</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/contribute/">Contribute</a>
        </li>
        <li class="nav-item">
        <a class="nav-link" href="/blog/">Blog</a>
        </li>
        <li class="nav-item">
        <a class="nav-link" href="https://JuliaReinforcementLearning.github.io/ReinforcementLearning.jl/latest/">Doc</a>
        </li>
        <li class="nav-item">
        <a class="nav-link" href="https://github.com/JuliaReinforcementLearning">Github</a>
        </li>
    </ul>
  </div>
</nav>
</div>
<div class="documenter-wrapper" id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="ReinforcementLearning.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">ReinforcementLearning.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../rl_base/">RLBase</a></li><li><a class="tocitem" href="../rl_core/">RLCore</a></li><li><a class="tocitem" href="../rl_envs/">RLEnvs</a></li><li class="is-active"><a class="tocitem" href>RLZoo</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Manual</a></li><li class="is-active"><a href>RLZoo</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>RLZoo</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/master/docs/src/rl_zoo.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="ReinforcementLearningZoo.jl"><a class="docs-heading-anchor" href="#ReinforcementLearningZoo.jl">ReinforcementLearningZoo.jl</a><a id="ReinforcementLearningZoo.jl-1"></a><a class="docs-heading-anchor-permalink" href="#ReinforcementLearningZoo.jl" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.A2CGAELearner" href="#ReinforcementLearningZoo.A2CGAELearner"><code>ReinforcementLearningZoo.A2CGAELearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">A2CGAELearner(;kwargs...)</code></pre><p><strong>Keyword arguments</strong></p><ul><li><code>approximator</code>, an <a href="../rl_core/#ReinforcementLearningCore.ActorCritic"><code>ActorCritic</code></a> based <a href="../rl_core/#ReinforcementLearningCore.NeuralNetworkApproximator"><code>NeuralNetworkApproximator</code></a></li><li><code>γ::Float32</code>, reward discount rate.</li><li>&#39;λ::Float32&#39;, lambda for GAE-lambda</li><li><code>actor_loss_weight::Float32</code></li><li><code>critic_loss_weight::Float32</code></li><li><code>entropy_loss_weight::Float32</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L9">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.A2CLearner" href="#ReinforcementLearningZoo.A2CLearner"><code>ReinforcementLearningZoo.A2CLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">A2CLearner(;kwargs...)</code></pre><p><strong>Keyword arguments</strong></p><ul><li><code>approximator</code>::<a href="../rl_core/#ReinforcementLearningCore.ActorCritic"><code>ActorCritic</code></a></li><li><code>γ::Float32</code>, reward discount rate.</li><li><code>actor_loss_weight::Float32</code></li><li><code>critic_loss_weight::Float32</code></li><li><code>entropy_loss_weight::Float32</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L10">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.BasicDQNLearner" href="#ReinforcementLearningZoo.BasicDQNLearner"><code>ReinforcementLearningZoo.BasicDQNLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">BasicDQNLearner(;kwargs...)</code></pre><p>See paper: <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Playing Atari with Deep Reinforcement Learning</a></p><p>This is the very basic implementation of DQN. Compared to the traditional Q learning, the only difference is that, in the updating step it uses a batch of transitions sampled from an experience buffer instead of current transition. And the <code>approximator</code> is usually a <a href="../rl_core/#ReinforcementLearningCore.NeuralNetworkApproximator"><code>NeuralNetworkApproximator</code></a>. You can start from this implementation to understand how everything is organized and how to write your own customized algorithm.</p><p><strong>Keywords</strong></p><ul><li><code>approximator</code>::<a href="../rl_core/#ReinforcementLearningCore.AbstractApproximator"><code>AbstractApproximator</code></a>: used to get Q-values of a state.</li><li><code>loss_func</code>: the loss function to use. TODO: provide a default <a href="../rl_core/#ReinforcementLearningCore.huber_loss-Tuple{Any,Any}"><code>huber_loss</code></a>?</li><li><code>γ::Float32=0.99f0</code>: discount rate.</li><li><code>batch_size::Int=32</code></li><li><code>min_replay_history::Int=32</code>: number of transitions that should be experienced before updating the <code>approximator</code>.</li><li><code>rng=Random.GLOBAL_RNG</code></li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.BestResponsePolicy-Tuple{Any,Any,Any}" href="#ReinforcementLearningZoo.BestResponsePolicy-Tuple{Any,Any,Any}"><code>ReinforcementLearningZoo.BestResponsePolicy</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">BestResponsePolicy(policy, env, best_responder)</code></pre><ul><li><code>policy</code>, the original policy to be wrapped in the best response policy.</li><li><code>env</code>, the environment to handle.</li><li><code>best_responder</code>, the player to choose best response action.</li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.DDPGPolicy-Tuple{}" href="#ReinforcementLearningZoo.DDPGPolicy-Tuple{}"><code>ReinforcementLearningZoo.DDPGPolicy</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">DDPGPolicy(;kwargs...)</code></pre><p><strong>Keyword arguments</strong></p><ul><li><code>behavior_actor</code>,</li><li><code>behavior_critic</code>,</li><li><code>target_actor</code>,</li><li><code>target_critic</code>,</li><li><code>start_policy</code>,</li><li><code>γ = 0.99f0</code>,</li><li><code>ρ = 0.995f0</code>,</li><li><code>batch_size = 32</code>,</li><li><code>start_steps = 10000</code>,</li><li><code>update_after = 1000</code>,</li><li><code>update_every = 50</code>,</li><li><code>act_limit = 1.0</code>,</li><li><code>act_noise = 0.1</code>,</li><li><code>step = 0</code>,</li><li><code>rng = Random.GLOBAL_RNG</code>,</li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.DQNLearner" href="#ReinforcementLearningZoo.DQNLearner"><code>ReinforcementLearningZoo.DQNLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">DQNLearner(;kwargs...)</code></pre><p>See paper: <a href="https://www.nature.com/articles/nature14236">Human-level control through deep reinforcement learning</a></p><p><strong>Keywords</strong></p><ul><li><code>approximator</code>::<a href="../rl_core/#ReinforcementLearningCore.AbstractApproximator"><code>AbstractApproximator</code></a>: used to get Q-values of a state.</li><li><code>target_approximator</code>::<a href="../rl_core/#ReinforcementLearningCore.AbstractApproximator"><code>AbstractApproximator</code></a>: similar to <code>approximator</code>, but used to estimate the target (the next state).</li><li><code>loss_func</code>: the loss function.</li><li><code>γ::Float32=0.99f0</code>: discount rate.</li><li><code>batch_size::Int=32</code></li><li><code>update_horizon::Int=1</code>: length of update (&#39;n&#39; in n-step update).</li><li><code>min_replay_history::Int=32</code>: number of transitions that should be experienced before updating the <code>approximator</code>.</li><li><code>update_freq::Int=4</code>: the frequency of updating the <code>approximator</code>.</li><li><code>target_update_freq::Int=100</code>: the frequency of syncing <code>target_approximator</code>.</li><li><code>stack_size::Union{Int, Nothing}=4</code>: use the recent <code>stack_size</code> frames to form a stacked state.</li><li><code>rng = Random.GLOBAL_RNG</code></li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.DQNLearner-Tuple{Any}" href="#ReinforcementLearningZoo.DQNLearner-Tuple{Any}"><code>ReinforcementLearningZoo.DQNLearner</code></a> — <span class="docstring-category">Method</span></header><section><div><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The state of the observation is assumed to have been stacked, if <code>!isnothing(stack_size)</code>.</p></div></div></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.DeepCFR" href="#ReinforcementLearningZoo.DeepCFR"><code>ReinforcementLearningZoo.DeepCFR</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">DeepCFR(;kwargs...)</code></pre><p>Symbols used here follow the paper: <a href="https://arxiv.org/abs/1811.00164">Deep Counterfactual Regret Minimization</a></p><p><strong>Keyword arguments</strong></p><ul><li><code>K</code>, number of traverrsal.</li><li><code>t</code>, number of iteration.</li><li><code>Π</code>, the policy network.</li><li><code>V</code>, a dictionary of each player&#39;s advantage network.</li><li><code>MΠ</code>, a strategy memory.</li><li><code>MV</code>, a dictionary of each player&#39;s advantage memory.</li><li><code>reinitialize_freq=1</code>, the frequency of reinitializing the value networks.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L14">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.ExternalSamplingMCCFRPolicy" href="#ReinforcementLearningZoo.ExternalSamplingMCCFRPolicy"><code>ReinforcementLearningZoo.ExternalSamplingMCCFRPolicy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ExternalSamplingMCCFRPolicy</code></pre><p>This implementation uses stochasticaly-weighted averaging.</p><p>Ref:</p><ul><li><a href="http://mlanctot.info/files/papers/PhD_Thesis_MarcLanctot.pdf">MONTE CARLO SAMPLING AND REGRET MINIMIZATION FOR EQUILIBRIUM COMPUTATION AND DECISION-MAKING IN LARGE EXTENSIVE FORM GAMES</a></li><li><a href="https://papers.nips.cc/paper/3713-monte-carlo-sampling-for-regret-minimization-in-extensive-games.pdf">Monte Carlo Sampling for Regret Minimization in Extensive Games</a></li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.GaussianNetwork" href="#ReinforcementLearningZoo.GaussianNetwork"><code>ReinforcementLearningZoo.GaussianNetwork</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">GaussianNetwork(;pre=identity, μ, σ)</code></pre><p><code>σ</code> should return the log of std, <code>exp</code> will be applied to it automatically.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L4">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.IQNLearner" href="#ReinforcementLearningZoo.IQNLearner"><code>ReinforcementLearningZoo.IQNLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">IQNLearner(;kwargs)</code></pre><p>See <a href="https://arxiv.org/abs/1806.06923">paper</a></p><p><strong>Keyworkd arugments</strong></p><ul><li><code>approximator</code>, a <a href="#ReinforcementLearningZoo.ImplicitQuantileNet"><code>ImplicitQuantileNet</code></a></li><li><code>target_approximator</code>, a <a href="#ReinforcementLearningZoo.ImplicitQuantileNet"><code>ImplicitQuantileNet</code></a>, must have the same structure as <code>approximator</code></li><li><code>κ = 1.0f0</code>,</li><li><code>N = 32</code>,</li><li><code>N′ = 32</code>,</li><li><code>Nₑₘ = 64</code>,</li><li><code>K = 32</code>,</li><li><code>γ = 0.99f0</code>,</li><li><code>stack_size = 4</code>,</li><li><code>batch_size = 32</code>,</li><li><code>update_horizon = 1</code>,</li><li><code>min_replay_history = 20000</code>,</li><li><code>update_freq = 4</code>,</li><li><code>target_update_freq = 8000</code>,</li><li><code>update_step = 0</code>,</li><li><code>default_priority = 1.0f2</code>,</li><li><code>β_priority = 0.5f0</code>,</li><li><code>rng = Random.GLOBAL_RNG</code>,</li><li><code>device_seed = nothing</code>,</li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.ImplicitQuantileNet" href="#ReinforcementLearningZoo.ImplicitQuantileNet"><code>ReinforcementLearningZoo.ImplicitQuantileNet</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ImplicitQuantileNet(;ψ, ϕ, header)</code></pre><pre><code class="language-none">        quantiles (n_action, n_quantiles, batch_size)
           ↑
         header
           ↑
feature ↱  ⨀   ↰ transformed embedding
       ψ       ϕ
       ↑       ↑
       s        τ</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L13">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.MinimaxPolicy" href="#ReinforcementLearningZoo.MinimaxPolicy"><code>ReinforcementLearningZoo.MinimaxPolicy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MinimaxPolicy(;value_function, depth::Int)</code></pre><p>The minimax algorithm with <a href="https://en.wikipedia.org/wiki/Alpha-beta_pruning">Alpha-beta pruning</a></p><p><strong>Keyword Arguments</strong></p><ul><li><code>maximum_depth::Int=30</code>, the maximum depth of search.</li><li><code>value_function=nothing</code>, estimate the value of <code>env</code>. <code>value_function(env) -&gt; Number</code>. It is only called after searching for <code>maximum_depth</code> and the <code>env</code> is not terminated yet.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L6">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.OutcomeSamplingMCCFRPolicy" href="#ReinforcementLearningZoo.OutcomeSamplingMCCFRPolicy"><code>ReinforcementLearningZoo.OutcomeSamplingMCCFRPolicy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">OutcomeSamplingMCCFRPolicy</code></pre><p>This implementation uses stochasticaly-weighted averaging.</p><p>Ref:</p><ul><li><a href="http://mlanctot.info/files/papers/PhD_Thesis_MarcLanctot.pdf">MONTE CARLO SAMPLING AND REGRET MINIMIZATION FOR EQUILIBRIUM COMPUTATION AND DECISION-MAKING IN LARGE EXTENSIVE FORM GAMES</a></li><li><a href="https://papers.nips.cc/paper/3713-monte-carlo-sampling-for-regret-minimization-in-extensive-games.pdf">Monte Carlo Sampling for Regret Minimization in Extensive Games</a></li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.PPOPolicy" href="#ReinforcementLearningZoo.PPOPolicy"><code>ReinforcementLearningZoo.PPOPolicy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">PPOPolicy(;kwargs)</code></pre><p><strong>Keyword arguments</strong></p><ul><li><code>approximator</code>,</li><li><code>γ = 0.99f0</code>,</li><li><code>λ = 0.95f0</code>,</li><li><code>clip_range = 0.2f0</code>,</li><li><code>max_grad_norm = 0.5f0</code>,</li><li><code>n_microbatches = 4</code>,</li><li><code>n_epochs = 4</code>,</li><li><code>actor_loss_weight = 1.0f0</code>,</li><li><code>critic_loss_weight = 0.5f0</code>,</li><li><code>entropy_loss_weight = 0.01f0</code>,</li><li><code>dist = Categorical</code>,</li><li><code>rng = Random.GLOBAL_RNG</code>,</li></ul><p>By default, <code>dist</code> is set to <code>Categorical</code>, which means it will only works on environments of discrete actions. To work with environments of</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.PrioritizedDQNLearner" href="#ReinforcementLearningZoo.PrioritizedDQNLearner"><code>ReinforcementLearningZoo.PrioritizedDQNLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">PrioritizedDQNLearner(;kwargs...)</code></pre><p>See paper: <a href="https://arxiv.org/abs/1511.05952">Prioritized Experience Replay</a> And also https://danieltakeshi.github.io/2019/07/14/per/</p><p><strong>Keywords</strong></p><ul><li><code>approximator</code>::<a href="../rl_core/#ReinforcementLearningCore.AbstractApproximator"><code>AbstractApproximator</code></a>: used to get Q-values of a state.</li><li><code>target_approximator</code>::<a href="../rl_core/#ReinforcementLearningCore.AbstractApproximator"><code>AbstractApproximator</code></a>: similar to <code>approximator</code>, but used to estimate the target (the next state).</li><li><code>loss_func</code>: the loss function.</li><li><code>γ::Float32=0.99f0</code>: discount rate.</li><li><code>batch_size::Int=32</code></li><li><code>update_horizon::Int=1</code>: length of update (&#39;n&#39; in n-step update).</li><li><code>min_replay_history::Int=32</code>: number of transitions that should be experienced before updating the <code>approximator</code>.</li><li><code>update_freq::Int=4</code>: the frequency of updating the <code>approximator</code>.</li><li><code>target_update_freq::Int=100</code>: the frequency of syncing <code>target_approximator</code>.</li><li><code>stack_size::Union{Int, Nothing}=4</code>: use the recent <code>stack_size</code> frames to form a stacked state.</li><li><code>default_priority::Float64=100.</code>: the default priority for newly added transitions.</li><li><code>rng = Random.GLOBAL_RNG</code></li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.PrioritizedDQNLearner-Tuple{Any}" href="#ReinforcementLearningZoo.PrioritizedDQNLearner-Tuple{Any}"><code>ReinforcementLearningZoo.PrioritizedDQNLearner</code></a> — <span class="docstring-category">Method</span></header><section><div><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The state of the observation is assumed to have been stacked, if <code>!isnothing(stack_size)</code>.</p></div></div></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.RainbowLearner" href="#ReinforcementLearningZoo.RainbowLearner"><code>ReinforcementLearningZoo.RainbowLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">RainbowLearner(;kwargs...)</code></pre><p>See paper: <a href="https://arxiv.org/abs/1710.02298">Rainbow: Combining Improvements in Deep Reinforcement Learning</a></p><p><strong>Keywords</strong></p><ul><li><code>approximator</code>::<a href="../rl_core/#ReinforcementLearningCore.AbstractApproximator"><code>AbstractApproximator</code></a>: used to get Q-values of a state.</li><li><code>target_approximator</code>::<a href="../rl_core/#ReinforcementLearningCore.AbstractApproximator"><code>AbstractApproximator</code></a>: similar to <code>approximator</code>, but used to estimate the target (the next state).</li><li><code>loss_func</code>: the loss function.</li><li><code>Vₘₐₓ::Float32</code>: the maximum value of distribution.</li><li><code>Vₘᵢₙ::Float32</code>: the minimum value of distribution.</li><li><code>n_actions::Int</code>: number of possible actions.</li><li><code>γ::Float32=0.99f0</code>: discount rate.</li><li><code>batch_size::Int=32</code></li><li><code>update_horizon::Int=1</code>: length of update (&#39;n&#39; in n-step update).</li><li><code>min_replay_history::Int=32</code>: number of transitions that should be experienced before updating the <code>approximator</code>.</li><li><code>update_freq::Int=4</code>: the frequency of updating the <code>approximator</code>.</li><li><code>target_update_freq::Int=500</code>: the frequency of syncing <code>target_approximator</code>.</li><li><code>stack_size::Union{Int, Nothing}=4</code>: use the recent <code>stack_size</code> frames to form a stacked state.</li><li><code>default_priority::Float32=1.0f2.</code>: the default priority for newly added transitions. It must be <code>&gt;= 1</code>.</li><li><code>n_atoms::Int=51</code>: the number of buckets of the value function distribution.</li><li><code>stack_size::Union{Int, Nothing}=4</code>: use the recent <code>stack_size</code> frames to form a stacked state.</li><li><code>rng = Random.GLOBAL_RNG</code></li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.SACPolicy-Tuple{}" href="#ReinforcementLearningZoo.SACPolicy-Tuple{}"><code>ReinforcementLearningZoo.SACPolicy</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">SACPolicy(;kwargs...)</code></pre><p><strong>Keyword arguments</strong></p><ul><li><code>policy</code>,</li><li><code>qnetwork1</code>,</li><li><code>qnetwork2</code>,</li><li><code>target_qnetwork1</code>,</li><li><code>target_qnetwork2</code>,</li><li><code>start_policy</code>,</li><li><code>γ = 0.99f0</code>,</li><li><code>ρ = 0.995f0</code>,</li><li><code>α = 0.2f0</code>,</li><li><code>batch_size = 32</code>,</li><li><code>start_steps = 10000</code>,</li><li><code>update_after = 1000</code>,</li><li><code>update_every = 50</code>,</li><li><code>step = 0</code>,</li><li><code>rng = Random.GLOBAL_RNG</code>,</li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.TD3Policy-Tuple{}" href="#ReinforcementLearningZoo.TD3Policy-Tuple{}"><code>ReinforcementLearningZoo.TD3Policy</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">TD3Policy(;kwargs...)</code></pre><p><strong>Keyword arguments</strong></p><ul><li><code>behavior_actor</code>,</li><li><code>behavior_critic</code>,</li><li><code>target_actor</code>,</li><li><code>target_critic</code>,</li><li><code>start_policy</code>,</li><li><code>γ = 0.99f0</code>,</li><li><code>ρ = 0.995f0</code>,</li><li><code>batch_size = 32</code>,</li><li><code>start_steps = 10000</code>,</li><li><code>update_after = 1000</code>,</li><li><code>update_every = 50</code>,</li><li><code>policy_freq = 2</code> # frequency in which the actor performs a gradient step and critic target is updated</li><li><code>target_act_limit = 1.0</code>, # noise added to actor target</li><li><code>target_act_noise = 0.1</code>, # noise added to actor target</li><li><code>act_limit = 1.0</code>, # noise added when outputing action</li><li><code>act_noise = 0.1</code>, # noise added when outputing action</li><li><code>step = 0</code>,</li><li><code>rng = Random.GLOBAL_RNG</code>,</li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.TabularCFRPolicy-Tuple{}" href="#ReinforcementLearningZoo.TabularCFRPolicy-Tuple{}"><code>ReinforcementLearningZoo.TabularCFRPolicy</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">TabularCFRPolicy(;kwargs...)</code></pre><p>Some useful papers while implementing this algorithm:</p><ul><li><a href="http://modelai.gettysburg.edu/2013/cfr/cfr.pdf">An Introduction to Counterfactual Regret Minimization</a></li><li><a href="http://mlanctot.info/files/papers/PhD_Thesis_MarcLanctot.pdf">MONTE CARLO SAMPLING AND REGRET MINIMIZATION FOR EQUILIBRIUM COMPUTATION AND DECISION-MAKING IN LARGE EXTENSIVE FORM GAMES</a></li><li><a href="https://arxiv.org/pdf/1407.5042.pdf">Solving Large Imperfect Information Games Using CFR⁺</a></li><li><a href="https://arxiv.org/pdf/1810.11542v1.pdf">Revisiting CFR⁺ and Alternating Updates</a></li><li><a href="https://arxiv.org/pdf/1809.04040.pdf">Solving Imperfect-Information Games via Discounted Regret Minimization</a></li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>is_alternating_update=true</code>: If <code>true</code>, we update the players alternatively.</li><li><code>is_reset_neg_regrets=true</code>: Whether to use <strong>regret matching⁺</strong>.</li><li><code>is_linear_averaging=true</code></li><li><code>weighted_averaging_delay=0</code>. The averaging delay in number of iterations. Only valid when <code>is_linear_averaging</code> is set to <code>true</code>.</li><li><code>state_type=String</code>, the data type of information set.</li><li><code>rng=Random.GLOBAL_RNG</code></li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.VPGPolicy" href="#ReinforcementLearningZoo.VPGPolicy"><code>ReinforcementLearningZoo.VPGPolicy</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Vanilla Policy Gradient</p><p>VPGPolicy(;kwargs)</p><p><strong>Keyword arguments</strong></p><ul><li><code>approximator</code>,</li><li><code>baseline</code>,</li><li><code>dist</code>, distribution function of the action</li><li><code>γ</code>, discount factor</li><li><code>α_θ</code>, step size of policy parameter</li><li><code>α_w</code>, step size of baseline parameter</li><li><code>batch_size</code>,</li><li><code>rng</code>,</li><li><code>loss</code>,</li><li><code>baseline_loss</code>,</li></ul><p>if the action space is continuous, then the env should transform the action value, (such as using tanh), in order to make sure low ≤ value ≤ high</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L21">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.update!-Tuple{DeepCFR,AbstractEnv}" href="#ReinforcementLearningBase.update!-Tuple{DeepCFR,AbstractEnv}"><code>ReinforcementLearningBase.update!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Run one interation</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.update!-Tuple{DeepCFR}" href="#ReinforcementLearningBase.update!-Tuple{DeepCFR}"><code>ReinforcementLearningBase.update!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Update Π (policy network)</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.update!-Tuple{ExternalSamplingMCCFRPolicy,AbstractEnv}" href="#ReinforcementLearningBase.update!-Tuple{ExternalSamplingMCCFRPolicy,AbstractEnv}"><code>ReinforcementLearningBase.update!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Run one interation</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.update!-Tuple{OutcomeSamplingMCCFRPolicy,AbstractEnv}" href="#ReinforcementLearningBase.update!-Tuple{OutcomeSamplingMCCFRPolicy,AbstractEnv}"><code>ReinforcementLearningBase.update!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Run one interation</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.update!-Tuple{TabularCFRPolicy,AbstractEnv}" href="#ReinforcementLearningBase.update!-Tuple{TabularCFRPolicy,AbstractEnv}"><code>ReinforcementLearningBase.update!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Run one interation</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningBase.update!-Tuple{TabularCFRPolicy}" href="#ReinforcementLearningBase.update!-Tuple{TabularCFRPolicy}"><code>ReinforcementLearningBase.update!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Update the <code>behavior_policy</code></p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.cfr!" href="#ReinforcementLearningZoo.cfr!"><code>ReinforcementLearningZoo.cfr!</code></a> — <span class="docstring-category">Function</span></header><section><div><p>Symbol meanings:</p><p>π: reach prob π′: new reach prob π₋ᵢ: opponents&#39; reach prob p: player to update. <code>nothing</code> means simultaneous update. w: weight v: counterfactual value <strong>before weighted by opponent&#39;s reaching probability</strong> V: a vector containing the <code>v</code> after taking each action with current information set. Used to calculate the <strong>regret value</strong></p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.evaluate-Tuple{SACPolicy,Any}" href="#ReinforcementLearningZoo.evaluate-Tuple{SACPolicy,Any}"><code>ReinforcementLearningZoo.evaluate</code></a> — <span class="docstring-category">Method</span></header><section><div><p>This function is compatible with a multidimensional action space.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.external_sampling!-Tuple{DeepCFR,AbstractEnv,Any}" href="#ReinforcementLearningZoo.external_sampling!-Tuple{DeepCFR,AbstractEnv,Any}"><code>ReinforcementLearningZoo.external_sampling!</code></a> — <span class="docstring-category">Method</span></header><section><div><p>CFR Traversal with External Sampling</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.masked_regret_matching-Tuple{Any,Any}" href="#ReinforcementLearningZoo.masked_regret_matching-Tuple{Any,Any}"><code>ReinforcementLearningZoo.masked_regret_matching</code></a> — <span class="docstring-category">Method</span></header><section><div><p>This is the specific regret matching method used in DeepCFR</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ReinforcementLearningZoo.update_advantage_networks-Tuple{Any,Any}" href="#ReinforcementLearningZoo.update_advantage_networks-Tuple{Any,Any}"><code>ReinforcementLearningZoo.update_advantage_networks</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Update advantage network</p></div></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../rl_envs/">« RLEnvs</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 9 November 2020 03:33">Monday 9 November 2020</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
