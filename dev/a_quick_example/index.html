<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>A Quick Example · ReinforcementLearning.jl</title><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-149861753-1', 'auto');
ga('send', 'pageview', {'page': location.pathname + location.search + location.hash});
</script><link rel="canonical" href="https://juliareinforcementlearning.github.io/ReinforcementLearning.jl/latest/a_quick_example/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../assets/custom.css" rel="stylesheet" type="text/css"/><link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" rel="stylesheet" type="text/css"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="top" class="navbar-wrapper">
<nav class="navbar navbar-expand-lg  navbar-dark fixed-top" style="background-color: #1fd1f9; background-image: linear-gradient(315deg, #1fd1f9 0%, #b621fe 74%); " id="mainNav">
  <div class="container-md">
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarTogglerDemo01" aria-controls="navbarTogglerDemo01" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>
  <div class="collapse navbar-collapse" id="navbarTogglerDemo01">
    <span class="navbar-brand">
        <a class="navbar-brand" href="/">
          <!-- <img src="/assets/site/logo.svg" width="30" height="30" alt="logo" loading="lazy"> -->
          JuliaReinforcementLearning
        </a>
    </span>

    <ul class="navbar-nav ml-auto">
        <li class="nav-item">
        <a class="nav-link" href="/get_started/">Get Started</a>
        </li>
        <li class="nav-item">
        <a class="nav-link" href="/guide/">Guide</a>
        </li>
        <li class="nav-item">
        <a class="nav-link" href="/blog/">Blog</a>
        </li>
        <li class="nav-item">
        <a class="nav-link" href="https://JuliaReinforcementLearning.github.io/ReinforcementLearning.jl/latest/">Doc</a>
        </li>
        <li class="nav-item">
        <a class="nav-link" href="https://github.com/JuliaReinforcementLearning">Github</a>
        </li>
    </ul>
  </div>
</nav>
</div>
<div class="documenter-wrapper" id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="ReinforcementLearning.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">ReinforcementLearning.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../rl_base/">RLBase</a></li><li><a class="tocitem" href="../rl_core/">RLCore</a></li><li><a class="tocitem" href="../rl_envs/">RLEnvs</a></li><li><a class="tocitem" href="../rl_zoo/">RLZoo</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>A Quick Example</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>A Quick Example</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/master/docs/src/a_quick_example.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="A-Quick-Example-1"><a class="docs-heading-anchor" href="#A-Quick-Example-1">A Quick Example</a><a class="docs-heading-anchor-permalink" href="#A-Quick-Example-1" title="Permalink"></a></h1><p>Welcome to the world of reinforcement learning in Julia! Here&#39;s a quick example to show you how to train an agent with to play the <a href="../rl_envs/#ReinforcementLearningEnvironments.CartPoleEnv"><code>CartPoleEnv</code></a>.</p><p>First, let&#39;s make sure that this package is properly installed.</p><pre><code class="language-none">using ReinforcementLearning</code></pre><p>Cartpole is considered to be one of the simplest environments for <strong>DRL(Deep Reinforcement Learning)</strong> algorithms testing. The state of the Cartpole environment can be described with 4 numbers and the actions are two integers(<code>1</code> and <code>2</code>). Before game terminates, agent can gain a reward of <code>+1</code> for each step. And the game will be forced to end after 200 steps, thus the maximum reward of an episode is <strong>200</strong>. </p><pre><code class="language-none">env = CartPoleEnv(;T=Float32, seed=123)</code></pre><p>Then we create an agent to play with the cartpole environment.</p><pre><code class="language-none">agent = Agent(
    policy = RandomPolicy(env;seed=456),
    trajectory = CircularCompactSARTSATrajectory(; capacity=3, state_type=Float32, state_size = (4,)),
)</code></pre><p>An agent is usually constructed by a policy and a trajectory. A policy is a mapping from an observation to an action. And a trajectory is used to store some important information of the interactions between agents and environments. The <a href="../rl_base/#ReinforcementLearningBase.RandomPolicy"><code>RandomPolicy</code></a> used here will do nothing but select an action randomly. And the <a href="../rl_core/#ReinforcementLearningCore.CircularCompactSARTSATrajectory"><code>CircularCompactSARTSATrajectory</code></a> here will store the <strong>S</strong>tate, <strong>A</strong>ction, <strong>R</strong>eward, <strong>T</strong>erminal, next-<strong>S</strong>tate and next-<strong>A</strong>ction in each step of the latest 3 episodes.</p><p>Now we can start to run simulations:</p><pre><code class="language-none">run(agent, env, StopAfterEpisode(1))</code></pre><p>Here the <a href="../rl_core/#ReinforcementLearningCore.StopAfterEpisode"><code>StopAfterEpisode</code></a><code>(1)</code> is a stop condition, which means stop after <code>1</code> episode here. Then we can take a look at the trajectory in the agent.</p><pre><code class="language-none">agent.trajectory
# 3-element Trajectory{(:state, :action, :reward, :terminal, :next_state, :next_action),Tuple{Float32,Int64,Float32,Bool,Float32,Int64},NamedTuple{(:reward, :terminal, :state, :action),Tuple{CircularArrayBuffer{Float32,1},CircularArrayBuffer{Bool,1},CircularArrayBuffer{Float32,2},CircularArrayBuffer{Int64,1}}}}:
#  (state = Float32[-0.116456345, -0.57231975, 0.16624497, 1.1284109], action = 2, reward = 1.0f0, terminal = false, next_state = Float32[-0.12790275, -0.37971866, 0.18881318, 0.89214355], next_action = 2)
#  (state = Float32[-0.12790275, -0.37971866, 0.18881318, 0.89214355], action = 2, reward = 1.0f0, terminal = false, next_state = Float32[-0.13549712, -0.18759018, 0.20665605, 0.6642545], next_action = 1) 
#  (state = Float32[-0.13549712, -0.18759018, 0.20665605, 0.6642545], action = 1, reward = 0.0f0, terminal = true, next_state = Float32[-0.13924892, -0.38489604, 0.21994114, 1.0142413], next_action = 2)</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Since we have set the random seed of the <a href="../rl_base/#ReinforcementLearningBase.RandomPolicy"><code>RandomPolicy</code></a> and the <a href="../rl_envs/#ReinforcementLearningEnvironments.CartPoleEnv"><code>CartPoleEnv</code></a>, you should see the exactly same result as above.</p></div></div><p>To record the total reward of each episode, we can add a hook during each <code>run</code>.</p><pre><code class="language-none">hook = TotalRewardPerEpisode()
run(agent, env, StopAfterEpisode(10000), hook)
sum(hook.rewards)/10000  # 21.0591</code></pre><p>Playing the cartpole environment with a <a href="../rl_base/#ReinforcementLearningBase.RandomPolicy"><code>RandomPolicy</code></a> is not very interesting. Next we will use a DQN to solve the problem.</p><p>Because this package relies on <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> to build deep learning models, you need to manually install Flux to run the following example. To show the rewards of each episode and the fps, <a href="https://github.com/JuliaPlots/Plots.jl">Plots.jl</a> and <a href="https://github.com/JuliaStats/StatsBase.jl">StatsBase.jl</a> are also required.</p><pre><code class="language-none">using Flux
using ReinforcementLearning
using StatsBase:mean

env = CartPoleEnv(; T = Float32, seed = 11)
ns, na = length(rand(get_observation_space(env))), length(get_action_space(env))
agent = Agent(
    policy = QBasedPolicy(
        learner = BasicDQNLearner(
            approximator = NeuralNetworkApproximator(
                model = Chain(
                    Dense(ns, 128, relu; initW = seed_glorot_uniform(seed = 17)),
                    Dense(128, 128, relu; initW = seed_glorot_uniform(seed = 23)),
                    Dense(128, na; initW = seed_glorot_uniform(seed = 39)),
                ) |&gt; gpu,
                optimizer = ADAM(),
            ),
            batch_size = 32,
            min_replay_history = 100,
            loss_func = huber_loss,
            seed = 22,
        ),
        explorer = EpsilonGreedyExplorer(
            kind = :exp,
            ϵ_stable = 0.01,
            decay_steps = 500,
            seed = 33,
        ),
    ),
    trajectory = CircularCompactSARTSATrajectory(
        capacity = 1000,
        state_type = Float32,
        state_size = (ns,),
    ),
)
hook = ComposedHook(TotalRewardPerEpisode(), TimePerStep())
run(agent, env, StopAfterStep(10000), hook)

@info &quot;stats for BasicDQNLearner&quot; avg_reward = mean(hook[1].rewards) avg_fps = 1 / mean(hook[2].times)
# ┌ Info: stats for BasicDQNLearner
# │   avg_reward = 107.43478260869566
# └   avg_fps = 531.283841452491</code></pre><p>The main difference here is that, now we are using a <a href="../rl_core/#ReinforcementLearningCore.QBasedPolicy"><code>QBasedPolicy</code></a> instead of a <a href="../rl_base/#ReinforcementLearningBase.RandomPolicy"><code>RandomPolicy</code></a>. A model of three Dense layers is used to calculate the Q values.</p><p>Relax! We promise that all the new concepts above will be explained in detail later.</p><p>Now we can also plot the rewards stored in our hook:</p><pre><code class="language-none">using Plots
plot(hook[1].rewards, xlabel=&quot;Episode&quot;, ylabel=&quot;Reward&quot;, label=&quot;&quot;)</code></pre><p><img src="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/raw/master/docs/src/assets/img/a_quick_example_cartpole_cpu_basic_dqn.png" alt/></p><p><strong>That&#39;s fantastic!</strong></p><ul><li><p><em>&quot;But I&#39;m new to Julia and RL. Can I learn RL by using this package?&quot;</em></p><p>Yes! One of this package&#39;s main goals is to be educational. <a href="http://incompleteideas.net/book/the-book-2nd.html">Reinforcement Learning: An Introduction</a> is a good introductory book. And we reproduce almost all the examples mentioned in that book by using this package <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningAnIntroduction.jl">here</a>.</p></li><li><p><em>&quot;What if I have a solid background in RL but new to Julia?&quot;</em></p><blockquote><p>Programming isn&#39;t hard. Programming <strong>well</strong> is <strong>very</strong> hard!  - <a href="https://www.cs.cornell.edu/courses/cs3110/">CS 3110</a></p></blockquote><p>Fortunately, Julia provides some amazing features together with many awesome packages to make things much easier. We provide a <a href="../tips_for_developers/#Tips-for-Developers-1">Tips for Developers</a> section to help you grasp Julia in depth.</p></li><li><p><em>&quot;I&#39;m experienced in both Julia and RL. But I find it hard to use this package...&quot;</em></p><p>Although we tried our best to make concepts and codes as simple as possible, it is still possible that they are not very intuitive enough. So do not hesitate to <strong>JOIN US</strong> (create an issue or a PR). We need <strong>YOU</strong> to improve all this stuff together!</p></li></ul></article></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Thursday 18 June 2020 04:29">Thursday 18 June 2020</span>. Using Julia version 1.3.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
