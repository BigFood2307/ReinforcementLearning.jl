<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>A Quick Example · ReinforcementLearning.jl</title><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-149861753-1', 'auto');
ga('send', 'pageview');
</script><link rel="canonical" href="https://juliareinforcementlearning.github.io/ReinforcementLearning.jl/latest/a_quick_example/index.html"/><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../assets/custom.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="../"><img class="logo" src="../assets/logo.png" alt="ReinforcementLearning.jl logo"/></a><h1>ReinforcementLearning.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li class="current"><a class="toctext" href>A Quick Example</a><ul class="internal"></ul></li><li><a class="toctext" href="../overview/">Overview</a></li><li><span class="toctext">Manual</span><ul><li><span class="toctext">Components</span><ul><li><a class="toctext" href="../components/agents/">Agents</a></li><li><a class="toctext" href="../components/environments/">Environments</a></li><li><a class="toctext" href="../components/buffers/">Buffers</a></li><li><a class="toctext" href="../components/policies/">Policies</a></li><li><a class="toctext" href="../components/learners/">Learners</a></li><li><a class="toctext" href="../components/approximators/">Approximators</a></li><li><a class="toctext" href="../components/action_selectors/">Action Selectors</a></li><li><a class="toctext" href="../components/environment_models/">Environment Models</a></li></ul></li><li><a class="toctext" href="../core/">Core</a></li><li><a class="toctext" href="../utils/">Utils</a></li></ul></li><li><a class="toctext" href="../tips_for_developers/">Tips for Developers</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>A Quick Example</a></li></ul><a class="edit-page" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/master/docs/src/a_quick_example.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>A Quick Example</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="A-Quick-Example-1" href="#A-Quick-Example-1">A Quick Example</a></h1><p>Welcome to the world of reinforcement learning in Julia! Here&#39;s a quick example to show you how to train an agent with a <a href="../components/learners/#ReinforcementLearning.BasicDQNLearner"><code>BasicDQNLearner</code></a> to play the <code>CartPoleEnv</code>.</p><div class="admonition note"><div class="admonition-title">Note</div><div class="admonition-text"><p>Notice that a lot of dependent packages are under rapid development. To make sure that you can reproduce the result in this example, you are suggested to:</p><ol><li>Make sure that your Julia version is <code>v1.3-rc3</code> or above</li><li>Clone <code>git@github.com:JuliaReinforcementLearning/ReinforcementLearning.jl.git</code></li><li><code>cd ReinforcementLearning.jl</code></li><li><code>julia --project=docs</code></li><li><code>]instantiate</code></li></ol></div></div><p>First, let&#39;s make sure that running the following code will not trigger any error:</p><pre><code class="language-julia">using ReinforcementLearning, ReinforcementLearningEnvironments, Flux
using StatsBase:mean</code></pre><p>Cartpole is considered to be one of the simplest environments for <strong>DRL(Deep Reinforcement Learning)</strong> algorithms testing. The state of the Cartpole environment can be described with 4 numbers and the actions are two integers(<code>1</code> and <code>2</code>). Before game terminates, agent can gain a reward of <code>+1</code> for each step. And the game will be forced to end after 200 steps, thus the maximum reward of an episode is <strong>200</strong>. </p><pre><code class="language-julia">env = CartPoleEnv(;T=Float32)
ns, na = length(observation_space(env)), length(action_space(env))  # (4, 2)</code></pre><pre><code class="language-none">(4, 2)</code></pre><p>Then we can create an agent:</p><pre><code class="language-julia">backend  = :Zygote
device = :cpu

agent = Agent(
    π = QBasedPolicy(
        learner = BasicDQNLearner(
            approximator = NeuralNetworkQ(
                model = Chain(
                    Dense(ns, 128, relu; backend=backend),
                    Dense(128, 128, relu; backend=backend),
                    Dense(128, na; backend=backend)
                    ),
                optimizer = ADAM(),
                device = device
            ),
            batch_size = 32,
            min_replay_history = 100,
            loss_fun = huber_loss,
        ),
        selector = EpsilonGreedySelector{:exp}(ϵ_stable = 0.01, decay_steps = 500),
    ),
    buffer = circular_RTSA_buffer(
        capacity = 1000,
        state_eltype = Vector{Float32},
        state_size = (ns,),
    )
)</code></pre><pre><code class="language-none">Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkQ{:cpu,Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM,Zygote.Params},typeof(huber_loss)},EpsilonGreedySelector{:exp}},CircularTurnBuffer{(:reward, :terminal, :state, :action),Tuple{Float32,Bool,Array{Float32,1},Int64},NamedTuple{(:reward, :terminal, :state, :action),Tuple{CircularArrayBuffer{Float32,1},CircularArrayBuffer{Bool,1},CircularArrayBuffer{Array{Float32,1},2},CircularArrayBuffer{Int64,1}}}},Symbol}(QBasedPolicy{BasicDQNLearner{NeuralNetworkQ{:cpu,Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM,Zygote.Params},typeof(huber_loss)},EpsilonGreedySelector{:exp}}(BasicDQNLearner{NeuralNetworkQ{:cpu,Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM,Zygote.Params},typeof(huber_loss)}(NeuralNetworkQ{:cpu,Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM,Zygote.Params}(Chain(Dense(4, 128, relu; backend=Zygote), Dense(128, 128, relu; backend=Zygote), Dense(128, 2; backend=Zygote)), Flux.Optimise.ADAM(0.001, (0.9, 0.999), IdDict{Any,Any}()), Params([Float32[-0.09408988 0.14165749 -0.092088565 -0.02588605; 0.11846615 -0.06076146 -0.023047086 -0.14934573; … ; -0.08106611 -0.18100913 0.212019 -0.006396727; -0.1463466 -0.05458412 -0.025337836 0.005903717], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.08302641 0.013984849 … 0.14609466 0.14286605; -0.057777695 -0.03390982 … 0.10506073 0.013057561; … ; 0.1041906 -0.066400036 … -0.07381944 0.017013201; -0.040603746 0.13533881 … 0.12392832 -0.11473322], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.16698317 0.1363972 … -0.0996437 0.08168056; -0.10660339 0.06148297 … 0.0036677995 0.13261491], Float32[0.0, 0.0]])), ReinforcementLearning.Utils.huber_loss, 0.99f0, 32, 1, 100), EpsilonGreedySelector{:exp}(0.01, 1.0, 0, 500, 1)), NamedTuple{(:reward, :terminal, :state, :action),Tuple{Float32,Bool,Array{Float32,1},Int64}}[], :DEFAULT)</code></pre><p>Relax! We promise that all the new concepts above will be explained in detail later.</p><p>For now, you only need to know that an <a href="../components/agents/#ReinforcementLearning.Agent"><code>Agent</code></a> is usually composed by a <em>policy</em> and a <em>buffer</em>. Here we are using a very common <a href="../components/policies/#ReinforcementLearning.QBasedPolicy"><code>QBasedPolicy</code></a> and a <a href="../components/buffers/#ReinforcementLearning.circular_RTSA_buffer"><code>circular_RTSA_buffer</code></a>. For a <a href="../components/policies/#ReinforcementLearning.QBasedPolicy"><code>QBasedPolicy</code></a> we need to provide a <em>learner</em> and a <em>selector</em>. The <em>learner</em> here is used to provide the value estimations of all actions in a step, and the <em>selector</em> is use to select an action based on those estimations. For a <em>buffer</em>, it stores some transitions between an <em>agent</em> and an <em>environment</em> and is used to improve the <em>policy</em>. That&#39;s all!</p><p>To record the reward and performance , we need some hooks:</p><pre><code class="language-julia">hook = ComposedHook(
    TotalRewardPerEpisode(),
    TimePerStep()
)</code></pre><pre><code class="language-none">ComposedHook{Tuple{TotalRewardPerEpisode,TimePerStep}}((TotalRewardPerEpisode(Float64[], 0.0, &quot;TRAINING&quot;), TimePerStep(Float64[], 0x000000681358e6cb)))</code></pre><p>And finally, let&#39;s push the button:</p><pre><code class="language-">run(agent, env, StopAfterStep(10000; is_show_progress=false); hook = hook)

print(&quot;&quot;&quot;
    backend = $backend, device = $device
    avg_reward = $(mean(hook[1].rewards))
    avg_fps = $(1/mean(hook[2].times))
    &quot;&quot;&quot;)</code></pre><p>We can also plot the rewards stored in our hook:</p><pre><code class="language-julia">using Plots
plot(hook[1].rewards, xlabel=&quot;Episode&quot;, ylabel=&quot;Reward&quot;, label=&quot;&quot;)</code></pre><pre><code class="language-none">WARNING: using Plots.backend in module ex-1 conflicts with an existing identifier.
/home/travis/.julia/packages/GR/ZI5OE/src/../deps/gr/bin/gksqt: error while loading shared libraries: libQt5Widgets.so.5: cannot open shared object file: No such file or directory
connect: Connection refused
GKS: can&#39;t connect to GKS socket application
Did you start &#39;gksqt&#39;?

GKS: Open failed in routine OPEN_WS
GKS: GKS not in proper state. GKS must be either in the state WSOP or WSAC in routine ACTIVATE_WS</code></pre><p><img src="../a_quick_example_cartpole_cpu_basic_dqn.png" alt/></p><p><strong>That&#39;s fantastic!</strong></p><ul><li><p><em>&quot;But I&#39;m new to Julia and RL. Can I learn RL by using this package?&quot;</em></p><p>Yes! One of this package&#39;s main goals is to be educational. <a href="http://incompleteideas.net/book/the-book-2nd.html">Reinforcement Learning: An Introduction</a> is a good introductory book. And we reproduce almost all the examples mentioned in that book by using this package <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningAnIntroduction.jl">here</a>.</p></li><li><p><em>&quot;What if I have a solid background in RL but new to Julia?&quot;</em></p><blockquote><p>Programming isn&#39;t hard. Programming <strong>well</strong> is <strong>very</strong> hard!  - <a href="https://www.cs.cornell.edu/courses/cs3110/">CS 3110</a></p></blockquote><p>Fortunately, Julia provides some amazing features together with many awesome packages to make things much easier. We provide a <a href="../tips_for_developers/#Tips-for-Developers-1">Tips for Developers</a> section to help you grasp Julia in depth.</p></li><li><p><em>&quot;I&#39;m experienced in both Julia and RL. But I find it hard to use this package...&quot;</em></p><p>Although we tried our best to make concepts and codes as simple as possible, it is still possible that they are not very intuitive enough. So do not hesitate to <strong>JOIN US</strong> (create an issue or a PR). We need <strong>YOU</strong> to improve all this stuff together!</p></li></ul><footer><hr/><a class="previous" href="../"><span class="direction">Previous</span><span class="title">Home</span></a><a class="next" href="../overview/"><span class="direction">Next</span><span class="title">Overview</span></a></footer></article></body></html>
